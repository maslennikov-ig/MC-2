# Vector Lifecycle Management with Content Deduplication

**Task**: T079 - Implement vector lifecycle management with content deduplication
**Status**: ‚úÖ IMPLEMENTED
**Date**: 2025-10-15

## Overview

This module implements reference counting for vector lifecycle management, preventing duplicate vector generation when identical content is uploaded multiple times across courses or organizations.

## Problem Statement

**Before Implementation:**
- ‚ùå Same file uploaded twice = 2√ó Docling processing
- ‚ùå Same file uploaded twice = 2√ó Jina embedding costs (~$0.02/M tokens each)
- ‚ùå Same file uploaded twice = 2√ó Qdrant storage
- ‚ùå Same file uploaded twice = 2√ó processing time

**Example Scenario:**
1. Professor uploads "Introduction to Machine Learning.pdf" to Course A
2. Same professor uploads same PDF to Course B
3. Different professor at different organization uploads same PDF to Course C

All 3 uploads process the same content independently, wasting time and money.

## Solution: Reference Counting

**After Implementation:**
- ‚úÖ Same file uploaded twice = 1√ó Docling processing (second upload skipped)
- ‚úÖ Same file uploaded twice = 1√ó Jina embedding cost (vectors reused)
- ‚úÖ Same file uploaded twice = 1.1√ó Qdrant storage (only duplicate points, not embeddings)
- ‚úÖ Second upload completes instantly (no processing queue)

**How It Works:**
1. Calculate SHA-256 hash on upload
2. Check if file with same hash exists and is indexed
3. If exists: Create reference record, duplicate vectors with new metadata, skip processing
4. If new: Create original record, save file, queue for processing
5. On delete: Decrement reference count, delete vectors for specific course
6. When reference count = 0: Delete physical file and all vectors

## Architecture

### Database Schema

**New Columns in `file_catalog`:**

```sql
-- Reference counting
reference_count INTEGER NOT NULL DEFAULT 1

-- Original file tracking
original_file_id UUID REFERENCES file_catalog(id)
```

**Indexes:**

```sql
-- Critical for deduplication lookups (queried on every upload)
CREATE INDEX idx_file_catalog_hash ON file_catalog(hash)
WHERE vector_status = 'indexed';

-- Track references to original files
CREATE INDEX idx_file_catalog_original_file_id ON file_catalog(original_file_id)
WHERE original_file_id IS NOT NULL;
```

**Helper Functions:**

```sql
-- Find duplicate file by hash
find_duplicate_file(p_hash TEXT) ‚Üí file_id, storage_path, vector_status, etc.

-- Increment reference count atomically
increment_file_reference_count(p_file_id UUID) ‚Üí new_count

-- Decrement reference count atomically
decrement_file_reference_count(p_file_id UUID) ‚Üí new_count
```

### Code Structure

```
src/shared/qdrant/
‚îú‚îÄ‚îÄ lifecycle.ts                          # Core deduplication logic
‚îú‚îÄ‚îÄ lifecycle-integration-example.ts      # Integration examples
‚îú‚îÄ‚îÄ __tests__/lifecycle.test.ts          # Test suite
‚îî‚îÄ‚îÄ LIFECYCLE-README.md                   # This file

supabase/migrations/
‚îú‚îÄ‚îÄ 20251015_add_content_deduplication.sql       # Main migration
‚îî‚îÄ‚îÄ 20251015_add_storage_quota_functions.sql     # Helper functions
```

## API Reference

### `handleFileUpload(fileBuffer, metadata): Promise<FileUploadResult>`

Handles file upload with automatic deduplication.

**Parameters:**
- `fileBuffer: Buffer` - File content
- `metadata: FileUploadMetadata` - Upload metadata
  - `filename: string` - Original filename
  - `organization_id: string` - Organization UUID
  - `course_id: string` - Course UUID
  - `mime_type: string` - MIME type (e.g., "application/pdf")
  - `user_id?: string` - Optional user UUID

**Returns:**
```typescript
{
  file_id: string;              // Created file_catalog record ID
  deduplicated: boolean;         // Whether file was deduplicated
  original_file_id?: string;     // Original file ID (if deduplicated)
  vector_status: string;         // 'indexed' or 'pending'
  vectors_duplicated?: number;   // Count of duplicated vectors
}
```

**Example:**
```typescript
import { handleFileUpload } from './shared/qdrant/lifecycle';

const result = await handleFileUpload(fileBuffer, {
  filename: 'lecture-notes.pdf',
  organization_id: 'org-uuid',
  course_id: 'course-uuid',
  mime_type: 'application/pdf',
});

if (result.deduplicated) {
  console.log('‚úì File ready immediately (vectors reused)');
} else {
  console.log('‚è≥ File queued for processing');
}
```

### `handleFileDelete(fileId): Promise<FileDeleteResult>`

Handles file deletion with reference counting.

**Parameters:**
- `fileId: string` - File UUID to delete

**Returns:**
```typescript
{
  physical_file_deleted: boolean;   // Whether physical file was deleted
  remaining_references: number;      // Remaining reference count
  vectors_deleted: number;           // Count of vectors deleted
  storage_freed_bytes: number;       // Storage freed
}
```

**Example:**
```typescript
import { handleFileDelete } from './shared/qdrant/lifecycle';

const result = await handleFileDelete('file-uuid');

if (result.physical_file_deleted) {
  console.log('‚úì Physical file deleted (no references remain)');
} else {
  console.log(`‚úì Reference deleted (${result.remaining_references} remain)`);
}
```

### `duplicateVectorsForNewCourse(originalFileId, newFileId, newCourseId, newOrganizationId): Promise<number>`

Duplicates vectors for a new course (internal function, called by `handleFileUpload`).

**Parameters:**
- `originalFileId: string` - Original file with existing vectors
- `newFileId: string` - New file reference
- `newCourseId: string` - New course UUID
- `newOrganizationId: string` - New organization UUID

**Returns:** Number of vectors duplicated

**Process:**
1. Query Qdrant for all vectors with `document_id = originalFileId`
2. Create new points with:
   - **Same embeddings** (dense + sparse)
   - **Different metadata** (document_id, course_id, organization_id)
3. Upload to Qdrant in batches

### `calculateFileHash(buffer): string`

Calculates SHA-256 hash of file buffer.

**Parameters:**
- `buffer: Buffer` - File content

**Returns:** Hex-encoded SHA-256 hash

### `updateStorageQuota(organizationId, fileSize, operation): Promise<void>`

Updates storage quota for an organization.

**Parameters:**
- `organizationId: string` - Organization UUID
- `fileSize: number` - File size in bytes
- `operation: 'increment' | 'decrement'` - Operation type

**Throws:** Error if quota exceeded (on increment)

### `getDeduplicationStats(organizationId): Promise<Stats>`

Gets deduplication statistics for an organization.

**Parameters:**
- `organizationId: string` - Organization UUID

**Returns:**
```typescript
{
  original_files: number;        // Count of original files
  reference_files: number;        // Count of reference files
  storage_saved_bytes: number;    // Estimated storage savings
  total_storage_bytes: number;    // Total storage used
}
```

## Integration Guide

### 1. Run Migrations

```bash
# Apply deduplication migration
supabase db reset

# Or manually:
psql $DATABASE_URL < supabase/migrations/20251015_add_storage_quota_functions.sql
psql $DATABASE_URL < supabase/migrations/20251015_add_content_deduplication.sql
```

### 2. Update Upload Endpoint

**Before:**
```typescript
app.post('/api/files/upload', async (req, res) => {
  const file = req.file;

  // Save file
  const path = await saveFile(file);

  // Create database record
  const record = await db.insert('file_catalog', { ... });

  // Queue for processing
  await queue.add('process', { file_id: record.id });

  res.json({ file_id: record.id, status: 'pending' });
});
```

**After:**
```typescript
import { handleFileUpload } from './shared/qdrant/lifecycle';

app.post('/api/files/upload', async (req, res) => {
  const file = req.file;

  // Handle upload with automatic deduplication
  const result = await handleFileUpload(file.buffer, {
    filename: file.originalname,
    organization_id: req.user.organization_id,
    course_id: req.body.course_id,
    mime_type: file.mimetype,
  });

  if (result.deduplicated) {
    // Instant response - file already indexed
    res.json({
      file_id: result.file_id,
      status: 'indexed',
      message: 'Content reused from existing upload',
    });
  } else {
    // Queue for processing
    await queue.add('process', { file_id: result.file_id });

    res.json({
      file_id: result.file_id,
      status: 'pending',
      message: 'File queued for processing',
    });
  }
});
```

### 3. Update Delete Endpoint

**Before:**
```typescript
app.delete('/api/files/:fileId', async (req, res) => {
  const fileId = req.params.fileId;

  // Get file record
  const file = await db.get('file_catalog', fileId);

  // Delete from Qdrant
  await qdrant.delete({ filter: { document_id: fileId } });

  // Delete physical file
  await fs.unlink(file.storage_path);

  // Delete database record
  await db.delete('file_catalog', fileId);

  res.json({ success: true });
});
```

**After:**
```typescript
import { handleFileDelete } from './shared/qdrant/lifecycle';

app.delete('/api/files/:fileId', async (req, res) => {
  const result = await handleFileDelete(req.params.fileId);

  res.json({
    success: true,
    physical_file_deleted: result.physical_file_deleted,
    remaining_references: result.remaining_references,
    message: result.physical_file_deleted
      ? 'File deleted permanently'
      : `File reference deleted (${result.remaining_references} references remain)`,
  });
});
```

## Testing

### Run Test Suite

```bash
# Set environment variables
export SUPABASE_URL=https://your-project.supabase.co
export SUPABASE_SERVICE_KEY=your-service-key
export UPLOADS_DIR=/tmp/megacampus/uploads

# Run tests
tsx src/shared/qdrant/__tests__/lifecycle.test.ts
```

### Expected Test Output

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  Vector Lifecycle Deduplication Tests ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

========================================
TEST 1: Same file uploaded to 2 courses (same organization)
========================================

Test file hash: a1b2c3d4e5f6g7h8...

--- Upload 1: First course ---
‚úì PASS: First upload created new file

--- Upload 2: Second course (same file) ---
‚úì Content deduplication: Found existing file with hash a1b2c3d4...
‚úì Duplicated 1000 vectors for course uuid-2
‚úì PASS: Second upload was deduplicated
  - Original file: uuid-1
  - Vectors duplicated: 1000

--- Deduplication Statistics ---
  - Original files: 1
  - Reference files: 1
  - Storage saved: 2458624 bytes

========================================
TEST 2: Delete one reference
========================================

‚úì PASS: Physical file retained
  - Remaining references: 1

‚úì PASS: Course 1 file record still exists
  - Vector status: indexed
  - Reference count: 1

========================================
TEST 3: Delete last reference
========================================

‚úì PASS: Physical file deleted when reference count = 0
  - Remaining references: 0

‚úì PASS: File record deleted from database

========================================
TEST 4: Cross-organization deduplication
========================================

‚úì PASS: Cross-organization deduplication works
  - Both orgs share physical file
  - Both orgs have isolated vectors (different document_id, course_id)

--- Quota Accounting ---
Org 1 storage: 2458624 bytes
Org 2 storage: 2458624 bytes
‚úì Both organizations pay for their reference

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         All Tests Completed!           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

## Cost Savings Analysis

### Example: 10MB PDF with 1000 chunks

**Scenario:** Same file uploaded 5 times (2 courses in Org A, 2 courses in Org B, 1 course in Org C)

**Before (No Deduplication):**
- Docling processing: 5√ó ~20 seconds = 100 seconds
- Jina embeddings: 5√ó ~$0.02 = $0.10
- Qdrant storage: 5√ó 1000 vectors √ó 768D √ó 4 bytes = 15.36 MB
- Processing time: 5√ó processing jobs

**After (With Deduplication):**
- Docling processing: 1√ó ~20 seconds = 20 seconds (80% savings)
- Jina embeddings: 1√ó ~$0.02 = $0.02 (80% savings)
- Qdrant storage: ~1.5√ó original (5√ó points with same embeddings) = ~4.6 MB (70% savings)
- Processing time: 1√ó processing job + 4√ó instant responses

**Savings:**
- ‚è±Ô∏è Time: 80 seconds saved
- üí∞ Cost: $0.08 saved per 5 uploads
- üíæ Storage: 10.76 MB saved
- üöÄ User Experience: 4/5 uploads complete instantly

## Monitoring

### Query Deduplication Statistics

```sql
-- View all deduplicated files
SELECT * FROM file_catalog_deduplication_stats
WHERE file_type = 'reference'
ORDER BY reference_copies DESC;

-- View organization deduplication stats
SELECT * FROM organization_deduplication_stats
ORDER BY storage_saved_bytes DESC;

-- Find files with most references
SELECT
  id,
  filename,
  reference_count,
  file_size,
  reference_count * file_size AS total_storage_saved
FROM file_catalog
WHERE original_file_id IS NULL
  AND reference_count > 1
ORDER BY reference_count DESC
LIMIT 10;
```

### Metrics to Track

1. **Deduplication Rate**: `reference_files / (original_files + reference_files)`
2. **Storage Savings**: `SUM(storage_saved_bytes) / SUM(total_storage_bytes)`
3. **Processing Time Savings**: Count of deduplicated uploads √ó avg processing time
4. **Cost Savings**: Count of deduplicated uploads √ó avg embedding cost

## Error Handling

### Deduplication Failures

If deduplication fails (e.g., vector duplication error), the system automatically falls back to normal upload:

```typescript
try {
  // Attempt deduplication
  const vectors = await duplicateVectorsForNewCourse(...);
} catch (error) {
  console.error('Deduplication failed, falling back to normal upload:', error);
  // Continue with normal upload path (save file, queue for processing)
}
```

### Reference Count Integrity

The system includes constraints and atomic operations to prevent reference count corruption:

```sql
-- Constraint: No self-references
ALTER TABLE file_catalog
ADD CONSTRAINT check_no_self_reference
CHECK (original_file_id IS NULL OR original_file_id != id);

-- Atomic increment/decrement via database functions
SELECT increment_file_reference_count('uuid');
SELECT decrement_file_reference_count('uuid');
```

## Multi-Tenancy Isolation

**Critical:** Even though physical files are shared, vectors are isolated per course/organization:

```typescript
// Vector metadata for Course A (Org 1)
{
  document_id: 'file-reference-1',
  course_id: 'course-a',
  organization_id: 'org-1',
  // ... same embeddings ...
}

// Vector metadata for Course B (Org 2)
{
  document_id: 'file-reference-2',
  course_id: 'course-b',
  organization_id: 'org-2',
  // ... SAME embeddings, DIFFERENT metadata ...
}
```

Search queries filter by `course_id` and `organization_id`, ensuring complete isolation.

## Future Enhancements

1. **Garbage Collection**: Periodic cleanup of orphaned vectors (vectors with no matching file_catalog record)
2. **Deduplication Analytics Dashboard**: Visual analytics for cost savings and deduplication rates
3. **Smart Deduplication Threshold**: Only deduplicate files above certain size (e.g., >1MB)
4. **Version-Aware Deduplication**: Track file versions and allow upgrading all references
5. **Cross-Organization Sharing**: Optional feature to share content across organizations with proper access control

## Troubleshooting

### Issue: Deduplication not working

**Symptoms:** All uploads create new files, no deduplication

**Diagnosis:**
```sql
-- Check if hash index exists
SELECT * FROM pg_indexes WHERE indexname = 'idx_file_catalog_hash';

-- Check if files have hashes
SELECT COUNT(*) FROM file_catalog WHERE hash IS NULL;

-- Check vector_status of existing files
SELECT vector_status, COUNT(*) FROM file_catalog GROUP BY vector_status;
```

**Solution:**
- Ensure migration ran successfully
- Verify hash is calculated on upload
- Check that files reach `vector_status = 'indexed'` before deduplication

### Issue: Reference count incorrect

**Symptoms:** Reference count doesn't match actual references

**Diagnosis:**
```sql
-- Count actual references vs reference_count
SELECT
  original_file_id,
  COUNT(*) AS actual_references,
  MAX(reference_count) AS stored_count
FROM file_catalog
WHERE original_file_id IS NOT NULL
GROUP BY original_file_id
HAVING COUNT(*) != MAX(reference_count);
```

**Solution:**
```sql
-- Recalculate reference counts
UPDATE file_catalog f1
SET reference_count = (
  SELECT COUNT(*) + 1  -- +1 for self
  FROM file_catalog f2
  WHERE f2.original_file_id = f1.id
)
WHERE original_file_id IS NULL;
```

## Summary

This implementation provides robust content deduplication with:

- ‚úÖ **Automatic deduplication** via SHA-256 hash comparison
- ‚úÖ **Reference counting** for proper lifecycle management
- ‚úÖ **Vector reuse** with isolated metadata per course
- ‚úÖ **Atomic operations** for data integrity
- ‚úÖ **Cost savings** from reduced processing and storage
- ‚úÖ **Instant uploads** for duplicated content
- ‚úÖ **Multi-tenancy isolation** via filtered vector search

**Files Created:**
1. `supabase/migrations/20251015_add_content_deduplication.sql`
2. `supabase/migrations/20251015_add_storage_quota_functions.sql`
3. `src/shared/qdrant/lifecycle.ts`
4. `src/shared/qdrant/lifecycle-integration-example.ts`
5. `src/shared/qdrant/__tests__/lifecycle.test.ts`
6. `src/shared/qdrant/LIFECYCLE-README.md` (this file)

**Next Steps:**
1. Run migrations in production
2. Update upload/delete endpoints
3. Monitor deduplication metrics
4. Measure cost savings
