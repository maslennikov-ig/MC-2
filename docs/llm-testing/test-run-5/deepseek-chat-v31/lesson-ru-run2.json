{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Данная секция знакомит с фундаментальными концепциями искусственных нейронных сетей. Вы изучите базовую архитектуру, принцип работы нейрона и освоите процесс обучения сети на простых примерах.",
  "learning_objectives": [
    "Построить и протестировать модель перцептрона для решения задачи бинарной классификации.",
    "Реализовать простую нейронную сеть с использованием библиотеки TensorFlow или PyTorch.",
    "Обучить нейронную сеть на предоставленном наборе данных, интерпретировать кривые обучения (loss/accuracy).",
    "Проанализировать и объяснить роль функции активации и алгоритма обратного распространения ошибки в процессе обучения."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Введение в искусственные нейронные сети",
      "lesson_objective": "Опишите архитектуру и вычислите выход простого искусственного нейрона (перцептрона) для заданных входных данных и весов.",
      "key_topics": [
        "Биологический нейрон vs. Искусственный нейрон",
        "Структура перцептрона: входы, веса, сумматор, функция активации",
        "Пороговая функция активации (Step function)",
        "Геометрическая интерпретация: разделяющая гиперплоскость",
        "Ограничения однослойного перцептрона (задача XOR)"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет выхода нейрона",
          "exercise_instructions": "1. Даны входные данные X = [1, 0.5, -0.2] и веса W = [0.7, -0.3, 1.0]. 2. Рассчитайте взвешенную сумму (z). 3. Примените пороговую функцию активации (ступенчатую функцию) с порогом b=0.5. 4. Запишите полученное выходное значение (0 или 1)."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Многослойные сети и функции активации",
      "lesson_objective": "Сравните результаты применения различных функций активации (ReLU, Sigmoid, Tanh) к одному и тому же входному значению.",
      "key_topics": [
        "Архитектура многослойного перцептрона (MLP): входной, скрытый, выходной слои",
        "Назначение нелинейных функций активации",
        "Обзор и свойства функций: Sigmoid, Hyperbolic Tangent (Tanh), Rectified Linear Unit (ReLU)",
        "Проблема исчезающих градиентов",
        "Понятие прямого распространения (Forward Pass)"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение функций активации",
          "exercise_instructions": "1. Возьмите входное значение z = 1.5. 2. Рассчитайте и запишите выходные значения для следующих функций активации: Sigmoid, Tanh, ReLU. 3. Для значения z = -1.0 повторите шаг 2. 4. Сравните результаты и сформулируйте одно ключевое различие между этими функциями."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обучение нейронной сети: Backpropagation",
      "lesson_objective": "Выполните одну итерацию алгоритма обратного распространения ошибки (backpropagation) для упрощенной сети, рассчитав обновленные веса.",
      "key_topics": [
        "Цель обучения: минимизация функции потерь (Loss Function)",
        "Интуиция behind градиентного спуска",
        "Цепное правило для вычисления градиентов",
        "Алгоритм обратного распространения ошибки (Backpropagation) пошагово",
        "Обновление весов с использованием learning rate"
      ],
      "exercises": [
        {
          "exercise_title": "Шаг обратного распространения",
          "exercise_instructions": "Дана простая сеть: один вход (x=2), один вес (w=0.5), желаемый выход (y_true=1). Функция активации - линейная. Функция потерь - MSE. Learning rate = 0.1. 1. Рассчитайте исходный выход (y_pred) и потерю (L). 2. Вычислите градиент функции потерь по весу (dL/dw). 3. Обновите вес, используя градиентный спуск: w_new = w_old - learning_rate * (dL/dw). 4. Запишите новое значение веса."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Практика: Построение первой нейронной сети",
      "lesson_objective": "Реализовать, скомпилировать и обучить модель нейронной сети для классификации набора данных MNIST, добившись accuracy на тестовой выборке выше 90%.",
      "key_topics": [
        "Обзор библиотек TensorFlow/Keras или PyTorch",
        "Создание последовательной модели (Sequential API)",
        "Определение архитектуры сети: Dense-слои, количество нейронов",
        "Компиляция модели: выбор оптимизатора, функции потерь и метрик",
        "Обучение модели методом .fit() и валидация"
      ],
      "exercises": [
        {
          "exercise_title": "Классификация рукописных цифр",
          "exercise_instructions": "1. Загрузите набор данных MNIST, используя встроенные датасеты Keras/PyTorch. Нормализуйте данные. 2. Определите модель с одним скрытым слоем (128 нейронов, ReLU) и выходным слоем (10 нейронов, softmax). 3. Скомпилируйте модель с оптимизатором 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']. 4. Обучите модель на 5 эпохах с validation_split=0.2. 5. Оцените точность модели на тестовом наборе данных."
        }
      ]
    }
  ]
}