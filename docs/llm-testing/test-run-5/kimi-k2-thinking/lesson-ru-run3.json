{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Изучите фундаментальные концепции нейронных сетей, от биологического вдохновения до математических основ. Освойте ключевые архитектуры и механизмы обучения, необходимые для построения первых моделей глубокого обучения.","learning_objectives":["Сформулируйте математическую модель искусственного нейрона и объясните процесс его работы","Разработайте нейронную сеть с одним скрытым слоем для решения задачи бинарной классификации","Сравните эффективность различных функций активации на практических задачах","Реализуйте алгоритм обратного распространения ошибки для простой сети вручную"],"lessons":[{"lesson_number":1,"lesson_title":"Искусственный нейрон: от биологии к математике","lesson_objective":"Постройте математическую модель искусственного нейрона и реализуйте его на Python для линейной классификации","key_topics":["Биологическая и искусственная модель нейрона","Взвешенная сумма входов и смещение","Функции активации: пороговая и сигмоидальная","Геометрическая интерпретация решений","Правило Хебба и его ограничения","Перцептрон Розенблатта"],"exercises":[{"exercise_title":"Реализация нейрона с пороговой активацией","exercise_instructions":"1. Создайте класс Neuron с параметрами: weights (numpy array), bias (float), activation_function (str). 2. Реализуйте метод forward_pass, который вычисляет взвешенную сумму и применяет пороговую функцию (threshold=0). 3. Обучите нейрон на датасете AND (4 точки) вручную, подбирая веса [-0.5, -0.5] и bias 0.75. 4. Проверьте точность на всех комбинациях входов. 5. Визуализируйте разделяющую гиперплоскость с помощью matplotlib."},{"exercise_title":"Сравнение функций активации","exercise_instructions":"1. Расширьте класс Neuron, добавив sigmoid и ReLU активации. 2. Создайте три идентичных нейрона с разными функциями активации. 3. Прогоните через них входные данные из диапазона [-5, 5] с шагом 0.5. 4. Постройте графики выходных значений для сравнения. 5. Опишите в комментариях, где каждая функция будет эффективнее."}]},{"lesson_number":2,"lesson_title":"Многослойные перцептроны и архитектуры","lesson_objective":"Спроектируйте и реализуйте многослойный перцептрон для решения нелинейной задачи классификации","key_topics":["Ограничения однослойных сетей (проблема XOR","Многослойные перцептроны (MLP","Прямое распространение (Forward Propagation","Выбор количества слоев и нейронов","Проблема переобучения и недообучения","Валидационная кривая обучения","Инициализация весов: Xavier и He"],"exercises":[{"exercise_title":"Построение MLP для XOR","exercise_instructions":"1. Создайте класс MLP с архитектурой: 2 входа, скрытый слой (4 нейрона, ReLU), выходной слой (1 нейрон, sigmoid). 2. Инициализируйте веса случайно из нормального распределения (mean=0, std=0.1). 3. Реализуйте метод forward_pass для всей сети. 4. Обучите модель на датасете XOR (100 эпох) используя библиотеку PyTorch. 5. Визуализируйте решающую границу с помощью contourf и оцените точность."},{"exercise_title":"Эксперимент с инициализацией весов","exercise_instructions":"1. Реализуйте три MLP для классификации 8-классового датасета make_classification. 2. Инициализируйте веса тремя способами: нулями, случайными из N(0,1), Xavier uniform. 3. Обучите каждую модель 50 эпох с одинаковым learning rate. 4. Запишите loss и accuracy для каждой эпохи. 5. Постройте три графика сравнения сходимости и сделайте выводы."}]},{"lesson_number":3,"lesson_title":"Функции активации: теория и практика","lesson_objective":"Оцените влияние различных функций активации на производительность сети и выберите оптимальную для конкретной задачи","key_topics":["Сигмоида и проблема затухающего градиента","Гиперболический тангенс (tanh)","ReLU и его варианты (Leaky ReLU, PReLU","Softmax для многоклассовой классификации","Выбор функции активации по слою","Экспериментальное сравнение производительности"],"exercises":[{"exercise_title":"Анализ затухающего градиента","exercise_instructions":"1. Создайте глубокую сеть (5 скрытых слоев по 50 нейронов) с сигмоидальной активацией. 2. Инициализируйте веса Xavier uniform. 3. Обучите на датасете MNIST (1000 образцов) 20 эпох. 4. Вычислите градиенты для каждого слоя после обучения. 5. Постройте график нормы градиентов по слоям и проанализируйте проблему."},{"exercise_title":"Сравнительный анализ ReLU-вариантов","exercise_instructions":"1. Реализуйте 4 MLP с архитектурой: 784-128-64-10 для MNIST. 2. Используйте активации: ReLU, Leaky ReLU (α=0.01), PReLU (обучаемый α), ELU (α=1.0). 3. Обучите каждую модель 30 эпох с одинаковыми гиперпараметрами. 4. Запишите final accuracy, training time и количество мертвых нейронов. 5. Создайте сводную таблицу и выберите лучшую функцию с обоснованием."}]},{"lesson_number":4,"lesson_title":"Градиентный спуск и обратное распространение","lesson_objective":"Реализуйте алгоритм обратного распространения ошибки вручную для 2-слойной сети и обучите её на простом датасете","key_topics":["Функция потерь: MSE и Cross-Entropy","Метод градиентного спуска","Вычислительный граф и цепное правило","Алгоритм обратного распространения (Backprop","Адаптивные методы оптимизации (SGD, Adam","Практические советы по настройке learning rate"],"exercises":[{"exercise_title":"Ручная реализация backpropagation","exercise_instructions":"1. Реализуйте 2-слойную сеть (2-3-1) без использования PyTorch/TensorFlow autograd. 2. Напишите функцию forward_pass, возвращающую все промежуточные значения. 3. Реализуйте функцию backward_pass, вычисляющую градиенты вручную через цепное правило. 4. Обучите сеть на датасете XOR с learning rate 0.1 в течение 1000 итераций. 5. Выведите финальные веса и убедитесь, что MSE < 0.01."},{"exercise_title":"Сравнение оптимизаторов","exercise_instructions":"1. Создайте MLP (784-128-64-10) для классификации MNIST. 2. Обучите модель 20 эпох используя SGD (lr=0.01), Adam (lr=0.001) и RMSprop (lr=0.001). 3. Используйте одинаковую инициализацию весов и batch size=64. 4. Запишите loss и accuracy каждую эпоху. 5. Постройте графики сходимости и сравните скорость сходимости и финальную точность."}]}]}