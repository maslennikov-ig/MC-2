{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Изучите устройство и принципы работы нейронных сетей, начиная с биологического вдохновения и заканчивая полноценным обучением простой модели на Python. Секция даёт прочную базу для последующего углубления в глубокое обучение.",
  "learning_objectives": [
    "Объяснить структуру и математику искусственного нейрона, включая взвешенную сумму и функцию активации",
    "Реализовать с нуля нейронную сеть прямого распространения с одним скрытым слоем для бинарной классификации",
    "Произвести прямой и обратный проход вручную для сети из 3-5 нейронов и сверить результат с кодом",
    "Выбрать и обосновать функцию активации, число слоёв и нейронов для заданной задачи",
    "Обучить модель на реальном датасете (например, Iris), оценить точность и интерпретировать матрицу ошибок"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному",
      "lesson_objective": "Вручную вычислить выход искусственного нейрона для заданных весов и объяснить роль каждого компонента.",
      "key_topics": [
        "Структура биологического нейрона",
        "Взвешенная сумма входов",
        "Смещение (bias)",
        "Функции активации: пороговая, сигмоидальная, ReLU",
        "Графическое представление нейрона"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной прямой проход",
          "exercise_instructions": "1) Задайте входной вектор x=[0.5, -1.2, 0.8], веса w=[0.4, 0.7, -0.5], bias=0.3. 2) Вычислите взвешенную сумму z. 3) Примените сигмоиду σ(z)=1/(1+e^(-z)). 4) Повторите для ReLU и сравните результаты."
        },
        {
          "exercise_title": "Визуализация функций активации",
          "exercise_instructions": "1) Сгенерируйте 200 точек в диапазоне [-5,5]. 2) Постройте на одном графике σ(x), tanh(x), ReLU(x). 3) Подпишите оси и добавьте легенду. 4) Сохраните изображение как activation_functions.png."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Архитектура полносвязной сети",
      "lesson_objective": "Построить схему полносвязной сети с 2 входами, 3 скрытыми нейронами и 1 выходом, указав размерность каждого тензора на всех этапах.",
      "key_topics": [
        "Слои: входной, скрытые, выходной",
        "Размерности матриц весов и смещений",
        "Параметры модели vs гиперпараметры",
        "Поток данных при прямом проходе",
        "Представление батчей тензором"
      ],
      "exercises": [
        {
          "exercise_title": "Подсчёт параметров",
          "exercise_instructions": "1) Для сети 4→5→2→1 слоя подсчитайте общее число обучаемых параметров. 2) Запишите формулы размерности каждой матрицы весов и вектора смещений. 3) Проверьте себя через код: создайте nn.Sequential в PyTorch и выведите model.parameters()."
        },
        {
          "exercise_title": "Прямой проход в numpy",
          "exercise_instructions": "1) Инициализируйте веса и смещения случайными числами. 2) Реализуйте функцию forward(x) без циклов, только операции numpy. 3) Протестируйте на батче из 10 примеров размерности 2. 4) Убедитесь, что выход имеет форму (10,1)."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции потерь и обратное распространение",
      "lesson_objective": "Выполнить полный цикл обратного распространения ошибки вручную для сети 2→1 и сверить градиенты с автоматическим дифференцированием.",
      "key_topics": [
        "MSE и кросс-энтропия",
        "Производная функции потерь",
        "Цепное правило для градиентов",
        "Вычислительный граф",
        "Алгоритм обратного распространения шаг за шагом"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной Backprop",
          "exercise_instructions": "1) Выберите сеть 2→1 с сигмоидой на выходе и MSE-лоcс. 2) Проведите прямой проход для одного примера. 3) Вычислите градиент потерь по выходу. 4) Рекурсивно найдите градиенты по весам и смещениям. 5) Сравните с torch.autograd."
        },
        {
          "exercise_title": "Влияние масштаба выхода",
          "exercise_instructions": "1) Сгенерируйте 100 точек из нормального распределения. 2) Обучите линейный нейрон предсказывать значения, масштабированные в 10×. 3) Повторите с масштабом 0.1×. 4) Постройте график потерь и сделайте вывод о выборе функции потерь."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Оптимизаторы и обучение",
      "lesson_objective": "Обучить нейросеть классифицировать цветки Iris с точностью ≥95 %, используя разные оптимизаторы и сравнивать скорость сходимости.",
      "key_topics": [
        "Градиентный спуск и его варианты",
        "Скорость обучения (learning rate)",
        "SGD, Momentum, Adam",
        "Перемешивание данных (shuffling)",
        "Мониторинг переобучения"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение оптимизаторов",
          "exercise_instructions": "1) Загрузите датасет Iris, преобразуйте метки в one-hot. 2) Реализуйте сеть 4→10→3. 3) Обучите её SGD, Momentum, Adam при фиксированном lr=0.01. 4) Постройте графики потерь на валидации за 200 эпох. 5) Укажите, какой оптимизатор достиг 95 % первым."
        },
        {
          "exercise_title": "Подбор learning rate",
          "exercise_instructions": "1) Проведите эксперименты с lr в диапазоне [0.0001, 1]. 2) Используйте экспоненциальный рост: lr_i=lr_0·10^(i/10). 3) Постройте кривую ‘loss vs lr’ и найдите оптимум. 4) Зафиксируйте лучший lr и переобучите модель до 98 % точности."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Регуляризация и практические советы",
      "lesson_objective": "Снизить переобучение сети на данных CIFAR-10 с 60 % до ≥75 % валидационной точности, применяя L2, дропаут и аугментацию.",
      "key_topics": [
        "Переобучение и недообучение",
        "L1 и L2 регуляризация",
        "Dropout как ensemble",
        "Data augmentation",
        "Нормализация входов и батч-нормализация"
      ],
      "exercises": [
        {
          "exercise_title": "Влияние веса L2",
          "exercise_instructions": "1) Используйте сеть 3072→256→128→10 на CIFAR-10. 2) Обучите без регуляризации и запишите точность. 3) Добавьте weight_decay=1e-4, 1e-3, 1e-2. 4) Постройте график train/val accuracy и выберите лучший коэффициент."
        },
        {
          "exercise_title": "Dropout vs без дропаута",
          "exercise_instructions": "1) Добавьте слои Dropout(p=0.5) после каждого плотного слоя. 2) Сравните кривые обучения при p=0, 0.2, 0.5, 0.7. 3) Убедитесь, что в режиме eval dropout отключён. 4) Сохраните лучшую модель как best_cifar.pt."
        }
      ]
    }
  ]
}