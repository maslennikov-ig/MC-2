{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Освойте базовые принципы построения, обучения и оценки нейронных сетей. Научитесь реализовывать простые модели и понимать, как они извлекают закономерности из данных.","learning_objectives":["Построить и обучить полносвязную нейронную сеть на синтетических данных с точностью ≥ 90 %","Рассчитать вручную прямое распространение и градиент для сети из 3 нейронов","Выбрать оптимальный набор гиперпараметров (learning rate, число эпох, скрытые нейроны) через кросс-валидацию","Оценить качество модели с помощью F1-меры и кривых обучения, выявить переобучение","Использовать современные фреймворки (PyTorch или TensorFlow) для загрузки данных, определения архитектуры и запуска процесса обучения"],"lessons":[{"lesson_number":1,"lesson_title":"Введение в нейронные сети и перцептрон","lesson_objective":"Объяснить устройство и обучение отдельного нейрона, реализовать перцептрон для линейно разделимой задачи","key_topics":["Биологическая мотивация и история","Структура искусственного нейрона: веса, смещение, функция активации","Правило обучения перцептрона и его сходимость","Линейная и нелинейная разделимость: XOR-проблема","Переход от одного нейрона к многослойной сети"],"exercises":[{"exercise_title":"Реализация перцептрона на Python","exercise_instructions":"1. Сгенерируйте 200 точек 2-D классов A и B, разделённых прямой y=0.5x+1. 2. Реализуйте класс Perceptron с методами fit и predict. 3. Обучите модель до 100 % точности на обучающей выборке. 4. Выведите финальные веса и нарисуйте разделяющую прямую поверх данных."},{"exercise_title":"XOR-проблема вручную","exercise_instructions":"1. Постройте таблицу истинности XOR. 2. Покажите, что ни одна линейная модель не может достичь 0 ошибок. 3. Добавьте ручно сконструированный признак x1·x2 и убедитесь, что задача стала линейно разделимой. 4. Обучите перцептрон на новых признаках и проверьте точность."}]},{"lesson_number":2,"lesson_title":"Архитектура многослойного перцептрона","lesson_objective":"Спроектировать полносвязную сеть заданной глубины и ширины, вычислить количество обучаемых параметров","key_topics":["Скрытые слои и глубина сети","Матричное представление пакетных вычислений","Функции активации: ReLU, Sigmoid, Tanh, LeakyReLU","Инициализация весов: Xavier, He","Поток градиентов при прямом проходе"],"exercises":[{"exercise_title":"Конструктор MLP","exercise_instructions":"1. Напишите функцию build_mlp(n_in, hidden_list, n_out, act='relu'), возвращающую последовательную модель Keras. 2. Для набора (n_in=784, hidden_list=[128,64], n_out=10) выведите число параметров каждого слоя и общее число. 3. Постройте сеть для задачи регрессии (n_out=1) с тремя скрытыми слоями и активацией tanh на выходе. 4. Сравните число параметров при изменении ширины слоёв в 2 раза."},{"exercise_title":"Выбор функции активации","exercise_instructions":"1. Сгенерируйте 1 000 точек для регрессии sin(x) с шумом. 2. Обучите три сети одинаковой архитектуры, но с разными активациями: ReLU, Sigmoid, Tanh. 3. Постройте кривые потерь на валидации и сравните RMSE. 4. Сделайте вывод о том, какая функция лучше подходит для данной задачи."}]},{"lesson_number":3,"lesson_title":"Обратное распространение ошибки","lesson_objective":"Выполнить ручной расчёт градиентов для сети с двумя слоями и сверить результат с автоматическим дифференцированием","key_topics":["Цепное правило в глубоких сетях","Вычислительный граф и локальные градиенты","Алгоритм обратного распространения шаг за шагом","Градиентная проверка (gradient checking)","Вычислительная сложность и векторизация"],"exercises":[{"exercise_title":"Backprop вручную","exercise_instructions":"1. Возьмите сеть 2-3-1 (вход 2, скрытый 3, выход 1) с сигмоидальными активациями. 2. Задайте произвольные веса и один пример (x,y). 3. Просчитайте прямой проход, затем вычислите градиенты dL/dW2 и dL/dW1 по формулам. 4. Сверьте результат с численным градиентом по центральной разности с ε=1e-5."},{"exercise_title":"Градиентная проверка в PyTorch","exercise_instructions":"1. Определите ту же архитектуру через torch.nn.Sequential. 2. Включите torch.autograd.set_detect_anomaly(True). 3. Реализуйте функцию compute_numerical_gradient(model, x, y) и сравните её вывод с model.parameters() градиентами. 4. Убедитесь, что относительная разница < 1e-7 для всех параметров."}]},{"lesson_number":4,"lesson_title":"Функции потерь и оптимизаторы","lesson_objective":"Подобрать оптимальный алгоритм обучения и функцию потерь для задач классификации и регрессии, добившись минимальной валидационной ошибки","key_topics":["MSE и Cross-Entropy: когда какую использовать","Стохастический градиентный спуск (SGD)","Momentum, Nesterov, AdaGrad, RMSProp, Adam","Настройка learning rate: decay, schedules, warm-up","Оvershooting и vanishing gradients: признаки и меры"],"exercises":[{"exercise_title":"Сравнение оптимизаторов","exercise_instructions":"1. Загрузите датасет Fashion-MNIST и преобразуйте метки в one-hot векторы. 2. Обучите одну и ту же сеть 784-128-10 пять раз: SGD, SGD+momentum, AdaGrad, RMSProp, Adam. 3. Используйте фиксированный learning_rate=0.001, batch_size=64, epochs=10. 4. Постройте графики train/val accuracy и сделайте таблицу финальных показателей."},{"exercise_title":"Подбор learning rate","exercise_instructions":"1. Реализуйте цикл по learning_rate ∈ [1e-5, 1e-1] с шагом ×10. 2. Для каждого значения обучите модель 5 эпох и запишите валидационную точность. 3. Постройте кривую accuracy vs lr (логарифмическая шкала). 4. Выберите наилучший lr и обучите модель до сходимости, зафиксируйте улучшение."}]},{"lesson_number":5,"lesson_title":"Оценка качества и борьба с переобучением","lesson_objective":"Провести полный цикл тренировки с кросс-валидацией, ранней остановкой и регуляризацией, добившись разницы между train и val F1 < 3 %","key_topics":["Train/validation/test split и k-fold CV","Метрики: accuracy, precision, recall, F1, ROC-AUC","Ранняя остановка (early stopping) и сохранение чекпоинтов","L1 и L2 регуляризация, dropout, data augmentation","Кривые обучения: диагностика high bias vs high variance"],"exercises":[{"exercise_title":"Кросс-валидация и early stopping","exercise_instructions":"1. Разделите данные CIFAR-10 на 5 фолдов. 2. Для каждого фолда обучите сеть 32-64-128-10 с dropout=0.5. 3. Используйте EarlyStopping(patience=3, restore_best_weights=True). 4. Соедините предсказания всех фолдов и сообщите средний F1-score и стандартное отклонение."},{"exercise_title":"Диагностика переобучения","exercise_instructions":"1. Обучите модель на 10 % шумных данных CIFAR-10. 2. Стройте кривые loss и accuracy на train и val каждую эпоху. 3. Определите момент, когда начинается overfit. 4. Добавьте L2=1e-4 и dropout=0.3, повторите обучение и сравните разрыв между train и val F1 до и после мер."}]}]}