{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Изучите устройство и принципы работы нейронных сетей, освойте прямое и обратное распространение ошибки, научитесь строить и обучать простейшие модели на Python без сторонних фреймворков. Секция подготовит к использованию профессиональных инструментов и глубокому обучению.",
  "learning_objectives": [
    "Объяснить биологическую мотивацию и математическую модель искусственного нейрона",
    "Реализовать прямое распространение (forward pass) для полносвязной сети на NumPy",
    "Выполнить обратное распространение ошибки (backpropagation) вручную для сети с одним скрытым слоем",
    "Обучить двухслойную сеть классифицировать 2D-данные и достичь accuracy ≥ 90 %",
    "Проанализировать влияние числа нейронов и скорости обучения на сходимость"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному: модель Перцептрона",
      "lesson_objective": "Построить и визуализировать модель Перцептрона, реализовать функции активации и объяснить её ограничения.",
      "key_topics": [
        "Биологическая мотивация",
        "Линейная комбинация весов и входов",
        "Пороговая и сигмоидальная функции активации",
        "Геометрическая интерпретация: разделяющая гиперплоскость",
        "Проблема XOR и необходимость скрытых слоёв"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация Перцептрона на NumPy",
          "exercise_instructions": "1) Создайте класс Perceptron с методами fit(X, y, lr=0.1, epochs=100) и predict(X). 2) Инициализируйте веса случайно из нормального распределения. 3) В цикле обновляйте веса по правилу: w += lr * (y_true - y_pred) * x. 4) Обучите модель на датасете AND и выведите финальные веса. 5) Постройте разделяющую прямую поверх точек."
        },
        {
          "exercise_title": "Визуализация функций активации",
          "exercise_instructions": "1) Сгенерируйте массив z от -5 до 5 с шагом 0.1. 2) Реализуйте и постройте на одном графике пороговую, сигмоиду, tanh и ReLU. 3) Под каждым графиком выведите производную в аналитическом виде. 4) Сохраните изображение activation_functions.png."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Архитектура многослойного перцептрона и прямое распространение",
      "lesson_objective": "Самостоятельно собрать полносвязную сеть произвольной глубины и выполнить прямой проход для батча данных.",
      "key_topics": [
        "Скрытые слои и матричные операции",
        "Инициализация весов: Xavier и He",
        "Батч-обработка для ускорения",
        "Loss-функции: MSE и кросс-энтропия",
        "Вычислительный граф и промежуточные значения"
      ],
      "exercises": [
        {
          "exercise_title": "Forward-pass векторизованным батчем",
          "exercise_instructions": "1) Реализуйте функцию forward(X, W1, b1, W2, b2) возвращающую logits и скрытый слой. 2) Используйте только NumPy, без циклов по примерам. 3) Проверьте размерности: X(64, 2), W1(2, 8), b1(8,), W2(8, 3), b2(3,) → logits(64, 3). 4) Убедитесь, что вывод совпадает с поэлементным вычислением в цикле."
        },
        {
          "exercise_title": "Инициализация весов и анализ дисперсии",
          "exercise_instructions": "1) Сгенерируйте 1000 наборов весов методами: случайные, Xavier, He. 2) Для каждого набора прогоните 1000 случайных входов размерности 100 через слой 100→64 с ReLU. 3) Постройте гистограммы выходов до и после активации. 4) Вычислите среднюю дисперсию и объясните, какая инициализация сохраняет сигнал."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обратное распространение ошибки: вывод и код",
      "lesson_objective": "Вывести градиенты для сети 2→4→3 вручную и реализовать backpropagation без автодифференциации.",
      "key_topics": [
        "Цепное правило для многих переменных",
        "Градиент loss по выходному слою",
        "Градиенты для W2, b2, скрытый слой",
        "Градиенты для W1, b1, производная ReLU",
        "Векторизация и проверка градиентов численно"
      ],
      "exercises": [
        {
          "exercise_title": "Аналитический вывод backprop",
          "exercise_instructions": "1) Напишите в ноутбуке полный вывод ∂L/∂W1 и ∂L/∂b1 для кросс-энтропии и softmax. 2) Используйте LaTeX-нотацию и поясняйте каждый шаг. 3) Подставьте размерности и убедитесь, что результат согласуется с векторизованным кодом."
        },
        {
          "exercise_title": "Численная проверка градиентов",
          "exercise_instructions": "1) Реализуйте функцию compute_numerical_gradient(f, x, eps=1e-5). 2) Сравните аналитические и численные градиенты для случайной сети 2→4→3; разница должна быть <1e-7. 3) Постройте график ошибки в зависимости от eps (1e-10 … 1e-1) и найдите оптимальное значение."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Обучение сети с нуля: полный цикл на Spiral-датасете",
      "lesson_objective": "Собрать из отдельных компонентов полный цикл обучения и достичь 95 % accuracy на тесте.",
      "key_topics": [
        "Класс MLP с методами fit и predict",
        "SGD с моментумом и адаптивным learning rate",
        "Метрики: accuracy, confusion matrix",
        "Overfitting и раннее прекращение (early stopping)",
        "Визуализация decision boundary"
      ],
      "exercises": [
        {
          "exercise_title": "Класс MLP без сторонних библиотек",
          "exercise_instructions": "1) Создайте класс MLP с конструктором layer_sizes=[2,32,3], lr=0.01. 2) Реализуйте методы _forward, _backward, _update, fit(X, y, epochs=5000, batch_size=32, verbose=100), predict(X). 3) Используйте кросс-энтропию и softmax. 4) Обучите на spiral.csv и выведите final train/test accuracy."
        },
        {
          "exercise_title": "Подбор скорости обучения на сетке",
          "exercise_instructions": "1) Попробуйте lr из множества [0.0001, 0.001, 0.01, 0.1, 1.0]. 2) Для каждого значения запустите 5 случайных инициализаций и усредните финальную accuracy. 3) Постройте кривые обучения (loss vs epoch) для лучшего и худшего lr. 4) Сохраните график как lr_comparison.png."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Регуляризация и практические советы",
      "lesson_objective": "Снизить переобучение на Spiral-датасете с 8 % до <2 % разницы между train и test accuracy, используя dropout и весовое декодирование.",
      "key_topics": [
        "L2-регуляризация (weight decay)",
        "Dropout как баггирующий ансамбль",
        "Аугментация данных и шум",
        "Выбор архитектуры: эвристика и эксперимент",
        "Сохранение и загрузка весов (numpy.save/load)"
      ],
      "exercises": [
        {
          "exercise_title": "Dropout своими руками",
          "exercise_instructions": "1) Добавьте в forward-проход маску dropout rate=0.3. 2) Масштабируйте оставшиеся значения во время обучения. 3) Отключите dropout при inference. 4) Сравните кривые обучения с/без dropout и оцените разницу accuracy на валидации."
        },
        {
          "exercise_title": "Сравнение L2 и ранней остановки",
          "exercise_instructions": "1) Зафиксируйте архитектуру 2→64→3. 2) Попробуйте λ ∈ {0, 0.0001, 0.001, 0.01}. 3) Для каждого λ стройте график train/test accuracy по эпохам и останавливайтесь, когда validation accuracy не растёт 300 эпох. 4) Заполните таблицу: λ, best epoch, train acc, test acc, gap."
        }
      ]
    }
  ]
}