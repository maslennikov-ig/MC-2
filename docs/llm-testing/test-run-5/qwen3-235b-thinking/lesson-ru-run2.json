{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Эта секция знакомит с архитектурой и принципами работы искусственных нейронных сетей. Вы изучите математические основы, процессы обучения и распространения ошибки, а также научитесь применять базовые модели к реальным задачам классификации.",
  "learning_objectives": [
    "Объяснить структуру искусственного нейрона и роль функций активации",
    "Реализовать алгоритм прямого и обратного распространения ошибки на простом перцептроне",
    "Описать процесс минимизации функции потерь с использованием градиентного спуска",
    "Оценить влияние переобучения и методов регуляризации на качество модели",
    "Применить нейронную сеть для решения задачи бинарной классификации с использованием библиотеки TensorFlow/Keras"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Архитектура искусственных нейронных сетей",
      "lesson_objective": "Построить математическую модель искусственного нейрона и продемонстрировать работу прямого распространения",
      "key_topics": [
        "Биологические аналоги искусственных нейронов",
        "Взвешенная сумма входов и порог активации",
        "Популярные функции активации (ReLU, Sigmoid, Tanh)",
        "Принцип работы многослойных перцептронов",
        "Визуализация архитектуры сети"
      ],
      "exercises": [
        {
          "exercise_title": "Моделирование нейрона",
          "exercise_instructions": "Создайте Python-функцию, реализующую искусственный нейрон с возможностью выбора функции активации (Sigmoid/ReLU). Протестируйте работу на входных данных [0.5, -1.2, 3.0] с весами [0.8, -0.5, 1.2] и порогом 0.5."
        },
        {
          "exercise_title": "Прямое распространение",
          "exercise_instructions": "Рассчитайте выходные значения для двухслойной сети вручную: 2 входа, 3 нейрона в скрытом слое (ReLU), 1 выходной нейрон (Sigmoid). Используйте заданные веса и вход [1.0, -0.5]. Сравните результат с вычислениями через NumPy."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Обучение нейронных сетей",
      "lesson_objective": "Реализовать шаг обратного распространения ошибки для однослойной сети и проанализировать влияние скорости обучения",
      "key_topics": [
        "Функции потерь (MSE, Cross-Entropy)",
        "Градиентный спуск и его модификации",
        "Алгоритм обратного распространения ошибки",
        "Выбор оптимальной скорости обучения",
        "Проблемы локальных минимумов и затухающих градиентов"
      ],
      "exercises": [
        {
          "exercise_title": "Вычисление градиентов",
          "exercise_instructions": "Для сети с 1 входом, 2 нейронами в скрытом слое (Sigmoid) и 1 выходом (MSE) рассчитайте градиенты по весам при заданных значениях: вход=0.7, целевое значение=1, текущий вывод=0.6. Используйте цепное правило дифференцирования."
        },
        {
          "exercise_title": "Шаг обратного распространения",
          "exercise_instructions": "Реализуйте на Python один шаг обратного распространения для перцептрона с функцией активации Tanh. Протестируйте на данных из предыдущего упражнения, используя скорость обучения 0.1. Выведите обновленные веса."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Практическое применение нейронных сетей",
      "lesson_objective": "Построить и обучить нейронную сеть для классификации изображений MNIST с оценкой метрик качества",
      "key_topics": [
        "Подготовка данных для обучения",
        "Выбор архитектуры сети",
        "Настройка гиперпараметров",
        "Оценка точности и матрица ошибок",
        "Визуализация процесса обучения"
      ],
      "exercises": [
        {
          "exercise_title": "Классификация MNIST",
          "exercise_instructions": "Используя Keras, создайте сеть с 1 скрытым слоем (128 нейронов, ReLU) для датасета MNIST. Обучите модель 10 эпох, постройте графики потерь и точности. Сравните результаты при использовании оптимизаторов SGD и Adam."
        }
      ]
    }
  ]
}