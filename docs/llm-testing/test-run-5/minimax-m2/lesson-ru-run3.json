{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции вы освоите базовые концепции нейронных сетей, поймёте разницу между перцептроном и многослойной сетью, изучите ключевые функции активации и принципы обратного распространения ошибки. Также вы научитесь организовывать процесс обучения и оценивать обобщающую способность модели.",
  "learning_objectives": [
    "Объяснить структуру нейрона, перцептрона и многослойной сети прямого распространения",
    "Выбрать и обосновать подходящую функцию активации и функцию потерь для задачи регрессии/классификации",
    "Вручную пройти прямой и обратный проход для простых моделей и проверить градиенты численно",
    "Реализовать цикл обучения нейронной сети с логированием метрик и ранней остановкой",
    "Провести базовые эксперименты по подбору гиперпараметров и оценить переобучение/недообучение"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От нейрона к перцептрону",
      "lesson_objective": "По завершении урока вы объясните разницу между биологическим нейроном и математической моделью, построите перцептрон для линейно разделимых данных и примените правило обновления весов.",
      "key_topics": [
        "Биологический нейрон vs математическая модель",
        "Перцептрон Розенблатта и его геометрическая интерпретация",
        "Функции активации: сигмоида, tanh, ReLU",
        "Функции потерь: MSE и кросс-энтропия",
        "Градиентный спуск и стохастический градиентный спуск (SGD)",
        "Сигналы: предсказание, ошибка, обновление весов"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчёт перцептрона и обновление весов",
          "exercise_instructions": "Дано: веса w = [0.2, -0.5], порог b = 0.1; вход x = [2, 1]; пороговая функция активации θ(z)=1 при z>=0 иначе 0. Шаги: 1) Вычислите z = w·x + b и выход y_hat. 2) Дайте истинный класс y = 1 и посчитайте ошибку e = y - y_hat. 3) Обновите веса по правилу перцептрона: w' = w + η·e·x, b' = b + η·e (η=0.5). 4) Выпишите итоговые w', b' и коротко объясните смысл обновления для данного примера."
        },
        {
          "exercise_title": "Сравнение активаций на одном примере",
          "exercise_instructions": "Используя x = [-1, 0.5] и веса w = [0.8, -0.3], порог b = 0.2, вычислите выходы для сигмоиды σ(z), tanh(z) и ReLU(z). Затем кратко обсудите, почему ReLU чаще помогает избежать затухания градиента в глубоких сетях."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Обратное распространение ошибки и цикл обучения",
      "lesson_objective": "По завершении урока вы выведете градиенты простой функции потерь по параметрам и реализуете базовый цикл обучения с логированием метрик.",
      "key_topics": "Обратное распространение ошибки и цепное правило; производные функций активации и потерь; инициализация весов; размер батча и learning rate; логирование метрик и мониторинг процесса обучения; проверка градиентов конечными разностями; критерии остановки и переобучение",
      "exercises": [
        {
          "exercise_title": "Ручной расчёт градиентов для MSE с линейной моделью",
          "exercise_instructions": "Дано: w_scalar=2, b=0.5; данные x=[1,2,3], y_true=[3,5,7]. Шаги: 1) Вычислите y_pred = w·x + b. 2) Найдите ошибки e_i = y_pred_i - y_true_i и MSE = (1/n)·Σe_i^2. 3) Выведите аналитически ∂MSE/∂w = (2/n)·Σe_i·x_i и ∂MSE/∂b = (2/n)·Σe_i. 4) Подставьте числа и найдите градиенты; округлите до 3 знаков. 5) Объясните, как использовать эти градиенты в одном шаге градиентного спуска с η=0.01."
        },
        {
          "exercise_title": "Реализация простого цикла обучения",
          "exercise_instructions": "Реализуйте на Python цикл обучения для модели y = w·x + b (MSE) на синтетических данных: сгенерируйте 100 точек y = 2x + 1 + шум; разбейте на train/val 80/20; используйте η=0.01, 300 эпох, batch_size=16. Шаги: 1) Инициализируйте w,b случайно; 2) В каждой эпохе перемешайте train; 3) Пройдите батчами, обновите параметры по градиентам MSE; 4) Валидируйте на val и логируйте train_loss, val_loss, val_RMSE; 5) Приложите графики train_loss и val_RMSE по эпохам и коротко интерпретируйте динамику."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Стабильность обучения и обобщение",
      "lesson_objective": "По завершении урока вы организуете эксперименты с гиперпараметрами, выявите признаки переобучения/недообучения и примените регуляризацию для улучшения обобщения.",
      "key_topics": [
        "Влияние learning rate, размера батча и инициализации",
        "Переобучение и недообучение: признаки и способы обнаружения",
        "Регуляризация: L1/L2, ранняя остановка, сокращение весов",
        "Валидационный набор и кросс-валидация",
        "Метрики качества: точность, полнота, F1, ROC-AUC",
        "План экспериментов и сравнение моделей"
      ],
      "exercises": [
        {
          "exercise_title": "Анализ переобучения и регуляризация",
          "exercise_instructions": "На данных для бинарной классификации (например, синтетических или «игрушечных» из sklearn) обучите логистическую регрессию или простую нейросеть. Шаги: 1) Разделите данные на train/val/test 60/20/20. 2) Обучите модель без регуляризации 50 эпох и постройте графики train_loss и val_accuracy; 3) Выберите модель с явным переобучением (валид снижается/колеблется при росте train). 4) Добавьте L2-регуляризацию с коэффициентом λ из [0.001, 0.01, 0.1] или early stopping по val_loss; 5) Сравните финальные метрики на test, объясните, какая настройка дала лучший баланс bias/variance."
        },
        {
          "exercise_title": "Эксперимент с гиперпараметрами",
          "exercise_instructions": "Проведите поиск по сетке или случайный поиск для простой нейросети (1–2 скрытых слоя) на классификационном датасете. Шаги: 1) Зафиксируйте train/val/test, используйте раннюю остановку по val_accuracy с patience=5. 2) Переберите lr ∈ {1e-4, 5e-4, 1e-3}, batch_size ∈ {32, 64}, λ(L2) ∈ {0, 1e-5, 1e-4, 1e-3}. 3) Оцените каждую конфигурацию на val и выберите топ-1. 4) Оцените лучшую модель на test; приложите таблицу результатов и краткое обоснование выбора."
        }
      ]
    }
  ]
}