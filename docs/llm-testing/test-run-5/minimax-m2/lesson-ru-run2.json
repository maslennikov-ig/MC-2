{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции вы изучите фундаментальные принципы работы нейронных сетей: как устроен нейрон, как работает прямой проход, как модели обучаются на данных и как оценивать их качество. Секция объединяет теорию и практику с упором на понятные объяснения и воспроизводимые упражнения.",
  "learning_objectives": [
    "Объяснить структуру нейрона и объяснить функции активации (ReLU, сигмоида, гиперболический тангенс) на примерах",
    "Выполнить прямой проход для однослойного персептрона и простой сети прямого распространения (numpy/Keras)",
    "Реализовать вручную простую функцию потерь и её градиент для задачи бинарной классификации",
    "Обучить базовую нейросеть с Keras, настроить веса и смещения, интерпретировать кривые обучения",
    "Оценить качество модели на валидационном наборе и диагностировать переобучение/недообучение по метрикам и графикам"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Что такое нейрон: модель, функции активации и прямой проход",
      "lesson_objective": "Завершив урок, вы сможете объяснить устройство нейрона, выбрать и применить подходящую функцию активации, а также выполнить прямой проход для однослойного персептрона и двухслойной сети на данных MNIST",
      "key_topics": [
        "Биологический и математический нейрон: сумматор и активация",
        "Функции активации: ReLU, сигмоида, гиперболический тангенс, softmax",
        "Однослойный персептрон: формула прямого прохода",
        "Многослойные сети прямого распространения: архитектура и прямой проход",
        "Интерпретация форм функций активации и их градиентов"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация персептрона на чистом numpy",
          "exercise_instructions": "Сгенерируйте синтетический набор данных для бинарной классификации (например, make_classification с 2 классами, n_samples=200). Реализуйте на numpy класс Perceptron с прямой функцией forward(x), которая считает z = W·x + b и a = sigmoid(z). Выполните прямой проход на 10 объектах, выведите вероятности и форму тензоров W, x, b."
        },
        {
          "exercise_title": "Прямой проход на MNIST с Keras (без обучения)",
          "exercise_instructions": "Загрузите набор данных MNIST (keras.datasets). Нормализуйте пиксели в [0, 1] и преобразуйте метки в one-hot. Соберите Sequential-модель: Dense(128, activation='relu'), Dense(10, activation='softmax'). Выполните model.compile с любыми параметрами (optimizer='adam', loss='categorical_crossentropy') и сделайте один шаг предсказания model.predict на батче из 32 изображений. Выведите форму выхода и убедитесь, что сумма по классу равна 1."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции потерь и градиенты: обучаемость и оптимизация",
      "lesson_objective": "Завершив урок, вы сможете выбрать подходящую функцию потерь, реализовать её и её градиент для бинарной классификации, а также объяснить влияние темпа обучения на сходимость",
      "key_topics": [
        "Кросс-энтропия и MSE: различия, области применения",
        "Градиентный спуск: интуиция и шаги обновления параметров",
        "Темп обучения (learning rate) и его влияние на обучение",
        "Векторные формы вычислений и проверка размерностей",
        "Стабильность численных вычислений для логарифмов"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация бинарной кросс-энтропии и её градиента",
          "exercise_instructions": "На синтетических данных из Упражнения 1 реализуйте функции binary_crossentropy(y_true, y_pred) и its gradient dL/dz для одного батча (numpy). Убедитесь, что y_pred лежат в (0, 1). Проверьте численную стабильность через добавление epsilon=1e-7 к y_pred перед log. Выведите среднее значение лосса на батче и форму градиента."
        },
        {
          "exercise_title": "Один шаг градиентного спуска для персептрона",
          "exercise_instructions": "Используя реализованную в предыдущем упражнении функцию потерь и градиент, выполните один шаг обновления параметров W и b: W = W - lr * dL/dW, b = b - lr * dL/db, где lr = 0.01. Выведите изменения норм ||W|| и |b| до и после шага и объясните, как выбор lr влияет на величину обновлений."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Практика обучения: Keras, метрики и кривые обучения",
      "lesson_objective": "Завершив урок, вы сможете собрать и обучить нейросеть в Keras, настроить параметры компиляции, вывести метрики качества и построить кривые обучения",
      "key_topics": [
        "Архитектура Sequential и настройка слоёв",
        "Компиляция модели: выбор оптимизатора, лосса и метрик",
        "Обучение model.fit: эпохи, батч-сайз, валидационный набор",
        "Валидация и метрики: точность, precision, recall, F1",
        "История обучения: лосс и метрики по эпохам"
      ],
      "exercises": [
        {
          "exercise_title": "Обучение модели на MNIST в Keras",
          "exercise_instructions": "На нормализованном наборе MNIST соберите модель: Dense(128, activation='relu'), Dense(10, activation='softmax'). Выполните model.compile с optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']. Обучите 5 эпох с batch_size=128 и validation_split=0.1. Выведите итоговые значения val_loss и val_accuracy после последней эпохи."
        },
        {
          "exercise_title": "График кривых обучения",
          "exercise_instructions": "Используя history = model.fit(...).history, постройте графики train_loss и val_loss по эпохам на одном рисунке и train_accuracy и val_accuracy — на другом. Сохраните графики как PNG. Интерпретируйте: есть ли переобучение или недообучение? Предложите одно практическое изменение (например, увеличить регуляризацию или добавить эпох) и объясните ожидаемый эффект."
        },
        {
          "exercise_title": "Эксперимент с размером батча и темпом обучения",
          "exercise_instructions": "Повторите обучение из упражнения 1 дважды: (а) batch_size=32, lr по умолчанию; (б) batch_size=256, optimizer=keras.optimizers.SGD(learning_rate=0.05). Для каждого варианта сравните скорость стабилизации val_loss и итоговую точность. Сделайте вывод о влиянии размера батча и темпа обучения на динамику обучения."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Регуляризация и устойчивость: dropout, weight decay и ранняя остановка",
      "lesson_objective": "Завершив урок, вы сможете применить L2-регуляризацию и dropout, построить кривые лосса, и применить раннюю остановку для повышения обобщающей способности модели",
      "key_topics": [
        "Переобучение и недообучение: признаки и диагностика",
        "L2-регуляризация (weight decay) и её параметр lambda",
        "Dropout: вероятность отключения и влияние на обучение",
        "Кривые обучения: анализ разрыва между тренингом и валидацией",
        "Ранняя остановка (EarlyStopping) и мониторинг метрик"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение базовой модели и модели с регуляризацией",
          "exercise_instructions": "На MNIST обучите две модели: (а) базовая Dense(128,relu)+Dense(10,softmax); (б) с L2-регуляризацией на Dense-слое (kernel_regularizer=l2(1e-4)) и Dropout(0.2) после первого Dense. Обе модели обучите 10 эпох с validation_split=0.1. Сравните итоговые val_loss и val_accuracy; объясните, где наблюдается переобучение и как регуляризация повлияла на результат."
        },
        {
          "exercise_title": "Ранняя остановка по валидационному лоссу",
          "exercise_instructions": "Повторите обучение базовой модели из предыдущего упражнения, но с EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1) и обучите до 20 эпох. Выведите число эпох до остановки и финальные метрики на валидации. Объясните, почему early stopping помогает уменьшить переобучение и когда её применять."
        }
      ]
    }
  ]
}