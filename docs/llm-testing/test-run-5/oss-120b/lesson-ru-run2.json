{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этом модуле рассматриваются базовые принципы работы нейронных сетей, их архитектура и процесс обучения. Вы научитесь строить простейшие сети и понимать, как они решают задачи классификации и регрессии.",
  "learning_objectives": [
    "Определить основные компоненты нейронной сети и их функции",
    "Построить и обучить простую полносвязную сеть в Python с использованием Keras",
    "Проанализировать влияние гиперпараметров (количество слоёв, нейронов, функции активации) на качество модели",
    "Визуализировать процесс обучения и оценить переобучение с помощью графиков потерь"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Структура нейрона и функции активации",
      "lesson_objective": "Объяснить роль входных весов, смещения и функции активации в работе отдельного нейрона",
      "key_topics": [
        "Входные веса и смещение",
        "Линейная комбинация",
        "Функции активации (sigmoid, ReLU, tanh)",
        "Преобразование сигнала",
        "Градиентный спуск для нейрона"
      ],
      "exercises": [
        {
          "exercise_title": "Рассчитайте выход нейрона вручную",
          "exercise_instructions": "1. Возьмите вектор входов x = [0.5, -1.2, 0.3] и веса w = [0.8, -0.5, 0.2].\n2. Добавьте смещение b = 0.1.\n3. Вычислите линейную комбинацию z = w·x + b.\n4. Примените функцию активации sigmoid: σ(z) = 1 / (1 + e^(-z)).\n5. Запишите полученный результат."
        },
        {
          "exercise_title": "Сравнение функций активации в Python",
          "exercise_instructions": "1. Откройте Jupyter Notebook.\n2. Сгенерируйте массив значений x от -5 до 5 с шагом 0.1.\n3. Постройте графики sigmoid, ReLU и tanh на одной диаграмме.\n4. Добавьте легенду и подписи осей.\n5. Сделайте вывод, при каких диапазонах x каждая функция полезна."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Однослойный перцептрон и задача классификации",
      "lesson_objective": "Построить и обучить однослойный перцептрон для бинарной классификации на датасете Iris",
      "key_topics": [
        "Однослойный перцептрон",
        "Бинарная кросс-энтропия",
        "Алгоритм обратного распространения ошибки",
        "Метрика точности",
        "Разделение данных на train/validation"
      ],
      "exercises": [
        {
          "exercise_title": "Имплементация перцептрона в Keras",
          "exercise_instructions": "1. Загрузите датасет Iris и отберите два класса (setosa и versicolor).\n2. Преобразуйте метки в 0 и 1.\n3. Разделите данные 80/20 на train и test.\n4. Создайте модель Sequential с одним Dense слоем (1 нейрон, activation='sigmoid').\n5. Скомпилируйте модель с loss='binary_crossentropy' и optimizer='sgd'.\n6. Обучите модель 100 эпох, batch_size=16.\n7. Оцените точность на тестовом наборе и запишите результат."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Многослойные перцептроны (MLP) и регрессия",
      "lesson_objective": "Создать MLP с двумя скрытыми слоями для предсказания цены дома в наборе Boston Housing",
      "key_topics": [
        "Многослойный перцептрон",
        "Функции активации ReLU и линейные выходы",
        "Среднеквадратичная ошибка (MSE)",
        "Регуляризация (Dropout, L2)",
        "Нормализация входных признаков"
      ],
      "exercises": [
        {
          "exercise_title": "Построение MLP в TensorFlow",
          "exercise_instructions": "1. Загрузите набор Boston Housing из keras.datasets.\n2. Нормализуйте признаки (StandardScaler).\n3. Разделите данные на train/validation (75/25).\n4. Создайте модель Sequential с тремя Dense слоями: 64‑ReLU, 32‑ReLU, 1‑linear.\n5. Добавьте Dropout (0.2) после каждого скрытого слоя.\n6. Скомпилируйте модель с loss='mse' и optimizer='adam'.\n7. Обучите 200 эпох, early stopping на validation loss.\n8. Выведите окончательное значение MSE на тестовом наборе."
        },
        {
          "exercise_title": "Исследование влияния количества нейронов",
          "exercise_instructions": "1. Скопируйте код из предыдущего упражнения.\n2. Измените количество нейронов в первом скрытом слое на 32, 128 и 256 (по отдельности).\n3. Для каждой конфигурации обучите модель 100 эпох.\n4. Запишите значения MSE на валидации.\n5. Сравните результаты и сделайте вывод, как размер слоя влияет на переобучение."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Проблема переобучения и методы регуляризации",
      "lesson_objective": "Продемонстрировать переобучение на небольшом наборе и применить L2‑регуляризацию и раннюю остановку для его снижения",
      "key_topics": [
        "Переобучение (overfitting)",
        "Кросс‑валидация",
        "L2‑регуляризация (weight decay)",
        "Ранняя остановка (early stopping)",
        "Визуализация кривых обучения"
      ],
      "exercises": [
        {
          "exercise_title": "Обучение без регуляризации",
          "exercise_instructions": "1. Сгенерируйте синтетический набор данных: 200 точек, 1 вход, шум σ=0.5.\n2. Постройте MLP с двумя скрытыми слоями (64, 64) без регуляризации.\n3. Обучите 500 эпох, запоминая loss для train и validation.\n4. Постройте графики потерь.\n5. Оцените разницу между train и validation loss."
        },
        {
          "exercise_title": "Применение L2 и early stopping",
          "exercise_instructions": "1. Возьмите тот же набор данных.\n2. Добавьте L2‑регуляризацию (lambda=0.01) к Dense слоям.\n3. Включите EarlyStopping (monitor='val_loss', patience=10).\n4. Обучите модель, сохранив лучшие веса.\n5. Сравните новые графики потерь с предыдущими и запишите улучшения в MSE."
        }
      ]
    }
  ]
}