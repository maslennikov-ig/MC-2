{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции рассматриваются фундаментальные принципы нейронных сетей, их архитектура и методы обучения. Студенты получат практические навыки построения простых моделей и понимание их поведения.",
  "learning_objectives": [
    "Определять основные компоненты нейронной сети и их функции.",
    "Объяснять процесс прямого распространения и обратного распространения ошибки.",
    "Сравнивать различные функции активации и их влияние на обучение.",
    "Разрабатывать и обучать простую многослойную перцептронную модель в TensorFlow/Keras."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Перцептрон и линейные классификаторы",
      "lesson_objective": "Сформулировать математическую модель перцептрона и реализовать её для решения задачи бинарной классификации.",
      "key_topics": [
        "История и идея перцептрона",
        "Линейная граница решений",
        "Алгоритм обучения (правило Хебба)",
        "Ограничения однослойных моделей"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона с нуля",
          "exercise_instructions": "1. Сгенерировать набор данных с двумя линейно разделимыми классами (например, с помощью make_blobs). 2. Написать функцию для предсказания метки на основе веса и порога. 3. Реализовать цикл обучения, обновляющий веса согласно правилу перцептрона. 4. Визуализировать данные и полученную границу решений."
        },
        {
          "exercise_title": "Анализ сходимости",
          "exercise_instructions": "1. Запустить обучение перцептрона на разных начальных значениях весов. 2. Зафиксировать количество итераций до полного классифицирования. 3. Составить таблицу зависимости количества итераций от начального веса и обсудить результаты."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Многослойный перцептрон и обратное распространение",
      "lesson_objective": "Объяснить принцип работы многослойного перцептрона и реализовать алгоритм обратного распространения ошибки для обучения сети.",
      "key_topics": [
        "Структура MLP (слои, нейроны)",
        "Формула прямого распространения",
        "Градиентный спуск и обратное распространение",
        "Проблема исчезающего градиента"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация MLP с одной скрытой слойой",
          "exercise_instructions": "1. Создать класс NeuralNetwork с методами forward и backward. 2. Использовать сигмоидную функцию активации. 3. Обучить сеть на задаче XOR, фиксируя параметры обучения (learning_rate=0.1, epochs=5000). 4. Вывести финальные предсказания и сравнить с целевыми метками."
        },
        {
          "exercise_title": "Визуализация градиентов",
          "exercise_instructions": "1. Во время обучения MLP сохранять величину градиентов весов каждого слоя каждые 100 эпох. 2. Построить графики изменения нормы градиентов во времени. 3. Сделать вывод о стабильности обучения."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции активации и регуляризация",
      "lesson_objective": "Сравнить влияние различных функций активации и техник регуляризации на качество обучения нейронных сетей.",
      "key_topics": [
        "ReLU, Leaky ReLU, tanh, softmax",
        "L1 и L2 регуляризация",
        "Dropout",
        "Batch Normalization"
      ],
      "exercises": [
        {
          "exercise_title": "Эксперимент с функциями активации",
          "exercise_instructions": "1. Построить одну и ту же MLP-архитектуру (2 скрытых слоя по 64 нейрона). 2. Обучить модель три раза, меняя функцию активации: ReLU, tanh, Leaky ReLU. 3. Оценить точность на валидационном наборе и записать результаты в таблицу."
        },
        {
          "exercise_title": "Применение Dropout и L2",
          "exercise_instructions": "1. Добавить слой Dropout (rate=0.5) после каждого скрытого слоя в предыдущей модели. 2. Включить L2‑регуляризацию (lambda=0.01) в функцию потерь. 3. Обучить модель и сравнить метрики с базовой моделью без регуляризации."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Практическая реализация в TensorFlow/Keras",
      "lesson_objective": "Создать, обучить и оценить нейронную сеть в Keras для решения задачи классификации изображений (MNIST).",
      "key_topics": [
        "Установка и импорт TensorFlow",
        "Создание Sequential модели",
        "Компиляция модели (optimizer, loss, metrics)",
        "Обучение (fit) и оценка (evaluate)",
        "Сохранение и загрузка модели"
      ],
      "exercises": [
        {
          "exercise_title": "Построение модели для MNIST",
          "exercise_instructions": "1. Загрузить набор данных MNIST через tf.keras.datasets. 2. Нормализовать пиксели к диапазону [0,1]. 3. Сконструировать Sequential модель: Flatten → Dense(128, activation='relu') → Dropout(0.2) → Dense(10, activation='softmax'). 4. Скомпилировать модель с optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']. 5. Обучить модель 5 эпох, используя 10% тренировочных данных как validation split."
        },
        {
          "exercise_title": "Оценка и сохранение модели",
          "exercise_instructions": "1. Оценить обученную модель на тестовом наборе и записать полученную точность. 2. Сохранить модель в файл формата HDF5 (model.save('mnist_model.h5')). 3. Загрузить модель из файла и убедиться, что точность воспроизводится."
        }
      ]
    }
  ]
}