{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Концептуальный теоретический раздел с примерами, объясняющий устройство нейронов, функции активации, обучение и регуляризацию с практическими упражнениями.",
  "learning_objectives": [
    "Различать основные архитектуры нейронных сетей и их применения",
    "Объяснять различия между функциями активации и их влияние на обучение",
    "Формулировать процесс обучения как оптимизацию функции потерь с градиентными методами",
    "Применять L1/L2-регуляризацию и понимать эффект переобучения"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Нейрон как базовая единица",
      "lesson_objective": "Вычислить выход нейрона и частные производные вручную для заданных весов и входа",
      "key_topics": [
        "Структура нейрона: веса, сумматор, функция активации",
        "Прямой проход и вычисление выхода",
        "Производные ReLU и сигмоиды",
        "Примеры расчётов для одиночного нейрона"
      ],
      "exercises": [
        {
          "exercise_title": "Расчёт выхода нейрона с ReLU",
          "exercise_instructions": "Для входа x=[1.0, -0.5], весов w=[0.8, -0.3], смещения b=0.2 вычислите выход y, покажите все шаги и округлите результат до четырёх знаков после запятой."
        },
        {
          "exercise_title": "Градиент по весам для нейрона с ReLU",
          "exercise_instructions": "Для выхода z=1.5 нейрона с функцией активации ReLU и входом x=[2, -1] вычислите производные dL/dw1 и dL/dw2, считая dL/dy=1. Покажите расчёты."
        },
        {
          "exercise_title": "График функции ReLU",
          "exercise_instructions": "Постройте график функции активации ReLU на диапазоне x∈[-3, 3] и сохраните изображение как relu_plot.png. В отчёте укажите диапазон выходов и точки перегиба."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации: сигмоида, tanh, ReLU",
      "lesson_objective": "Сравнить свойства функций активации и вычислить их значения и градиенты",
      "key_topics": [
        "Определения сигмоиды, tanh и ReLU",
        "Графики, диапазоны и поведение при больших |x|",
        "Проблемы насыщения и затухающих градиентов",
        "Примеры расчёта значений и производных"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение активаций на одном входе",
          "exercise_instructions": "Для входа x=1.0 вычислите значения сигмоиды, tanh и ReLU, округлите до трёх знаков после запятой и объясните различия."
        },
        {
          "exercise_title": "Построение графиков функций активации",
          "exercise_instructions": "Постройте на одном графике сигмоиду, tanh и ReLU на диапазоне x∈[-4, 4]. Сохраните файл как activations.png и опишите области насыщения каждой функции."
        },
        {
          "exercise_title": "Градиент softmax",
          "exercise_instructions": "Для вектора z=[1.0, 2.0, 0.5] вычислите значения softmax, а затем производные ∂s1/∂z2. Покажите формулы и численный результат с точностью до четырёх знаков."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции потерь и градиентный спуск",
      "lesson_objective": "Применить MSE/кросс-энтропию и градиентные шаги к однослойной сети",
      "key_topics": [
        "Среднеквадратичная ошибка (MSE) и кросс-энтропия",
        "Градиентный спуск и стохастический градиентный спуск (SGD)",
        "Прямой и обратный проходы для двухслойной сети",
        "Практика на численных примерах"
      ],
      "exercises": [
        {
          "exercise_title": "Градиенты MSE для линейного нейрона",
          "exercise_instructions": "Для линейного нейрона с w=[2, -1], b=0.5, входа x=[1, 2] и целевого значения y=3 вычислите MSE и производные ∂L/∂w и ∂L/∂b."
        },
        {
          "exercise_title": "Два шага SGD на игрушечном примере",
          "exercise_instructions": "Для однослойной сети с w=[0.5, -0.2], b=0.1, входа x=[1, 1] и цели y=1 выполните два шага SGD с learning rate=0.1, покажите w и b после каждого шага."
        },
        {
          "exercise_title": "Векторизация и обратный проход",
          "exercise_instructions": "Для скрытого слоя h=ReLU(x·W1+b1) и выхода y=sigmoid(h·W2+b2) запишите матричные формы прямого прохода и формулы градиентов dL/dW2, dL/dW1 с псевдокодом."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Переобучение и регуляризация",
      "lesson_objective": "Вычислить штрафы L1/L2 и предложить стратегии борьбы с переобучением",
      "key_topics": [
        "Понятие переобучения и недообучения",
        "L1 и L2 регуляризация, смысл гиперпараметра λ",
        "Ранняя остановка (early stopping) и dropout",
        "Анализ кривых обучения"
      ],
      "exercises": [
        {
          "exercise_title": "Расчёт штрафов L1 и L2",
          "exercise_instructions": "Для весов w=[2.0, -0.5, 0.8] и λ=0.1 вычислите L1 и L2 штрафы, затем L2 штраф при обнулении веса w2=-0.5. Покажите расчёты."
        },
        {
          "exercise_title": "Анализ кривых обучения",
          "exercise_instructions": "Постройте учебные графики потерь на train и val по эпохам и опишите, где начинается переобучение. Предложите два способа борьбы и обоснуйте выбор."
        },
        {
          "exercise_title": "Эффект регуляризации на веса",
          "exercise_instructions": "Объясните, как увеличение λ влияет на распределение весов и способы подбора λ через валидацию. Приведите пример диапазона λ и критерий остановки."
        }
      ]
    }
  ]
}