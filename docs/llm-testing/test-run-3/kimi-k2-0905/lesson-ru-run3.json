{
  "section_number": 1,
  "section_title": "Основы нейронных сетей: от нейрона к глубокому обучению",
  "section_description": "Разберёмся, как устроен искусственный нейрон, как он учится и почему слои превращают простую модель в мощный инструмент машинного обучения.",
  "learning_objectives": [
    "Объяснить биологическую и математическую модель нейрона своими словами",
    "Вычислить выход логистического нейрона вручную для заданных весов и входов",
    "Сравнить алгоритм обратного распространения ошибки с градиентным спуском и указать 2 ключевых отличия",
    "Собрать 3-слойную сеть на Python с нуля и достичь ≥90 % точности на XOR",
    "Проанализировать 3 типичные проблемы обучения (переобучение, недообучение, затухание градиента) и предложить решения"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному: история и модель",
      "lesson_objective": "Сформулировать 4 основных этапа превращения биологического нейрона в математическую модель и записать уравнение активации",
      "key_topics": [
        "История McCulloch-Pitts и перцептрон Розенблатта",
        "Структура биологического нейрона: дендриты, сома, аксон",
        "Математическая модель: взвешенная сумма + функция активации",
        "Пороговая, сигмоидная, ReLU-функции: графики и производные"
      ],
      "exercises": [
        {
          "exercise_title": "Постройте графики 5 функций активации вручную",
          "exercise_instructions": "На миллиметровке начертите пороговую, сигмоиду, tanh, ReLU и Leaky-ReLU для x от –3 до 3. Подпишите точки перегиба и наклоны."
        },
        {
          "exercise_title": "Калькулятор нейрона в Excel",
          "exercise_instructions": "В таблице задайте 3 входа, 3 веса и смещение. Создайте формулу, вычисляющую выход сигмоидального нейрона. Проверьте при весах [0.5, –1.2, 0.7] и входе [1, 0, 1]."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Архитектура многослойного перцептрона: как слои создают смысл",
      "lesson_objective": "Нарисовать схему 3-х слойного MLP, подписать размерности матриц между слоями и объяснить, почему без скрытого слоя XOR нерешаем",
      "key_topics": [
        "Полносвязные vs свёрточные vs рекуррентные архитектуры",
        "Теорема универсальной аппроксимации Cybenko",
        "Размерности тензоров: [batch, input] → [batch, hidden] → [batch, output]",
        "Инициализация весов: Xavier, He, случайное нормальное"
      ],
      "exercises": [
        {
          "exercise_title": "Считаем параметры вручную",
          "exercise_instructions": "Дана сеть 4→7→3. Посчитайте общее число обучаемых параметров и запишите формулу для случая N→H→M."
        },
        {
          "exercise_title": "Почему XOR требует скрытый слой?",
          "exercise_instructions": "На листе в клетку нарисуйте таблицу истинности XOR и покажите, что никакая прямая не разделяет классы. Затем добавьте 2 скрытые точки и проведите линии."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функция потерь и обратное распространение: математика обучения",
      "lesson_objective": "Вывести градиенты для MSE и кросс-энтропии по весам последнего слоя и проверить их на численном примере 2×2",
      "key_topics": [
        "MSE vs кросс-энтропия: когда какую использовать",
        "Цепное правило дифференцирования для слоёв",
        "Алгоритм backprop: forward pass → loss → backward pass",
        "Вычислительный граф и автоматическое дифференцирование"
      ],
      "exercises": [
        {
          "exercise_title": "Backprop вручную для 2-2-1 сети",
          "exercise_instructions": "Дана сеть 2→2→1 с сигмоидой everywhere. Заданы вход [0.5, –1], цель 1. Проведите forward pass, вычислите MSE, затем найдите ∂L/∂w для всех 9 весов."
        },
        {
          "exercise_title": "Численная проверка градиента",
          "exercise_instructions": "Реализуйте функцию, которая для заданного веса w вычисляет численный градиент (f(w+ε)–f(w–ε))/2ε и сравнивает с аналитическим. Отчёт: относительная ошибка <1e-5."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Практика: обучаем двухслойную сеть на Python с нуля",
      "lesson_objective": "Написать 50-строчный NumPy-скрипт, который обучает 2-слойную сеть на датасете XOR и выводит точность ≥ 98 % за ≤ 5000 эпох",
      "key_topics": [
        "Классическая структура кода: класс DenseLayer, forward, backward",
        "Стохастический градиентный спуск с мини-батчами 4",
        "Гиперпараметры: learning rate, число эпох, размер батча",
        "Визуализация потерь и accuracy во времени"
      ],
      "exercises": [
        {
          "exercise_title": "Реализуйте слой Dense и функции потерь",
          "exercise_instructions": "Создайте файл mlp.py. Реализуйте класс Dense с инициализацией Xavier, forward и backward методами. Проверьте на батче 2×2."
        },
        {
          "exercise_title": "Обучите сеть на circles-датасете",
          "exercise_instructions": "Сгенерируйте 1000 точек в двух кольцах. Обучите 2→8→1 сеть, постройте decision boundary. Добейтесь ≥ 95 % точности."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Проблемы обучения и их решения: переобучение, затухание, взрыв градиента",
      "lesson_objective": "Перечислить 3 симптома переобучения, 2 метода регуляризации и 1 технику борьбы с затуханием градиента, продемонстрировать на графике",
      "key_topics": [
        "Кривые обучения: train vs validation loss",
        "L1 и L2 регуляризация: формулы и влияние на веса",
        "Dropout как ансамбль: инверсия при тесте",
        "BatchNorm: нормализация и обучаемые параметры"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение L1 и L2 на синтетических данных",
          "exercise_instructions": "Сгенерируйте 100 признаков, 5 полезных. Обучите логистическую регрессию с L1 и L2. Сравните число нулевых весов и точность."
        },
        {
          "exercise_title": "Визуализируйте эффект Dropout",
          "exercise_instructions": "Обучите 3 сети на MNIST: без dropout, dropout 0.5, dropout 0.8. Постройте кривые потерь и точности, сделайте вывод в 2 предложения."
        }
      ]
    }
  ]
}