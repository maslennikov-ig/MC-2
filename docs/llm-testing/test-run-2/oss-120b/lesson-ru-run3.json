{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел посвящён теоретическим основам нейронных сетей: от математических принципов до практических примеров их применения. Студенты изучат линейную алгебру, функции активации, алгоритмы обучения, типичные архитектуры и способы оценки качества моделей.",
  "learning_objectives": [
    "Студент сможет описать фундаментальные математические концепции, лежащие в основе нейронных сетей",
    "Студент сможет построить простейшие архитектуры нейронных сетей и объяснить их работу",
    "Студент сможет реализовать алгоритм обратного распространения ошибки и настроить гиперпараметры обучения",
    "Студент сможет применить обученную нейронную сеть к задаче классификации и интерпретировать полученные результаты"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Математические основы нейронных сетей",
      "lesson_objective": "Студент сможет объяснить основные математические принципы, лежащие в основе нейронных сетей, включая линейную алгебру, функции активации и градиентный спуск.",
      "key_topics": [
        "Векторные представления и операции над ними",
        "Матрицы весов и их роль в вычислениях",
        "Функции активации: сигмоида, гиперболический тангенс, ReLU",
        "Градиентный спуск и правило обновления весов"
      ],
      "exercises": [
        {
          "exercise_title": "Вычисление выхода одиночного нейрона",
          "exercise_instructions": "Даны входные значения x1=0.5, x2=‑0.3, веса w1=0.8, w2=‑0.5 и порог b=0.1. Вычислите линейную комбинацию, примените функцию активации ReLU и запишите результат."
        },
        {
          "exercise_title": "Градиентный спуск на простом примере",
          "exercise_instructions": "Для функции потерь L = (y‑ŷ)^2, где y=1, а ŷ = σ(w·x+b) с σ – сигмоида, вычислите градиент по w и b при x=0.7, w=0.4, b=‑0.2 и выполните один шаг обновления с шагом обучения η=0.1."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Архитектуры простых нейронных сетей",
      "lesson_objective": "Студент сможет построить и описать однослойный и многослойный перцептрон, а также понять их ограничения.",
      "key_topics": [
        "Однослойный перцептрон и задача линейной разделимости",
        "Многослойный перцептрон (MLP) с одним скрытым слоем",
        "Роль скрытых нейронов и нелинейных функций активации",
        "Функция потерь для задач классификации"
      ],
      "exercises": [
        {
          "exercise_title": "Конструирование однослойного перцептрона",
          "exercise_instructions": "Сформулируйте уравнение решения задачи AND для двух бинарных входов, задав соответствующие веса и порог. Проверьте правильность работы на всех комбинациях входов."
        },
        {
          "exercise_title": "Рисование схемы MLP",
          "exercise_instructions": "Нарисуйте схему многослойного перцептрона с 3 входными нейронами, 2 скрытыми нейронами и 1 выходным нейроном. Укажите, какие функции активации вы бы использовали на каждом слое и почему."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обучение нейронных сетей: алгоритмы и практики",
      "lesson_objective": "Студент сможет применить алгоритм обратного распространения ошибки для обучения сети и настроить основные гиперпараметры.",
      "key_topics": [
        "Алгоритм backpropagation: вычисление градиентов по слоям",
        "Обновление весов с учётом скорости обучения",
        "Регуляризация: L2, Dropout, ранняя остановка",
        "Выбор и настройка гиперпараметров (learning rate, batch size, epochs)"
      ],
      "exercises": [
        {
          "exercise_title": "Псевдокод backpropagation для MLP",
          "exercise_instructions": "Составьте пошаговый псевдокод алгоритма обратного распространения ошибки для сети с одним скрытым слоем. Опишите, как вычисляются градиенты для весов входного и скрытого слоёв."
        },
        {
          "exercise_title": "Эксперимент с learning rate",
          "exercise_instructions": "Возьмите простую задачу регрессии y = 2x + 1. Обучите однослойную сеть с двумя разными значениями скорости обучения (η=0.01 и η=0.5). Сравните скорость сходимости и зафиксируйте полученные ошибки после 100 эпох."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Применение нейронных сетей к задаче классификации изображений",
      "lesson_objective": "Студент сможет применить обученную сеть к задаче классификации изображений MNIST и интерпретировать полученные метрики качества.",
      "key_topics": [
        "Подготовка данных: нормализация пикселей, one‑hot кодирование меток",
        "Архитектура сети для MNIST (вход 784, скрытый слой 128, выход 10)",
        "Метрики оценки: точность, матрица ошибок, ROC‑кривые",
        "Тонкая настройка модели: изменение количества скрытых нейронов, применение Dropout"
      ],
      "exercises": [
        {
          "exercise_title": "Создание тренировочного набора MNIST",
          "exercise_instructions": "Опишите шаги предобработки данных MNIST: масштабирование значений пикселей к диапазону [0,1], преобразование меток в one‑hot векторы и разделение на обучающую и тестовую выборки (80/20)."
        },
        {
          "exercise_title": "Эксперимент с числом нейронов в скрытом слое",
          "exercise_instructions": "Обучите сеть с скрытым слоем из 64, 128 и 256 нейронов при одинаковых остальных параметрах. Запишите полученную точность на тестовой выборке для каждой конфигурации и сделайте вывод о влиянии размера слоя."
        }
      ]
    }
  ]
}