{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел посвящён фундаментальным концепциям нейронных сетей, их архитектуре, методам обучения и практическим примерам реализации. Студенты познакомятся с простейшими моделями, такими как перцептрон, перейдут к многослойным сетям с обратным распространением ошибки, изучат техники регуляризации и оптимизации, а также получат базовые навыки построения сверточных нейронных сетей для работы с изображениями.",
  "learning_objectives": [
    "Студент сможет описать структуру и принцип работы перцептрона.",
    "Студент сможет реализовать и обучить многослойный перцептрон с помощью алгоритма обратного распространения ошибки.",
    "Студент сможет применять методы регуляризации и современные оптимизаторы для повышения качества моделей.",
    "Студент сможет построить простую сверточную нейронную сеть и объяснить роль её компонентов."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Перцептрон: построение и обучение простейшего нейрона",
      "lesson_objective": "Студент сможет построить и обучить перцептрон для решения задачи линейной классификации.",
      "key_topics": [
        "структура перцептрона",
        "функция активации сигмоида",
        "правило обучения Хебба",
        "градиентный спуск для перцептрона"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона на Python",
          "exercise_instructions": "Напишите функцию, принимающую набор входов и векторы весов, возвращающую выход после применения сигмоиды. Затем реализуйте обучение на линейно разделимом наборе данных, используя градиентный спуск с фиксированным шагом обучения."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Многослойный перцептрон и обратное распространение ошибки",
      "lesson_objective": "Студент сможет объяснить архитектуру MLP и выполнить её обучение с помощью алгоритма backpropagation.",
      "key_topics": [
        "слой скрытых нейронов",
        "функции активации ReLU и tanh",
        "вычисление градиентов",
        "алгоритм обратного распространения",
        "параметры обучения: learning rate, batch size"
      ],
      "exercises": [
        {
          "exercise_title": "Обучение MLP для классификации рукописных цифр",
          "exercise_instructions": "С помощью библиотеки Keras построьте сеть с одним скрытым слоем, обучите её на наборе MNIST, используя кросс-энтропию и стохастический градиентный спуск. Оцените точность на тестовой выборке."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Регуляризация и оптимизация нейронных сетей",
      "lesson_objective": "Студент сможет применять техники регуляризации и использовать продвинутые оптимизаторы для улучшения обобщающей способности модели.",
      "key_topics": [
        "L2‑регуляризация (weight decay)",
        "dropout",
        "ранняя остановка (early stopping)",
        "Adam optimizer",
        "подбор гиперпараметров"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение моделей с разными методами регуляризации",
          "exercise_instructions": "Обучите две версии MLP на наборе Fashion‑MNIST: одну без регуляризации, другую с dropout = 0.5 и L2‑регуляризацией λ = 0.001. Сравните их точность и графики обучения, сделайте выводы о влиянии регуляризации."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Сверточные нейронные сети: принципы и применение",
      "lesson_objective": "Студент сможет построить простую CNN для классификации изображений и объяснить роль сверточных слоёв, пуллинга и нормализации.",
      "key_topics": [
        "сверточный слой и ядро фильтра",
        "stride и padding",
        "max‑pooling",
        "batch normalization",
        "архитектура LeNet‑5"
      ],
      "exercises": [
        {
          "exercise_title": "Создание CNN для классификации CIFAR‑10",
          "exercise_instructions": "С помощью TensorFlow/Keras реализуйте небольшую сверточную сеть (два сверточных слоя, слой max‑pooling, batch normalization и полносвязный слой). Обучите её на наборе CIFAR‑10, достигните минимум 70 % точности на тестовой выборке и визуализируйте фильтры первого сверточного слоя."
        }
      ]
    }
  ]
}