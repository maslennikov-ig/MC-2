{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел даёт прочное концептуальное понимание искусственных нейронов, функций активации, функций потерь и их градиентов, а также алгоритма градиентного спуска. На практических примерах разбираются обновление весов, оценка качества модели и визуализация результатов. Раздел завершается реализацией персептрона на данных Iris.",
  "learning_objectives": [
    "объяснить структуру искусственного нейрона и отличия функций активации",
    "вычислять выход нейрона для заданных весов и входа в скалярном и векторном виде",
    "различать подходящие функции потерь для задач регрессии и классификации и вычислять их производные",
    "применять шаг градиентного спуска и анализировать влияние гиперпараметров на сходимость",
    "реализовать персептрон и визуализировать разделяющую границу на данных Iris",
    "оценивать качество модели на валидации метриками точность, полнота, F1 и ROC-AUC",
    "объяснять переобучение и подбирать простую стратегию регуляризации/останова"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Искусственный нейрон: вычисления и функции активации",
      "lesson_objective": "Научиться вычислять выход одного нейрона и выбирать подходящую функцию активации для различных задач.",
      "key_topics": ["линейная комбинация весов и входа", "смещение (bias)", "функции активации: identity, ReLU, sigmoid, tanh", "векторизация вычислений и пример на NumPy"],
      "exercises": [
        {
          "exercise_title": "Ручной расчёт выхода нейрона",
          "exercise_instructions": "Даны w=[2, -1, 3], x=[1, 0.5, -2] и b=0.5. Вычислите выход нейрона: а) для линейного выхода (без активации), б) для ReLU, в) для sigmoid. Покажите пошаговые выкладки и результаты."
        },
        {
          "exercise_title": "Векторизованный расчёт на мини-батче",
          "exercise_instructions": "Реализуйте на Python функцию neuron_forward(X, w, b, activation), где X — массив размера (n_samples, 3), w — вектор весов, b — скаляр, activation — одна из 'linear', 'relu', 'sigmoid'. Пропустите через функцию батч X=[[1,0.5,-2],[0,2,1],[-1,-1,1]] с w=[2,-1,3], b=0.5 и выведите результаты для каждой активации."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции потерь и градиенты для обучения",
      "lesson_objective": "Уметь выбирать функцию потерь под задачу и вычислять её градиент по параметрам модели.",
      "key_topics": ["MSE и его производная для линейной регрессии", "бинарная кросс-энтропия для логистической регрессии", "градиент по весам и смещению", "численная проверка градиента (finite differences)"],
      "exercises": [
        {
          "exercise_title": "Градиент MSE для линейного нейрона",
          "exercise_instructions": "Дан набор: X=[[1,2],[3,4]], y_true=[5,11], параметры w=[1,-0.5], b=2. Выведите вручную: а) предсказания y_pred для линейной модели, б) значение MSE, в) аналитический градиент dMSE/dw и dMSE/db."
        },
        {
          "exercise_title": "Кросс-энтропия и проверка градиента",
          "exercise_instructions": "Реализуйте функцию binary_cross_entropy(y_true, y_pred) и её градиент по y_pred для одной выборки. Для y_true=1 и y_pred=0.8 вычислите значение функции. Затем реализуйте численную проверку градиента с ε=1e-5 и сравните аналитический и численный результаты."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Градиентный спуск: обновление весов и гиперпараметры",
      "lesson_objective": "Применять градиентный спуск для обучения линейного нейрона и анализировать влияние гиперпараметров.",
      "key_topics": ["обновление w := w - lr * grad_w, b := b - lr * grad_b", "скорость обучения и условия останова", "графики функции потерь по эпохам и наблюдение сходимости", "эффект слишком малого/слишком большого шага обучения"],
      "exercises": [
        {
          "exercise_title": "Шаг градиентного спуска вручную",
          "exercise_instructions": "Для X=[1,2], y_true=5, текущих w=2, b=0 и lr=0.01: а) вычислите градиент MSE по w и b, б) выполните один шаг обновления, в) рассчитайте новое значение MSE после шага."
        },
        {
          "exercise_title": "Обучение линейного нейрона на синтетике",
          "exercise_instructions": "Сгенерируйте X = np.random.randn(200,1), y = 3*X[:,0] + np.random.randn(200)*0.1. Реализуйте обучение градиентным спуском (lr=0.01, epochs=500) с логированием MSE по эпохам. Постройте график потерь и объясните, достигнута ли сходимость."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Персептрон на данных Iris: обучение и визуализация",
      "lesson_objective": "Реализовать персептрон и визуализировать разделяющую границу на реальных данных.",
      "key_topics": ["персептрон как линейный классификатор", "обновление весов по правилу персептрона", "предобработка и разделение Iris на два класса", "визуализация разделяющей прямой w1*x1 + w2*x2 + b = 0"],
      "exercises": [
        {
          "exercise_title": "Реализация персептрона и обучение",
          "exercise_instructions": "Скачайте Iris dataset (например, через sklearn.datasets.load_iris). Оставьте классы 0 и 1 (setosa, versicolor) и признаки sepal_length, sepal_width. Реализуйте класс Perceptron с fit(X,y) и predict(X), обучите на 80% данных, оцените точность на 20%."
        },
        {
          "exercise_title": "Визуализация разделяющей границы",
          "exercise_instructions": "Используя обученные веса персептрона, постройте диаграмму рассеяния (sepal_length vs sepal_width) с точками разных классов и нанесите прямую w1*x + w2*y + b = 0. Проверьте, что она согласуется с прогнозами модели."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Оценка качества и переобучение: метрики и валидация",
      "lesson_objective": "Выбирать метрики классификации и применять простые способы контроля переобучения.",
      "key_topics": ["точность, полнота, F1-score", "ROC-AUC для бинарной классификации", "валидационное множество и ранняя остановка", "L2-регуляризация весов"],
      "exercises": [
        {
          "exercise_title": "Расчёт метрик на валидации",
          "exercise_instructions": "Для обученного персептрона вычислите на валидационном наборе: precision, recall, F1, а также ROC-AUC (через sklearn.metrics). Объясните, чем различаются precision и recall в данной задаче."
        },
        {
          "exercise_title": "Регуляризация и ранняя остановка",
          "exercise_instructions": "Модифицируйте обучение персептрона, добавив L2-регуляризацию весов (шаг обновления включает -lr*grad_w - lr*lambda*w) и раннюю остановку при отсутствии улучшения ROC-AUC на валидации в течение N эпох. Сравните метрики с базовой моделью."
        }
      ]
    }
  ]
}