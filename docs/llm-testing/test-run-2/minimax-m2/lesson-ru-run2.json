{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Вводный раздел по концептуальным основам нейронных сетей, охватывающий базовые принципы их построения, функционирования и обучения. Раздел включает изучение структуры искусственных нейронов, различных типов функций активации, архитектур сетей и фундаментальных алгоритмов обучения с практическими примерами применения.",
  "learning_objectives": [
    "студенты смогут объяснить структуру и принципы работы искусственного нейрона",
    "студенты смогут классифицировать и применять различные функции активации",
    "студенты смогут проектировать базовые архитектуры нейронных сетей",
    "студенты смогут выбирать подходящие функции потерь для различных задач",
    "студенты смогут реализовать алгоритм обратного распространения ошибки"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Искусственный нейрон и перцептрон Розенблатта",
      "lesson_objective": "студенты смогут описать математическую модель искусственного нейрона и реализовать перцептрон для решения задач линейной классификации",
      "key_topics": ["модель МакКаллока-Питтса", "взвешенная сумма входных сигналов", "пороговая функция активации", "алгоритм обучения перцептрона", "геометрическая интерпретация разделяющей гиперплоскости"],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона для логических операций",
          "exercise_instructions": "Реализуйте перцептрон с пороговой функцией активации для решения задачи XOR. Проанализируйте, почему перцептрон не может решить эту задачу и объясните ограничения. Визуализируйте процесс обучения для задач AND и OR."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и их производные",
      "lesson_objective": "студенты смогут выбирать и применять различные функции активации, анализируя их свойства и производные для обучения нейронных сетей",
      "key_topics": ["сигмоидальная функция (sigmoid)", "гиперболический тангенс (tanh)", "функция ReLU и её модификации", "функция softmax для многоклассовой классификации", "проблема исчезающих градиентов", "практические рекомендации по выбору активации"],
      "exercises": [
        {
          "exercise_title": "Сравнительный анализ функций активации",
          "exercise_instructions": "Создайте визуализацию графиков различных функций активации и их производных в диапазоне [-5, 5]. Проанализируйте диапазон значений производных и объясните влияние на процесс обучения. Реализуйте простую нейронную сеть, меняя только функцию активации, и сравните скорость сходимости."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Архитектуры нейронных сетей и топологии",
      "lesson_objective": "студенты смогут проектировать различные архитектуры нейронных сетей и объяснять принципы организации слоёв для решения специфических задач",
      "key_topics": ["полносвязные сети (DNN)", "свёрточные нейронные сети (CNN)", "рекуррентные нейронные сети (RNN)", "долгая краткосрочная память (LSTM)", "архитектура трансформеров", "соединения между слоями", "гиперпараметры архитектуры (количество слоёв, нейронов)"],
      "exercises": [
        {
          "exercise_title": "Проектирование архитектуры для распознавания рукописных цифр",
          "exercise_instructions": "Спроектируйте оптимальную архитектуру нейронной сети для датасета MNIST. Обоснуйте выбор количества слоёв, нейронов в каждом слое и функций активации. Реализуйте базовую версию и объясните, как изменения в архитектуре влияют на точность и время обучения."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Функции потерь и методы оптимизации",
      "lesson_objective": "студенты смогут выбирать подходящие функции потерь для различных типов задач и применять методы оптимизации для обучения нейронных сетей",
      "key_topics": ["среднеквадратичная ошибка (MSE)", "кросс-энтропия для бинарной классификации", "категориальная кросс-энтропия", "функция потерь для регрессии", "градиентный спуск и его варианты", "метод стохастического градиентного спуска (SGD)", "адаптивные методы оптимизации (Adam, RMSprop)"],
      "exercises": [
        {
          "exercise_title": "Сравнение функций потерь на синтетических данных",
          "exercise_instructions": "Создайте синтетические данные для задачи регрессии и бинарной классификации. Реализуйте обучение нейронной сети с разными функциями потерь. Проанализируйте влияние формы функции потерь на градиенты и скорость сходимости. Постройте графики обучения и объясните различия."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Алгоритм обратного распространения ошибки",
      "lesson_objective": "студенты смогут математически обосновать и практически реализовать алгоритм обратного распространения для обучения многослойных нейронных сетей",
      "key_topics": ["цепное правило для вычисления производных", "прямой проход и вычисление активаций", "обратный проход и вычисление градиентов", "правило дельты для скрытых слоёв", "инициализация весов и её влияние", "регуляризация и предотвращение переобучения", "практические аспекты реализации"],
      "exercises": [
        {
          "exercise_title": "Пошаговая реализация backpropagation",
          "exercise_instructions": "Реализуйте алгоритм обратного распространения для двухслойной нейронной сети с прямой математической проверкой всех вычислений. Создайте пошаговый трекер градиентов, показывающий изменения весов на каждой итерации. Протестируйте на простой задаче XOR и объясните каждый этап вычислений."
        }
      ]
    }
  ]
}