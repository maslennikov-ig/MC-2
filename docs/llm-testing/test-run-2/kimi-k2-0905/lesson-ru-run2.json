{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает фундаментальные концепции нейронных сетей: от биологического нейрона до искусственного перцептрона, включая механизмы обучения и применение на практике. Студенты изучат архитектуру нейросетей, функции активации, алгоритм обратного распространения ошибки и реализуют простую нейронную сеть с нуля на Python.",
  "learning_objectives": [
    "Объяснять принцип работы биологического и искусственного нейрона",
    "Рассчитывать выходное значение перцептрона вручную",
    "Реализовывать алгоритм обратного распространения ошибки для однослойной сети",
    "Выбирать подходящую функцию активации под задачу классификации или регрессии",
    "Оценивать качество обученной модели с помощью метрик точности и потерь"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному перцептрону",
      "lesson_objective": "Сформировать у студентов четкое представление о структуре и функциях биологического нейрона и научить их переводить эти принципы в математическую модель перцептрона.",
      "key_topics": [
        "Структура биологического нейрона: дендриты, сoma, аксон",
        "Механизм генерации потенциала действия",
        "Математическая модель МакКаллока-Питтса",
        "Взвешенная сумма входов и пороговое значение",
        "Геометрическая интерпретация перцептрона как разделяющей гиперплоскости"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет выхода перцептрона",
          "exercise_instructions": "Даны веса w=[0.5, -1.2, 0.8] и входной вектор x=[1, 0, 1]. Пороговое значение θ=0.3. Вычислите взвешенную сумму и определите выход перцептрона с пороговой функцией активации. Запишите каждый шаг вычислений."
        },
        {
          "exercise_title": "Классификация точек на плоскости",
          "exercise_instructions": "Имеется перцептрон с весами w1=1, w2=-1 и порогом θ=0. Проверьте, как классифицируются точки A(2,1), B(-1,2), C(0,0). Постройте разделяющую прямую и визуально подтвердите результаты классификации."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и их влияние на обучение",
      "lesson_objective": "Научить студентов выбирать и применять различные функции активации в зависимости от типа задачи и структуры данных, а также анализировать их влияние на градиент при обучении.",
      "key_topics": [
        "Пороговая и сигмоидальная функции: проблема исчезающего градиента",
        "Гиперболический тангенс: центрирование данных вокруг нуля",
        "ReLU и её модификации: Leaky-ReLU, Parametric-ReLU",
        "Softmax для многоклассовой классификации",
        "Экспериментальные функции: Swish, GELU, Mish"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнительный анализ производных",
          "exercise_instructions": "Постройте на одном графике функции ReLU, сигмоиду и tanh вместе с их производными в диапазоне x от -5 до 5. Выделите области, где градиент становится меньше 0.01, и объясните, как это влияет на скорость обучения."
        },
        {
          "exercise_title": "Выбор функции активации под задачу",
          "exercise_instructions": "Для каждой из задач укажите две наиболее подходящие функции активации и обоснуйте выбор: бинарная классификация изображений, регрессия цен на недвижимость, многоклассовая классификация текста на 10 языков."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Алгоритм обратного распространения ошибки на конкретных числах",
      "lesson_objective": "Позволить студентам вручную провести полный цикл прямого и обратного прохода через двуслойную сеть и увидеть, как именно обновляются веса при градиентном спуске.",
      "key_topics": [
        "Вычисление градиента потерь по весам скрытого и выходного слоя",
        "Правило цепного дифференцирования для сложных функций",
        "Матричное представление градиентов для векторизованного кода",
        "Выбор learning rate: эксперимент с разными значениями",
        "Накопление градиентов в пакетном обучении"
      ],
      "exercises": [
        {
          "exercise_title": "Пошаговый backprop в таблице",
          "exercise_instructions": "Дана сеть 2-2-1 с единичными весами и сигмоидами. Вход x=[0.5, -0.3], целевой выход y=1. Заполните таблицу: 1) выходы каждого нейрона, 2) ошибки δ на каждом слое, 3) градиенты по каждому весу. Используйте learning rate=0.1."
        },
        {
          "exercise_title": "Визуализация траектории весов",
          "exercise_instructions": "Запустите обучение перцептрона на датасете XOR в течение 50 эпох. Каждые 5 эпох сохраняйте текущие веса. Постройте 3D-график, где ось X и Y — веса, ось Z — значение функции потерь. Отметьте точку начала и конца обучения."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Инициализация весов и проблема исчезающих градиентов",
      "lesson_objective": "Обучить студентов правильно инициализировать параметры сети, чтобы избежать замедления обучения, и показать на практике, как глубина сети влияет на распространение градиента.",
      "key_topics": [
        "Инициализация нулями: симметрия нейронов и отсутствие обучения",
        "Случайная инициализация: дисперсия выходов слоя",
        "Метод Ксавье (Glorot) для сигмоид и tanh",
        "Инициализация He для ReLU и её производных",
        "Проверка распространения градиента: гистограммы ∂L/∂W по слоям"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение методов инициализации",
          "exercise_instructions": "Создайте сеть 784-128-64-10. Обучите три копии на MNIST: с нулевой, случайной нормальной (σ=1) и инициализацией Ксавье. Запишите точность на тесте после 5 эпох и постройте графики потерь по эпохам для каждого случая."
        },
        {
          "exercise_title": "Анализ градиентов по глубине",
          "exercise_instructions": "Во время обучения сети 10 слоёв сохраняйте средний абсолютный градиент для каждого слоя. Постройте линейный график 'номер слоя → средний |градиент|' после 1, 10 и 50 эпох. Определите, на каком слое градиент уменьшается в 100 раз."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Построение и обучение полносвязной сети с нуля на NumPy",
      "lesson_objective": "Студенты самостоятельно реализуют полный пайплайн: инициализацию, прямой проход, вычисление потерь, обратное распространение и обновление весов без использования фреймворков.",
      "key_topics": [
        "Класс Layer с прямым и обратным методами",
        "Функции потерь: MSE и кросс-энтропия с регуляризацией",
        "Реализация SGD с импульсом и адаптивный learning rate",
        "Разбиение данных на мини-пакеты и shuffle перед каждой эпохой",
      "Логирование метрик и визуализация кривых обучения в реальном времени"
      ],
      "exercises": [
        {
          "exercise_title": "Мини-фреймворк на чистом NumPy",
          "exercise_instructions": "Напишите модуль network.py содержащий классы DenseLayer, Sigmoid, ReLU, MSELoss. Обучите сеть 2-16-1 аппроксимировать функцию f(x)=sin(2πx) на 1000 точках из интервала [0,1]. Достигните MSE < 0.01 за ≤ 500 эпох."
        },
        {
          "exercise_title": "Классификация рукописных цифр без сторонних библиотек",
          "exercise_instructions": "Загрузите MNIST вручную через gzip и numpy.frombuffer. Реализуйте кросс-энтропийную потерю и softmax. Обучите сеть 784-128-10 до точности ≥ 92 % на тестовой выборке. Сравните время обучения с SGD и мини-batch=32."
        }
      ]
    }
  ]
}