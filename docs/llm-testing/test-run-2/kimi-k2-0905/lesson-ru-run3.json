{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает фундаментальные концепции нейронных сетей: биологические прототипы, математическая формализация перцептрона, функции активации, лосс-функции и процесс обучения через обратное распространение ошибки. Практические примеры реализуются на Python с использованием NumPy и PyTorch.",
  "learning_objectives": [
    "Объяснять биологическую основу искусственного нейрона и формализовать его математическую модель",
    "Вычислять градиенты лосс-функции вручную для линейного слоя и применять правило обновления весов",
    "Реализовывать перцептрон с нуля для бинарной классификации на Python без сторонних библиотек",
    "Сравнивать ReLU, Sigmoid и Tanh по производительности и проблемам затухающего градиента",
    "Запускать цикл обучения нейросети на синтетическом датасете и визуализировать динамику потерь"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к математическому перцептрону",
      "lesson_objective": "Сформулировать уравнение перцептрона и реализовать его на NumPy для предсказания класса точек на плоскости",
      "key_topics": ["МакКаллок-Питтс модель 1943", "Взвешенная сумма и смещение", "Пороговая функция активации", "Геометрическая интерпретация разделяющей гиперплоскости", "Представление данных в виде тензоров"],
      "exercises": [
        {
          "exercise_title": "Ручной расчёт выхода перцептрона",
          "exercise_instructions": "Даны веса w1=0.7, w2=-0.4, bias=0.1 и входной вектор x=[3, 5]. Вычислите выход перцептрона с пороговой функцией (порог=0). Проверьте результат через код на NumPy."
        },
        {
          "exercise_title": "Визуализация разделяющей линии",
          "exercise_instructions": "Сгенерируйте 50 точек каждого из двух классов в диапазоне [-5, 5]. Обучите перцептрон с двумя входами и постройте график, где цветом показаны предсказания сети, а точками — истинные метки."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и проблема затухающего градиента",
      "lesson_objective": "Проанализировать производные ReLU, Sigmoid и Tanh и выбрать оптимальную функцию для глубокой сети на основе эксперимента с 5-слойной моделью",
      "key_topics": ["Производная Sigmoid и её максимальное значение 0.25", "Насыщение Tanh при |x|>3", "ReLU и проблема «мёртвых» нейронов", "Leaky-ReLU и Parametric-ReLU", "Аналитическое доказательство исчезающего градиента"],
      "exercises": [
        {
          "exercise_title": "Градиентный поток вручную",
          "exercise_instructions": "Для сети 2-3-1 с сигмоидальными активациями вычислите градиент dL/dW1 по цепному правилу при L=(y-ŷ)². Зафиксируйте, на каком множителе градиент уменьшается в 3 слое."
        },
        {
          "exercise_title": "Эксперимент: ReLU vs Sigmoid в глубокой сети",
          "exercise_instructions": "Обучите две 7-слойные полносвязные сети на MNIST: одну с ReLU, вторую с Sigmoid. Сравните среднее абсолютное значение градиента на первом слое после 10 эпох. Выведите график."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "MSE и кросс-энтропия: выбор лосс-функции под задачу",
      "lesson_objective": "Обосновать выбор лосс-функции для регрессии, бинарной и многоклассовой классификации, реализовать каждую на чистом Python с поддержкой батчей",
      "key_topics": ["Производная MSE и её зависимость от размера ошибки", "Логистическая функция и лог-правдоподобие", "Кросс-энтропия как расстояние Кульбака-Лейблера", "Softmax и проблема переполнения", "Несбалансированные классы и weighted loss"],
      "exercises": [
        {
          "exercise_title": "Реализация кросс-энтропии с нуля",
          "exercise_instructions": "Напишите функцию binary_cross_entropy(y_true, y_pred) без использования библиотечных лоссов. Добавьте эпсилон=1e-7 для численной стабильности. Проверьте градиент вручную."
        },
        {
          "exercise_title": "Сравнение MSE и BCE на логистической регрессии",
          "exercise_instructions": "На датасете Wisconsin Breast Cancer обучите логистическую регрессию дважды: с MSE и с BCE. Постройте кривые обучения по loss и accuracy. Объясните разницу."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Обратное распространение ошибки: вывод формул и код",
      "lesson_objective": "Вывести уравнения обратного распространения для полносвязного слоя и реализовать минибатч-градиентный спуск без автодифференциации",
      "key_topics": ["Цепное правило в многомерном случае", "Якобиан для вектор-функции", "Алгоритм backprop в матричной форме", "Векторизация вычислений на NumPy", "Сравнение с torch.autograd"],
      "exercises": [
        {
          "exercise_title": "Матричное обратное распространение",
          "exercise_instructions": "Для слоя z=XW+b и функции потерь L=½‖y-σ(z)‖² выведите ∂L/∂W размером (m×n). Реализуйте функцию backward(dL_dz, X) и убедитесь, что результат совпадает с torch.grad."
        },
        {
          "exercise_title": "Полный цикл обучения без PyTorch",
          "exercise_instructions": "Обучите 2-слойную сеть на синтетическом круге (make_circles) используя только NumPy. Покажите decision boundary каждые 50 итераций. Достигните accuracy > 95 %."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Инициализация весов: why Xavier и He работают",
      "lesson_objective": "Показать, что неправильная инициализация приводит к взрыву или затуханию градиента, и реализовать Xavier и He инициализаторы на Python",
      "key_topics": ["Дисперсия выхода слоя при инициализации", "Xavier (Glorot) и баланс дисперсии", "He инициализация для ReLU", "Гауссово и равномерное распределение", "Экспериментальная проверка дисперсии активаций"],
      "exercises": [
        {
          "exercise_title": "Измерение дисперсии на 10 слоях",
          "exercise_instructions": "Создайте линейный стек 10 слоёв по 256 нейронов. Пропустите через него 1000 случайных векторов. Измерьте дисперсию активаций на каждом слое при инициализации: a) N(0,1), b) Xavier, c) He. Постройте график."
        },
        {
          "exercise_title": "Сравнение сходимости инициализаторов",
          "exercise_instructions": "Обучите 6-слойную сеть на CIFAR-10 трижды: с инициализацией N(0,0.01), Xavier и He. Фиксируйте loss каждые 10 батчей. Постройте кривые и укажите, какой метод достигает 50 % accuracy быстрее."
        }
      ]
    }
  ]
}