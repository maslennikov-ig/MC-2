{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает фундаментальные концепции нейронных сетей: от биологического нейрона до математической модели перцептрона. Изучаются механизмы обучения, функции активации, архитектура слоёв и процесс обратного распространения ошибки. Приводятся примеры реализации базовых моделей на Python с использованием NumPy и визуализация работы сети в двумерном пространстве признаков.",
  "learning_objectives": [
    "Объяснять биологическую основу нейрона и формализовать его математическую модель",
    "Вычислять выход перцептрона вручную для заданных весов и входов",
    "Реализовать градиентный спуск для обучения линейного классификатора",
    "Визуализировать разделяющую гиперплоскость и области решений перцептрона",
    "Определять вид и параметры функции потерь для задач классификации и регрессии"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному: мембранный потенциал и порог огня",
      "lesson_objective": "Сформулировать математическую модель искусственного нейрона, повторяющую пороговое поведение биологического",
      "key_topics": [
        "Мембранный потенциал и порог огня у Ходжкина-Хаксли",
        "Сумматорная функция и взвешенные входы",
        "Пороговая функция Хевисайда и её производная",
        "Геометрическая интерпретация в пространстве признаков"
      ],
      "exercises": [
        {
          "exercise_title": "Расчёт потенциала активации",
          "exercise_instructions": "Для вектора входов x=[1, -2, 0.5] и весов w=[0.3, -0.8, 1.2] вычислите сумматорную функцию и примените пороговую функцию при θ=0.5. Запишите выход нейрона и покажите промежуточные шаги."
        },
        {
          "exercise_title": "Визуализация разделяющей линии",
          "exercise_instructions": "По заданным весам w1=0.6, w2=-0.4, bias=-0.3 постройте в Jupiter Notebook прямую w1·x1 + w2·x2 + bias = 0 для x1,x2 ∈ [-2,2]. Покрасьте области, где нейрон выдаёт 1 и 0."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Перцептрон Розенблатта: алгоритм обучения с учителем",
      "lesson_objective": "Реализовать правило обновления весов перцептрона и довести точность до 100% на линейно разделимых данных",
      "key_topics": [
        "Правило обучения перцептрона: Δw = η·(y_true - y_pred)·x",
        "Эпохи обучения и критерий сходимости",
        "Линейная разделимость и ограничения XOR",
        "Выбор скорости обучения η и начальных весов"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация обучения вручную",
          "exercise_instructions": "Создайте таблицу Excel с данными XOR (4 строки). Произведите 5 эпох обучения перцептрона при η=0.1, записывая веса после каждого примера. Покажите, что сходимость не наступает."
        },
        {
          "exercise_title": "Классификация цветов ирисов",
          "exercise_instructions": "Загрузите датасет iris, возьмите два первых признака и классы 0 и 1. Напишите на Python цикл обучения перцептрона до полной сходимости. Выведите итоговые веса и число эпох."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции активации: от сигмоиды до ReLU, градиенты и проблема исчезающего градиента",
      "lesson_objective": "Построить графики производных функций активации и выбрать оптимальную для глубокой сети",
      "key_topics": [
        "Сигмоида: экспонента и ограничение выхода (0,1)",
        "Гиперболический тангенс: симметрия вокруг нуля",
        "ReLU и его производная, проблема «мёртвых» нейронов",
        "Сравнение скорости сходимости при разных функциях"
      ],
      "exercises": [
        {
          "exercise_title": "Графики и производные",
          "exercise_instructions": "В matplotlib постройте на одном полотне сигмоиду, tanh и ReLU с их производными в диапазоне x∈[-3,3]. Подпишите оси и добавьте легенду. Сохраните в SVG."
        },
        {
          "exercise_title": "Эксперимент с исчезающим градиентом",
          "exercise_instructions": "Создайте цепочку из 10 сигмоидных нейронов. Подайте на вход x=1 и вычислите градиент на первом слое при случайных весах ~N(0,0.1). Измерите, как градиент убывает по слоям."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Многослойный перцептрон: прямое и обратное распространение на доске",
      "lesson_objective": "Произвести полный проход вперёд и назад для сети 2-3-1 и обновить веса по правилу цепной производной",
      "key_topics": [
        "Матричное представление слоёв: Z = X·W + b",
        "Цепное правило: ∂L/∂W2 = ∂L/∂A2 · ∂A2/∂Z2 · ∂Z2/∂W2",
        "Вычислительный граф и промежуточные градиенты",
        "Векторизация обратного распространения на NumPy"
      ],
      "exercises": [
        {
          "exercise_title": "От руки через 3 слоя",
          "exercise_instructions": "Даны входы x=[0.5, -1], цель y=1, сеть 2-3-1 с весами W1=[[0.1,0.2],[-0.3,0.4],[0.5,-0.6]], b1=[0,0,0], W2=[[0.7],[-0.8],[0.9]], b2=[0]. Проведите прямой проход, вычислите MSE, затем обратный и обновите W2 при η=0.1. Покажите все тензоры."
        },
        {
          "exercise_title": "Векторизованная реализация",
          "exercise_instructions": "Напишите функции forward(x,Ws,bs) и backward(y_true,y_pred,Ws,bs) без циклов по слоям. Проверьте на случайных данных размера 100×10, скрытый слой 20, выход 1. Сравните время с циклическим вариантом."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Функции потерь: MSE, кросс-энтропия и их градиенты для разных задач",
      "lesson_objective": "Вывести аналитически градиенты кросс-энтропии и выбрать функцию потерь под задачу регрессии, бинарной и многоклассовой классификации",
      "key_topics": [
        "MSE: производная 2·(y_pred - y_true)/N",
        "Бинарная кросс-энтропия: −(y·log(p)+(1−y)·log(1−p))",
        "Категориальная кросс-энтропия и one-hot вектор",
        "Сходимость SGD при разных функциях потерь"
      ],
      "exercises": [
        {
          "exercise_title": "Вывод градиента кросс-энтропии",
          "exercise_instructions": "Докажите, что ∂L/∂z = p − y для бинарной кросс-энтропии при сигмоиде на выходе. Выпишите все шаги цепного правила и упростите выражение."
        },
        {
          "exercise_title": "Эксперимент на данных Fashion-MNIST",
          "exercise_instructions": "Обучите сеть 784-128-10 на 1000 примерах. Сравните сходимость MSE и категориальной кросс-энтропии: постройте кривые потерь и точности на валидации за 20 эпох. Сделайте вывод в 2 предложениях."
        }
      ]
    }
  ]
}