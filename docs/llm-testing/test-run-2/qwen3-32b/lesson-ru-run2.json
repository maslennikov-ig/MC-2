{"section_number": 1, "section_title": "Основы нейронных сетей", "section_description": "В этом разделе рассматриваются базовые концепции и теоретические основы нейронных сетей, включая архитектуры, методы обучения и практические примеры реализации. Студенты научатся анализировать структуру сетей, применять активационные функции и оптимизировать параметры моделей.", "learning_objectives": ["Описывать архитектуры нейронных сетей", "Объяснять принцип работы отдельных слоев", "Строить простые модели с использованием активационных функций", "Анализировать параметры обучения сети", "Реализовывать алгоритмы на языке Python"], "lessons": [{"lesson_number": 1, "lesson_title": "Структура и компоненты нейронной сети", "lesson_objective": "Студенты смогут описать базовые элементы нейронной сети и их взаимодействие", "key_topics": ["Нейрон как базовая единица", "Слои: входной, скрытый, выходной", "Математическое представление весов и смещения", "Пример двоичной классификации"], "exercises": [{"exercise_title": "Ручной расчет выходных значений", "exercise_instructions": "Даны три нейрона с входными данными [2,3], весами [0.5, -1] и смещением 0.3. Рассчитайте выходные значения каждого нейрона при использовании линейной функции активации."}, {"exercise_title": "Построение простой архитектуры", "exercise_instructions": "Спроектируйте архитектуру сети для задачи прогнозирования цен на дом. Укажите количество и типы слоев, примеры активационных функций для каждого слоя."}]}, {"lesson_number": 2, "lesson_title": "Персептрон и линейная классификация", "lesson_objective": "Студенты смогут применять персептрон для решения задач линейной классификации", "key_topics": ["Однослойный персептрон", "Функция потерь для классификации", "Алгоритм обучения с учителем", "Пример: разделение XOR-проблемы"], "exercises": [{"exercise_title": "Реализация персептрона", "exercise_instructions": "Напишите код на Python для реализации однослойного персептрона, который классифицирует точки на плоскости в двух категориях. Используйте библиотеку numpy для вычислений."}, {"exercise_title": "Тестирование линейной разделимости", "exercise_instructions": "Создайте набор данных с 300 образцами для задачи линейной классификации. Проверьте, может ли персептрон сходиться к решению, и визуализируйте границу разделения."}]}, {"lesson_number": 3, "lesson_title": "Функции активации и их влияние", "lesson_objective": "Студенты смогут объяснить роль функций активации и выбирать подходящие для конкретных задач", "key_topics": ["Сигмоида и её альтернативы", "ReLU и его модификации", "Softmax для многоклассовой классификации", "Градиентный спуск и нелинейность"], "exercises": [{"exercise_title": "Сравнение активационных функций", "exercise_instructions": "Постройте график функций ReLU, сигмоиды и tanh. Сравните их чувствительность к входным значениям при разных гиперпараметрах."}, {"exercise_title": "Проблемы с градиентами", "exercise_instructions": "Реализуйте простую сеть с сигмоидными активациями. Запустите обучение и проанализируйте, возникает ли проблема затухающих градиентов при увеличении количества слоев."}]}, {"lesson_number": 4, "lesson_title": "Обратное распространение ошибки", "lesson_objective": "Студенты смогут объяснить и применять алгоритм обратного распространения для обновления весов", "key_topics": ["Цепное правило дифференцирования", "Вычисление градиентов по слоям", "Обновление параметров", "Пример: автоматическое дифференцирование в PyTorch"], "exercises": [{"exercise_title": "Ручное вычисление градиентов", "exercise_instructions": "Для сети с 2 скрытыми слоями рассчитайте градиенты ошибки по весам после одного шага обратного распространения, используя входные данные [1,0] и целевые значения [0.8, 0.2]."}, {"exercise_title": "Имплементация алгоритма", "exercise_instructions": "Напишите код на Python, реализующий обратное распространение ошибки для сети с 1 скрытым слоем. Используйте квадратичную функцию потерь и метод градиентного спуска."}]}, {"lesson_number": 5, "lesson_title": "Оптимизация и регуляризация", "lesson_objective": "Студенты смогут применять методы оптимизации и регуляризации для предотвращения переобучения", "key_topics": ["Методы оптимизации: SGD, Adam", "Регуляризация L1/L2", "Dropout и его реализация", "Кросс-валидация для настройки"], "exercises": [{"exercise_title": "Сравнение оптимизаторов", "exercise_instructions": "Обучите сеть на наборе данных MNIST, используя SGD и Adam. Сравните скорость сходимости и качество классификации после 10 эпох."}, {"exercise_title": "Регуляризация в действии", "exercise_instructions": "Добавьте L2-регуляризацию к модели и dropout с вероятностью 0.5 в скрытый слой. Оцените влияние на точность на тестовой выборке."}]}]}