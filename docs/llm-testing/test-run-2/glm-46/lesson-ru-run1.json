
{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Концептуальная теоретическая секция с примерами, охватывающая фундаментальные принципы работы нейронных сетей, их математические основы, архитектурные особенности и методы обучения",
  "learning_objectives": [
    "Понимать историческое развитие нейронных сетей от перцептрона до современных архитектур",
    "Применять математические формулы для расчета активации нейрона и прямого распространения",
    "Различать основные архитектуры нейронных сетей и их компоненты",
    "Объяснять процесс обратного распространения ошибки и градиентного спуска"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "История и эволюция нейронных сетей",
      "lesson_objective": "Студенты смогут проследить развитие нейронных сетей от концепции перцептрона до современных глубоких архитектур и идентифицировать ключевые вехи в их эволюции",
      "key_topics": [
        "Модель Маккалока-Питтса 1943 года",
        "Перцептрон Розенблатта 1958 года",
        "Проблема XOR и winters of AI",
        "Воскрешение через обратное распространение 1986 года",
        "Глубокая революция 2012 года"
      ],
      "exercises": [
        {
          "exercise_title": "Временная шкала развития",
          "exercise_instructions": "Создайте временную шкалу из 10 ключевых событий в истории нейронных сетей с кратким описанием вклада каждого события в развитие области"
        },
        {
          "exercise_title": "Анализ ограничений перцептрона",
          "exercise_instructions": "Объясните математически, почему однослойный перцептрон не может решить проблему XOR, и приведите хотя бы два других примера линейно неразделимых задач"
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Математические основы нейронных сетей",
      "lesson_objective": "Студенты смогут вычислять активацию нейрона, применять различные функции активации и понимать матричные операции в контексте нейронных сетей",
      "key_topics": [
        "Векторное представление данных и весов",
        "Скалярное произведение и смещение",
        "Функции активации: сигмоида, ReLU, tanh",
        "Матричное умножение в пакетном режиме",
        "Градиент и частные производные"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет активации нейрона",
          "exercise_instructions": "Дан нейрон с весами [0.5, -0.3, 0.8] и смещением 0.1. Вычислите выход нейрона для входа [1, 0, 1] используя сигмоидную функцию активации"
        },
        {
          "exercise_title": "Сравнение функций активации",
          "exercise_instructions": "Постройте графики и сравните свойства сигмоиды, гиперболического тангенса и ReLU, указав преимущества и недостатки каждой для обучения глубоких сетей"
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Архитектура многослойного перцептрона",
      "lesson_objective": "Студенты смогут проектировать базовую архитектуру MLP, определять количество параметров и объяснять роль каждого слоя в сети",
      "key_topics": [
        "Структура входного, скрытых и выходного слоев",
        "Полностью связанные слои и их параметры",
        "Прямое распространение сигнала",
        "Универсальная аппроксимационная теорема",
        "Сверточные и рекуррентные модификации"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет параметров сети",
          "exercise_instructions": "Для MLP с архитектурой 784-128-64-10 вычислите общее количество обучаемых параметров, включая веса и смещения для каждого слоя"
        },
        {
          "exercise_title": "Проектирование архитектуры",
          "exercise_instructions": "Спроектируйте MLP для классификации изображений CIFAR-10 (32x32x3), обоснуйте выбор количества слоев и нейронов в каждом слое"
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Процесс обучения нейронных сетей",
      "lesson_objective": "Студенты смогут объяснять алгоритм обратного распространения ошибки, вычислять градиенты и применять различные варианты градиентного спуска",
      "key_topics": [
        "Функции потерь: MSE, кросс-энтропия",
        "Обратное распространение ошибки",
        "Градиентный спуск и его модификации",
        "Скорость обучения и методы ее адаптации",
        "Проблема исчезающих и взрывающихся градиентов"
      ],
      "exercises": [
        {
          "exercise_title": "Вывод градиентов",
          "exercise_instructions": "Выведите формулы градиентов для простого двухслойного перцептрона с функцией активации ReLU и функцией потерь MSE"
        },
        {
          "exercise_title": "Анализ оптимизаторов",
          "exercise_instructions": "Сравните SGD, Momentum и Adam оптимизаторы, создав таблицу преимуществ и недостатков каждого для различных типов задач"
        }
      ]
    }
  ]
}