{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции вы познакомитесь с фундаментальными принципами работы искусственных нейронов и научитесь строить простейшие нейронные сети. Вы реализуете свою первую сеть с нуля и поймёте процесс её обучения.",
  "learning_objectives": [
    "Объяснить структуру и функцию искусственного нейрона.",
    "Реализовать прямой проход (forward pass) через однослойную нейронную сеть.",
    "Вычислить функцию потерь для оценки качества предсказаний нейронной сети.",
    "Применить простейший алгоритм градиентного спуска для обновления весов."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Искусственный нейрон: строительный блок ИИ",
      "lesson_objective": "Создать функцию на Python, которая реализует вычисление выхода искусственного нейрона для заданных входов и весов.",
      "key_topics": [
        "Биологический прототип и его искусственная модель",
        "Компоненты нейрона: входы, веса, сумматор, функция активации",
        "Пороговая функция активации и сигмоида",
        "Понятие смещения (bias)"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация искусственного нейрона",
          "exercise_instructions": "1. Напишите функцию `artificial_neuron(inputs, weights, bias)`. 2. Функция должна принимать три аргумента: список входных значений, список весов и значение смещения. 3. Реализуйте внутри функции вычисление взвешенной суммы входов. 4. Добавьте к сумме смещение. 5. Примените пороговую функцию активации: если сумма >= 0, верните 1, иначе верните 0. 6. Протестируйте функцию на примере: inputs = [1.5, 2.0, 0.5], weights = [0.5, -0.5, 1.0], bias = -0.7."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "От нейрона к сети: архитектура и прямой проход",
      "lesson_objective": "Построить матрицу весов и реализовать вычисление выходов для полносвязного слоя нейронов.",
      "key_topics": [
        "Архитектура однослойной нейронной сети",
        "Представление весов в виде матрицы",
        "Вычисление выходного слоя через матричное умножение",
        "Введение в многоклассовую классификацию"
      ],
      "exercises": [
        {
          "exercise_title": "Прямой проход через полносвязный слой",
          "exercise_instructions": "1. Создайте функцию `dense_layer_forward(input_vector, weight_matrix, biases)`. 2. Используйте библиотеку NumPy для матричных операций. 3. Функция должна выполнять матричное умножение входного вектора на матрицу весов. 4. Добавьте вектор смещений к полученному результату. 5. Примените функцию активации ReLU (max(0, x)) поэлементно к результату. 6. Верните итоговый вектор выходов. 7. Протестируйте на: input_vector = [1, 2], weight_matrix = [[0.5, 0.1], [0.3, -0.2]], biases = [0.1, -0.1]."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обучение сети: функция потерь и градиентный спуск",
      "lesson_objective": "Реализовать вычисление среднеквадратичной ошибки и выполнить одну итерацию обновления весов с помощью градиентного спуска.",
      "key_topics": [
        "Функция потерь (стоимостная функция) и её роль",
        "Среднеквадратичная ошибка (MSE)",
        "Интуиция градиента и градиентный спуск",
        "Шаг обучения (learning rate)"
      ],
      "exercises": [
        {
          "exercise_title": "Расчёт функции потерь",
          "exercise_instructions": "1. Напишите функцию `mean_squared_error(y_true, y_pred)`. 2. Функция должна принимать два списка (или массива NumPy) одинаковой длины: истинные значения и предсказанные. 3. Вычислите среднеквадратичную ошибку по формуле: MSE = (1/n) * Σ(y_true - y_pred)^2. 4. Верните вычисленное значение. 5. Протестируйте на: y_true = [1, 0, 1, 1], y_pred = [0.9, 0.2, 0.8, 0.4]."
        },
        {
          "exercise_title": "Один шаг градиентного спуска",
          "exercise_instructions": "1. Напишите функцию `update_weights_simple(weights, predictions, targets, inputs, learning_rate)`. 2. Предположим упрощённую формулу градиента для одного веса w_j: gradient_j = (prediction - target) * input_j. 3. Для каждого веса в вашей модели вычислите его градиент по этой формуле. 4. Обновите каждый вес по правилу: new_weight = old_weight - learning_rate * gradient. 5. Используйте learning_rate = 0.01. 6. Протестируйте на примере с одним нейроном: weights = [0.5, 0.5], prediction=0.8, target=1, inputs=[1.0, 0.5]."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Практикум: Создание и обучение простой сети",
      "lesson_objective": "Объединить все изученные компоненты для обучения однослойной нейронной сети на синтетическом наборе данных для задачи бинарной классификации.",
      "key_topics": [
        "Синтетические данные для классификации",
        "Полный цикл обучения: прямой проход, вычисление ошибки, обратное распространение",
        "Визуализация процесса обучения",
        "Интерпретация результатов"
      ],
      "exercises": [
        {
          "exercise_title": "Обучение сети на логической операции И",
          "exercise_instructions": "1. Создайте набор данных для операции AND: inputs = [[0,0], [0,1], [1,0], [1,1]], targets = [0, 0, 0, 1]. 2. Инициализируйте веса и смещение маленькими случайными значениями. 3. Напишите цикл на 100 эпох обучения. 4. На каждой эпохе: выполните прямой проход для всех данных, вычислите общую ошибку (MSE), для каждого примера вычислите градиенты и обновите веса и смещение. 5. Выведите финальные веса, смещение и значение ошибки. 6. Проверьте, что сеть научилась правильно предсказывать выход для всех четырёх комбинаций входов."
        }
      ]
    }
  ]
}