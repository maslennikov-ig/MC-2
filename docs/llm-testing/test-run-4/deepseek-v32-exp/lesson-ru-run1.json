{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции вы познакомитесь с фундаментальными принципами работы искусственных нейронных сетей. Вы узнаете об их базовой архитектуре, процессе обучения и научитесь создавать простые модели для решения задач.",
  "learning_objectives": [
    "Построить и визуализировать архитектуру полносвязной нейронной сети с нуля, используя NumPy.",
    "Реализовать алгоритм прямого распространения (forward propagation) для получения предсказаний модели.",
    "Реализовать алгоритм обратного распространения ошибки (backpropagation) для вычисления градиентов.",
    "Обучить простую нейронную сеть на синтетическом наборе данных и добиться точности предсказаний выше 85%."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Введение в архитектуру нейронных сетей",
      "lesson_objective": "Объяснить роль нейронов, слоёв и функций активации в нейронной сети, а также нарисовать схему сети с заданными параметрами.",
      "key_topics": [
        "Биологический прототип и искусственный нейрон",
        "Структура полносвязной сети: входной, скрытый и выходной слои",
        "Функции активации: Sigmoid, Tanh, ReLU и их сравнение",
        "Понятие весов, смещений и линейной комбинации",
        "Визуальное представление архитектуры сети"
      ],
      "exercises": [
        {
          "exercise_title": "Схема моей первой нейронной сети",
          "exercise_instructions": "1. Выберите задачу: классификация изображений (10x10 пикселей) на 3 класса. 2. Определите архитектуру сети: 100 входных нейронов, 2 скрытых слоя (64 и 32 нейрона), 3 выходных нейрона. 3. На листе бумаги или в графическом редакторе нарисуйте схему этой сети, обозначив все слои и связи между ними. 4. Подпишите, какие функции активации (ReLU для скрытых, Softmax для выходного) вы будете использовать на каждом слое."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Прямое распространение (Forward Propagation)",
      "lesson_objective": "Реализовать вычисление выхода нейронной сети для заданного входного примера, используя поэлементные операции.",
      "key_topics": [
        "Матричное представление весов и входных данных",
        "Вычисление взвешенной суммы: Z = W * A + b",
        "Применение функции активации: A = activation(Z)",
        "Цепочка вычислений через все слои сети",
        "Интерпретация выходного слоя (логиты и вероятности)"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчёт forward propagation",
          "exercise_instructions": "1. Рассмотрим сеть: 2 входа, 1 скрытый слой с 2 нейронами (функция активации ReLU), 1 выходной нейрон (функция активации Sigmoid). 2. Задайте веса и смещения: W1 = [[1, -1], [0, 2]], b1 = [0, 1], W2 = [[1, 1]], b2 = [0]. 3. Возьмите входной пример X = [2, 3]. 4. Выполните пошаговый расчёт, записывая результаты для Z1, A1, Z2 и итогового предсказания A2. 5. Проверьте свои вычисления, реализовав эту же логику в коде на Python с NumPy."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обратное распространение ошибки (Backpropagation)",
      "lesson_objective": "Вычислить градиенты функции потерь по всем параметрам сети (весам и смещениям) для одного обучающего примера.",
      "key_topics": [
        "Функции потерь (бинарная и категориальная кросс-энтропия)",
        "Интуиция градиентного спуска и цепного правила",
        "Вычисление градиентов на выходном слое",
        "Распространение градиентов назад через скрытые слои",
        "Производные популярных функций активации (Sigmoid, ReLU)"
      ],
      "exercises": [
        {
          "exercise_title": "Вычисление градиентов вручную",
          "exercise_instructions": "1. Используйте архитектуру и веса из упражнения к Уроку 2. 2. Предположим, истинная метка для примера X=[2,3] равна y=1. 3. Используя бинарную кросс-энтропию в качестве функции потерь, вычислите градиенты dL/dW2, dL/db2, dL/dW1 и dL/db1. 4. Выполните расчёты вручную, применяя цепное правило. 5. Сравните ваши результаты с автоматическим вычислением через фреймворк (например, PyTorch с torch.autograd.grad)."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Обучение нейронной сети: градиентный спуск",
      "lesson_objective": "Обучить простую нейронную сеть на синтетическом наборе данных, реализовав полный цикл обучения с обновлением параметров.",
      "key_topics": [
        "Цикл эпохи и итерации (batch)",
        "Алгоритм градиентного спуска (Gradient Descent)",
        "Влияние скорости обучения (learning rate)",
        "Мониторинг процесса обучения: лосс и точность",
        "Инференс: использование обученной модели для предсказаний"
      ],
      "exercises": [
        {
          "exercise_title": "Обучение сети на синтетических данных",
          "exercise_instructions": "1. Сгенерируйте синтетический набор данных для бинарной классификации с помощью make_moons из sklearn.datasets (200 примеров, шум=0.2). 2. Реализуйте нейронную сеть с 2 входами, 1 скрытым слоем (4 нейрона, ReLU) и 1 выходом (Sigmoid). 3. Напишите цикл обучения на 1000 эпох с использованием градиентного спуска. 4. В каждой эпохе выводите значение функции потерь. 5. После обучения оцените точность модели на тестовых данных и постройте разделяющую границу, найденную сетью."
        }
      ]
    }
  ]
}