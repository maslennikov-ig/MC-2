{
  "section_number": 1,
  "section_title": "Архитектура и обучение нейронных сетей",
  "section_description": "В этой секции вы изучите базовые компоненты нейронных сетей, включая нейроны, слои и функции активации. Вы освоите принципы прямого и обратного распространения ошибки, а также реализуете простые архитектуры с нуля. По окончании вы сможете объяснять работу нейронных сетей и применять их для решения практических задач классификации.",
  "learning_objectives": [
    "Объяснить архитектуру искусственного нейрона и роль его компонентов (веса, порог, функция активации)",
    "Реализовать однослойный персептрон для бинарной классификации с использованием Python и NumPy",
    "Применить метод обратного распространения ошибки для обновления весов нейронной сети",
    "Оценить влияние выбора функции активации на сходимость обучения сети"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Структура искусственного нейрона",
      "lesson_objective": "Построить математическую модель искусственного нейрона и рассчитать его выход для заданных параметров",
      "key_topics": [
        "Биологическая аналогия нейрона",
        "Взвешенная сумма входов",
        "Функции активации (ступенчатая, сигмоида)",
        "Порог срабатывания",
        "Векторизация вычислений"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет выхода нейрона",
          "exercise_instructions": "Для входов [1.2, 0.5, -0.3], весов [0.8, -1.0, 0.4] и порога 0.5 вычислите выход нейрона с сигмоидной функцией активации. Используйте формулу: σ(Σ(w_i * x_i) - порог)"
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Прямое распространение в многослойных сетях",
      "lesson_objective": "Реализовать алгоритм прямого распространения для двухслойной нейронной сети",
      "key_topics": [
        "Организация слоев (входной, скрытый, выходной)",
        "Матричные операции умножения",
        "ReLU и Softmax функции активации",
        "Векторизация для批量 обработки данных",
        "Потери кросс-энтропии"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация прямого прохода",
          "exercise_instructions": "Напишите функцию на Python, которая принимает входную матрицу X (3x2), веса скрытого слоя W1 (2x4) и веса выходного слоя W2 (4x3), возвращает вероятности классов. Используйте ReLU для скрытого слоя и Softmax для выходного."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обратное распространение ошибки",
      "lesson_objective": "Применить цепное правило для вычисления градиентов и обновить веса сети",
      "key_topics": [
        "Градиентный спуск",
        "Цепное правило дифференцирования",
        "Вычисление дельт для скрытых слоев",
        "Скорость обучения (learning rate)",
        "Регуляризация L2"
      ],
      "exercises": [
        {
          "exercise_title": "Шаг обратного распространения",
          "exercise_instructions": "Для сети с 1 скрытым слоем вычислите градиенты dW1 и dW2 при заданных: выходной ошибке 0.25, активации скрытого слоя [0.6, 0.3], входных данных [1.0, -0.5]. Используйте производную сигмоиды и learning rate = 0.1."
        },
        {
          "exercise_title": "Реализация обучающего цикла",
          "exercise_instructions": "Дополните код прямого распространения функцией обратного распространения. Обучите сеть на датасете XOR в течение 1000 эпох, отслеживая изменение функции потерь. Постройте график сходимости."
        }
      ]
    }
  ]
}