{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Эта секция знакомит с архитектурой и принципами работы нейронных сетей. Вы изучите математические основы, этапы обучения и ключевые компоненты современных моделей. Практические задания помогут закрепить теорию через реализацию простых сетей.",
  "learning_objectives": [
    "Объяснять структуру нейрона и взаимодействие слоев в полносвязной сети",
    "Применять функции активации и оценивать их влияние на обучение",
    "Реализовывать прямое и обратное распространение ошибки на учебном датасете",
    "Настроить гиперпараметры обучения для улучшения точности модели"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Архитектура нейронных сетей: от нейрона к многослойным моделям",
      "lesson_objective": "Построить схему полносвязной сети и рассчитать выходные значения для заданного входа",
      "key_topics": [
        "Биологическая аналогия искусственного нейрона",
        "Веса, смещения и линейная комбинация",
        "Структура входного/скрытого/выходного слоев",
        "Матричные операции в прямом распространении",
        "Понятие глубины и ширины сети"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет выхода нейрона вручную",
          "exercise_instructions": "Для нейрона с весами [0.5, -0.2], смещением 0.1 и функцией активации ReLU вычислите выход при входе [2.0, 3.5]. Представьте все шаги вычислений."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и их роль в обучении",
      "lesson_objective": "Сравнить поведение 3 функций активации на графике и выбрать оптимальную для задачи классификации",
      "key_topics": [
        "Sigmoid, Tanh, ReLU и их производные",
        "Проблема затухающих градиентов",
        "Выбор функции под тип задачи (классификация/регрессия)",
        "Утечка градиента в Leaky ReLU",
        "Нормализация активаций (BatchNorm)"
      ],
      "exercises": [
        {
          "exercise_title": "Визуализация функций активации",
          "exercise_instructions": "Используя Python и Matplotlib, постройте графики Sigmoid, Tanh и ReLU в диапазоне [-5, 5]. Добавьте комментарии о преимуществах каждой функции для скрытых слоев."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обучение сетей: Loss-функции и обратное распространение",
      "lesson_objective": "Реализовать шаг обратного распространения для сети с 1 скрытым слоем",
      "key_topics": [
        "MSE и категориальная кросс-энтропия",
        "Цепное правило вычисления градиентов",
        "Обновление весов через градиентный спуск",
        "Роль скорости обучения (learning rate)",
        "Регуляризация L1/L2"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет градиентов",
          "exercise_instructions": "Для сети с 2 входами, 3 нейронами в скрытом слое (ReLU) и 2 выходами (Softmax) вычислите градиенты по весам при заданной ошибке. Используйте данные из примера в методических указаниях."
        },
        {
          "exercise_title": "Реализация шага обучения",
          "exercise_instructions": "На ноутбуке Colab создайте сеть Keras с 1 скрытым слоем. Обучите её на датасете MNIST, визуализируйте изменение loss и accuracy в процессе тренировки."
        }
      ]
    }
  ]
}