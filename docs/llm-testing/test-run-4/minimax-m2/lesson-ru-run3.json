{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Эта секция раскрывает архитектуру и принципы обучения нейронных сетей: от персептрона до многослойных моделей. Вы реализуете прямой проход, изучите функции активации и разберётесь с обратным распространением ошибки. Практические задания помогут закрепить интуицию и освоить базовые навыки программирования нейросетей.",
  "learning_objectives": [
    "Создать и обучить простой персептрон для решения задач OR и XOR",
    "Реализовать прямой проход (forward pass) для однослойной нейронной сети с функциями активации ReLU, сигмоида и tanh",
    "Вычислить градиенты для обратного распространения ошибки вручную и с использованием автоградиента",
    "Объяснить влияние скорости обучения и инициализации весов на сходимость",
    "Применить регуляризацию (L2 и ранняя остановка) и оценить её эффект на переобучение"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к персептрону",
      "lesson_objective": "Реализовать простейший персептрон и добиться 100% точности на задаче OR на синтетических данных",
      "key_topics": [
        "Биологическая мотивация нейронных сетей",
        "Архитектура персептрона: веса, смещение, ступенчатая функция активации",
        "Синтетические данные для OR и XOR",
        "Обучение персептрона: критерий остановки, сходимость"
      ],
      "exercises": [
        {
          "exercise_title": "Персептрон для функции OR",
          "exercise_instructions": "Создайте массив данных X для функции OR с входами (0,0), (0,1), (1,0), (1,1) и метками y = [0,1,1,1]. Инициализируйте персептрон с двумя входами и смещением, используя нулевые или случайные малые веса. Обучите модель с правилом обучения персептрона до тех пор, пока все примеры не будут классифицированы правильно, и верните обученные веса и смещение. Дополнительно проверьте, что на данных XOR классификация не достигает 100% и опишите, почему."
        },
        {
          "exercise_title": "Анализ сходимости персептрона",
          "exercise_instructions": "Зафиксируйте датасет OR и запустите обучение персептрона 1000 раз, каждый раз инициализируя веса случайно из распределения Uniform(-0.1, 0.1). Подсчитайте число итераций до сходимости и долю запусков, где сходимость достигнута. Выведите среднее, минимум и максимум числа итераций, а также интуицию о влиянии инициализации на скорость сходимости."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и прямой проход",
      "lesson_objective": "Реализовать прямой проход однослойной сети с минимум тремя разными функциями активации и объяснить разницу в их поведении",
      "key_topics": [
        "ReLU и её производная: простота и разрежённость активаций",
        "Сигмоида: гладкость, вероятностная интерпретация, затухание градиентов",
        "tanh: центрированные выходы, сравнение с сигмоидой",
        "Прямой проход: w·x + b и применение активации по элементам"
      ],
      "exercises": [
        {
          "exercise_title": "Forward pass для однослойной сети",
          "exercise_instructions": "Определите однослойную сеть с одним скрытым слоем из 4 нейронов и входной размерностью 2. Инициализируйте веса из нормального распределения N(0, 0.5), смещения нулями и зафиксируйте seed=42. Реализуйте функцию прямого прохода, которая принимает X и возвращает выходы для активаций ReLU, sigmoid и tanh. На входе используйте X = [[0,0],[0,1],[1,0],[1,1]]. Выведите выходы для каждой активации и кратко объясните различия."
        },
        {
          "exercise_title": "Визуализация активаций",
          "exercise_instructions": "Для того же X постройте гистограммы выходов для каждой активации и опишите долю нулевых значений (для ReLU), среднее и дисперсию для каждого случая. Сделайте вывод о том, как свойства активаций могут влиять на обучение."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функция потерь, градиенты и обратное распространение",
      "lesson_objective": "Вывести и реализовать градиенты для однослойной сети с MSE и сигмоидой на выходе, добиться убывания потерь на обучении",
      "key_topics": [
        "MSE для регрессии и её производная",
        "Логистическая функция и вероятностная интерпретация",
        "Цепное правило и градиенты по весам и смещениям",
        "Обратное распространение ошибки шаг за шагом"
      ],
      "exercises": [
        {
          "exercise_title": "Обратное распространение с MSE",
          "exercise_instructions": "Задайте X = [[-1, 2], [0, -1], [1, 1]] и цели y = [0.2, 0.8, 0.5]. Используйте скрытый слой из 3 нейронов с tanh, выходной слой 1 нейрон с сигмоидой, скорость обучения 0.1. Реализуйте forward и backward проходы, вычислите градиенты dW1, db1, dW2, db2 и обновите веса на один шаг. Верните значения потерь до и после шага, а также значения градиентов и обновлённые веса."
        },
        {
          "exercise_title": "Обучение и мониторинг сходимости",
          "exercise_instructions": "На тех же данных обучите модель 500 эпох, записывая потери после каждой эпохи. Выведите график зависимости потерь от эпохи и итоговые потери. Объясните, уменьшаются ли потери и почему."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Обучаем глубже: архитектура, инициализация и регуляризация",
      "lesson_objective": "Спроектировать и обучить двухслойную сеть, сравнить влияние скорости обучения, Xavier/He инициализации и L2/ранней остановки",
      "key_topics": [
        "Двухслойная архитектура: выбор числа нейронов",
        "Инициализация Xavier/He и её влияние на градиенты",
        "Переобучение vs недообучение",
        "L2-регуляризация и ранняя остановка по валидации"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение инициализаций на синтетической задаче",
          "exercise_instructions": "Сгенерируйте синтетический датасет для бинарной классификации с помощью make_classification(n_samples=400, n_features=2, n_redundant=0, n_informative=2, random_state=42), разбейте на train/val 80/20. Обучите двухслойную сеть (8→4→1, ReLU в скрытом, sigmoid на выходе) с MSE, lr=0.1, 300 эпох при трёх инициализациях: zeros, Xavier, He. Постройте кривые обучения (потери на train/val) для каждого случая и сделайте вывод о влиянии инициализации на стабильность."
        },
        {
          "exercise_title": "Регуляризация: L2 и ранняя остановка",
          "exercise_instructions": "Используя лучшую инициализацию из предыдущего упражнения, обучите модель с L2-регуляризацией на веса с коэффициентом 0.01 и без неё. Также реализуйте раннюю остановку, если val_loss не улучшается 30 эпох. Сравните финальные метрики accuracy и F1-score на валидации, опишите различия в переобучении."
        }
      ]
    }
  ]
}