{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "В этой секции вы изучите фундаментальные строительные блоки нейронных сетей: нейрон, веса, смещения, функции активации, архитектуры и процесс обучения. Научитесь вручную реализовывать прямое распространение и простейшие градиентные шаги, а также построите небольшую полносвязную сеть для задачи бинарной классификации.",
  "learning_objectives": [
    "Объяснить роль нейронов, весов, смещений и функций активации; различать слои и архитектуры",
    "Реализовать прямое распространение для полносвязных слоев с нелинейными активациями и посчитать MSE для регрессии",
    "Вывести и запрограммировать градиенты MSE по весам для линейного слоя и реализовать градиентный спуск",
    "Собрать мини-нейросеть (MLP) с 1 скрытым слоем и обучить её на синтетической задаче бинарной классификации",
    "Внедрить L2-регуляризацию и dropout в цикл обучения и оценить их влияние на обобщающую способность"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Нейрон, слой и прямое распространение",
      "lesson_objective": "Понять модель нейрона, слоя и вычислить прямое распространение для полносвязной сети с регрессионным выходом, включая функцию потерь MSE",
      "key_topics": [
        "Модель нейрона: w, b, активация",
        "Полносвязный слой: z = Wx + b",
        "Активации: Identity, ReLU, Sigmoid, Tanh",
        "Прямое распространение (forward pass)",
        "MSE как функция потерь для регрессии"
      ],
      "exercises": [
        {
          "exercise_title": "Прямое распространение + MSE",
          "exercise_instructions": "Реализуйте функцию forward_regression(W, b, X) для модели y_hat = W x + b (без активации) и функцию mse_loss(X, y, W, b). На вход подается матрица признаков X формы (n_samples, n_features), вектор истинных значений y формы (n_samples,) и параметры слоя W (n_features,), b (scalar). Функция должна вернуть (y_hat, loss), где loss = mean((y_hat - y)**2). Напишите небольшой тест на синтетических данных и выведите MSE."
        },
        {
          "exercise_title": "Сетка с нелинейностью",
          "exercise_instructions": "Реализуйте функцию forward_mlp(W1, b1, W2, b2, X, activation='relu'), возвращающую (h, y_hat). Первый слой: z1 = X @ W1 + b1, h = act(z1). Второй (выходной) слой: z2 = h @ W2 + b2, y_hat = z2 (регрессия). Поддержите 'identity', 'relu', 'sigmoid', 'tanh'. Выведите MSE на тех же данных."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Активации и их свойства",
      "lesson_objective": "Объяснить различия между Identity, Sigmoid, Tanh, ReLU, LeakyReLU и знать их области применения; реализовать производные для обратного распространения",
      "key_topics": [
        "Identity: f(a)=a, f'(a)=1",
        "Sigmoid: σ(a)=1/(1+e^{-a}), σ'(a)=σ(a)(1-σ(a))",
        "Tanh: tanh(a), d/da tanh(a)=1 - tanh(a)^2",
        "ReLU: max(0, a), d/da ReLU = 1[a>0]",
        "LeakyReLU: max(α a, a), d/da LeakyReLU = 1[a>0] + α 1[a<=0]"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация функций активации и их производных",
          "exercise_instructions": "Создайте функции activate(x, name) и activate_prime(x, name) для 'identity', 'sigmoid', 'tanh', 'relu', 'leaky'. Параметр α=0.01 для LeakyReLU. Функции должны работать для скаляров и numpy-массивов. Постройте графики функций и их производных на отрезке [-5, 5]."
        },
        {
          "exercise_title": "Эксперимент с затухающими/взрывающимися градиентами",
          "exercise_instructions": "Сгенерируйте последовательность из 50 слоев с весами, инициализированными: вариант 1) N(0, 0.5) и вариант 2) N(0, 1.2). Для каждого варианта рассчитайте прямой проход с активацией tanh и произведение производных вдоль пути dL/da для одного батча. Визуализируйте нормы градиентов по глубине. Объясните, где наблюдается затухание/взрыв."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_objective": "Вывести градиенты MSE по параметрам линейного слоя и реализовать обратное распространение для простых моделей; применить градиентный спуск",
      "key_topics": [
        "MSE и его градиенты по y_hat",
        "Градиенты для линейного слоя: dL/dW = X^T dL/dz / m, dL/db = mean(dL/dz)",
        "Цепное правило для активаций",
        "Градиентный спуск: W ← W - lr * dW, b ← b - lr * db"
      ],
      "exercises": [
        {
          "exercise_title": "Градиенты для линейной регрессии вручную",
          "exercise_instructions": "Реализуйте backward_mse(W, b, X, y) для модели y_hat = X @ W + b, вычисляя градиенты dW и db. Сгенерируйте синтетические данные (n_samples=200, n_features=3). Выполните 300 итераций градиентного спуска с lr=0.05 и выведите финальный MSE, а также L2-норму весов."
        },
        {
          "exercise_title": "Backprop для однослойного MLP (регрессия)",
          "exercise_instructions": "Напишите backward_hidden(W1, b1, W2, b2, X, y, activation='relu'), который для модели из forward_mlp и MSE считает градиенты dW1, db1, dW2, db2. Запустите обучение на синтетических данных в течение 500 итераций с lr=0.05 и отобразите кривую обучения (MSE по эпохам)."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Небольшой MLP и обучение",
      "lesson_objective": "Собрать и обучить двухслойную сеть на синтетической задаче бинарной классификации, реализовать обучение мини-батчами и рассчитать метрики",
      "key_topics": [
        "Архитектура MLP: вход → скрытый слой → выход",
        "Классификация: логистическая функция на выходе и логистическая потеря",
        "Мини-батчи и эпохи",
        "Метрики: точность, ROC-AUC (при необходимости)"
      ],
      "exercises": [
        {
          "exercise_title": "MLP для бинарной классификации и обучение",
          "exercise_instructions": "Реализуйте forward_classification(W1, b1, W2, b2, X, activation_hidden='relu'), где выход через σ(z2). Напишите функцию обучения train_mlp(X, y, hidden_dim=16, epochs=600, lr=0.1, batch_size=64) с мини-батчами, используя ранее созданные backward и активации. Разбейте данные на train/val 80/20, выведите финальную accuracy на val и постройте ROC-кривую."
        },
        {
          "exercise_title": "Сравнение активаций в скрытом слое",
          "exercise_instructions": "Запустите обучение из предыдущего упражнения с активациями 'relu', 'sigmoid', 'tanh', 'leaky'. Для каждой активации сохраните кривые val_loss по эпохам и зафиксируйте время обучения. Постройте графики зависимости val_loss от эпохи для всех активаций и объясните различия в сходимости."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Обобщение: регуляризация и начало",
      "lesson_objective": "Добавить L2-регуляризацию и dropout в цикл обучения и сравнить качество на валидации; объяснить влияние инициализации на обучение",
      "key_topics": [
        "L2-регуляризация: потеря + λ ||W||^2, градиенты по W",
        "Dropout: прямое распространение и инвертированный дропаут",
        "He/Xavier инициализация",
        "Ранняя остановка, контроль переобучения"
      ],
      "exercises": [
        {
          "exercise_title": "Добавление L2-регуляризации",
          "exercise_instructions": "Модифицируйте функцию потерь и градиенты из упражнения 4, добавив L2-регуляризацию с коэффициентом λ=1e-3. Переобучите модель и сравните val_loss/accuracy с базовой версией без L2. Выведите L2-норму весов до и после регуляризации."
        },
        {
          "exercise_title": "Dropout и инициализация",
          "exercise_instructions": "Реализуйте прямое распространение с dropout (инвертированный, p=0.5 на тренинге, без дропаула на валидации) и используйте He-инициализацию для W1/W2. Сравните результаты обучения (val_accuracy, стабильность val_loss) с вариантом без dropout и с инициализацией N(0, 1.0). Визуализируйте кривые обучения."
        }
      ]
    }
  ]
}