{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Освойте фундаментальные концепции искусственных нейронных сетей, от биологического вдохновения до первой полноценной реализации на Python. Секция подготовит вас к построению, обучению и оценке простейших сетей для задач регрессии и бинарной классификации.","learning_objectives":["Объяснить принцип работы искусственного нейрона и сигмоидной функции активации","Реализовать векторизованный forward-pass полносвязной сети с 1 скрытым слоем на NumPy","Вычислить производные MSE и бинарной cross-entropy потерь вручную","Обучить сеть градиентным спуском и остановить обучение по валидационной метрике"],"lessons":[{"lesson_number":1,"lesson_title":"От биологического нейрона к искусственному","lesson_objective":"Построить и визуализировать математическую модель искусственного нейрона, реализовав веса, смещение и сигмоидную активацию на Python","key_topics":["Биологический нейрон: дендриты, сома, аксон","Модель McCulloch-Pitts: взвешенная сумма + порог","Функции активации: сигмоида, ReLU, tanh графики и производные","Векторизация: переход от циклов к dot-product","Градиент и его роль в обновлении весов"],"exercises":[{"exercise_title":"Реализовать нейрон-сигмоиду","exercise_instructions":"Создайте функцию neuron(x, w, b), которая принимает вектор признаков x, вектор весов w и смещение b, возвращает вероятность после сигмоиды. Проверьте на x=[0.5, -1.2], w=[2.0, -3.0], b=0.4; убедитесь, что результат ≈ 0.8176."},{"exercise_title":"Визуализировать поверхность активации","exercise_instructions":"Постройте 3D-график сигмоиды для двух входов: создайте сетку x1, x2 ∈ [-3, 3] с шагом 0.1, вычислите z=w1·x1+w2·x2+b при w1=1, w2=-1, b=0 и отобразите sigmoid(z) через plot_surface."}]},{"lesson_number":2,"lesson_title":"Архитектура полносвязной сети","lesson_objective":"Собрать двухслойную полносвязную сеть (вход-скрытый-выход) и выполнить векторизованный проход вперёд для батча из 64 объектов за один вызов NumPy","key_topics":["Слоистая архитектура: input → hidden → output","Размерности тензоров: (batch, features), (in, out)","Инициализация весов: случайная нормальная и Xavier","Функции активации на скрытом слое: ReLU vs сигмоида","Проход forward: линейная часть + нелинейность","Кеширование промежуточных значений для backprop"],"exercises":[{"exercise_title":"Инициализировать сеть для XOR","exercise_instructions":"Создайте класс TwoLayerNet с методами __init__(self, n_in, n_h, n_out) и forward(self, X). Проверьте размерности: при X.shape=(4,2) выход должен быть (4,1). Используйте случайную нормальную инициализацию с масштабом 0.1."},{"exercise_title":"Проектировать проход вперёд без циклов","exercise_instructions":"Реализуйте только матричные операции: Z1 = X·W1 + b1, A1 = ReLU(Z1), Z2 = A1·W2 + b2, A2 = sigmoid(Z2). Убедитесь, что весь батч размера 64 пропускается одним выражением, время выполнения <1 мс на CPU."}]},{"lesson_number":3,"lesson_title":"Функции потерь и их производные","lesson_objective":"Вывести аналитические градиенты MSE и binary-cross-entropy по весам выходного слоя и реализовать их в коде без автоматического дифференцирования","key_topics":["MSE: формула и производная по предсказанию","Binary cross-entropy: −(y·log(p)+(1−y)·log(1−p))","Производная BCE: (p − y)/(p·(1−p))","Связь градиента потерь с градиентом активации","Цепное правило: dL/dW = (dL/dZ)·(dZ/dW)","Векторизация градиентов: outer product"],"exercises":[{"exercise_title":"Вывести градиент BCE вручную","exercise_instructions":"Докажите, что ∂L/∂z = p − y при L = −[y log p + (1−y) log(1−p)] и p = σ(z). Подставьте σ′(z)=σ(z)(1−σ(z)) и покажите сокращение. Реализуйте функцию dL_dp(y, p) и проверьте на y=1, p=0.7 → ожидается −1/0.7 ≈ −1.4286."},{"exercise_title":"Сравнить MSE и BCE на несбалансированных данных","exercise_instructions":"Сгенерируйте 1000 объектов, 95 % класса 0. Обучите нейрон с сигмоидой два раза: под MSE и под BCE. Зафиксируйте одинаковую скорость обучения. Сравните точность и log-loss на отложенной выборке; выведите, какая функция быстрее сходится."}]},{"lesson_number":4,"lesson_title":"Обратное распространение ошибки","lesson_objective":"Реализовать полный цикл backprop для двухслойной сети и обновить веса градиентным спуском так, чтобы потери на обучающей выборке уменьшились в 2 раза за 50 итераций","key_topics":["Цепное правило для слоёв: δ^L, δ^L−1","Градиенты dW, db для каждого слоя","Векторизованный backprop за один проход","Алгоритм обучения: forward → loss → backward → update","Мини-батчи: выбор размера и перемешивание","Контроль переобучения: отслеживание train vs val loss"],"exercises":[{"exercise_title":"Реализовать backward для двух слоёв","exercise_instructions":"Дополните TwoLayerNet методом backward(self, X, y, lr=0.01). Вычислите dZ2, dW2, db2, затем dA1, dZ1, dW1, db1. Убедитесь, что размерности совпадают с W1, b1, W2, b2. Проверьте на батче 32 объекта XOR."},{"exercise_title":"Обучить сеть до 99 % accuracy на XOR","exercise_instructions":"Сгенерируйте 1000 случайных точек в квадрате [−1,1]×[−1,1], метка 1 если точка внутри круга радиуса 0.7. Обучайте сеть 2-10-1 до тех пор, пока accuracy на валидации не достигнет 99 %. Выведите количество эпох и график потерь."}]},{"lesson_number":5,"lesson_title":"Оценка качества и отладка модели","lesson_objective":"Провести полный эксперимент: разбить данные на train/val/test, обучить сеть с ранней остановкой, измерить точность, recall и F1, выявить признаки переобучения по кривым обучения","key_topics":["Разбиение данных: 60/20/20 стратификация","Метрики бинарной классификации: accuracy, precision, recall, F1","Кривые обучения: high bias vs high variance","Ранняя остановка по val-loss с patience=5","Настройка скорости обучения: grid-search по 0.1, 0.01, 0.001","Визуализация решающей границы в 2D"],"exercises":[{"exercise_title":"Провести grid-search по learning_rate","exercise_instructions":"Используйте набор данных Moon (500样本, шум 0.25). Для lr ∈ {0.5, 0.1, 0.05, 0.01, 0.005} обучите сеть 2-16-1 в течение 200 эпох. Запишите лучшую валидационную accuracy и постройте график зависимости final_val_acc от lr."},{"exercise_title":"Построить кривые обучения и решающую границу","exercise_instructions":"Для лучшего lr из предыдущего упражнения сохраняйте train и val loss каждую эпоху. Постройте два графика: loss vs epoch и contour-plot решающей границы на тестовой выборке. Убедитесь, что граница визуально разделяет луны."}]}]}