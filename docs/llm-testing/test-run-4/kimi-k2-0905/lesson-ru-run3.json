{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Освойте базовые принципы искусственных нейронных сетей и научитесь проектировать простые модели для задач регрессии и классификации. Секция охватывает устройство нейрона, forward и backward проходы, а также практическое построение сети с нуля на Python.","learning_objectives":["Объяснить биологическую мотивацию и математическую модель искусственного нейрона","Реализовать forward pass полносвязной сети вручную с использованием NumPy","Выполнить backward pass и обновить веса с помощью градиентного спуска","Оценить качество модели на задаче бинарной классификации и добиться ≥85 % accuracy на синтетическом наборе","Сравнить влияние различных функций активации на скорость сходимости"],"lessons":[{"lesson_number":1,"lesson_title":"От биологического нейрона к искусственному","lesson_objective":"Построить и визуализировать математическую модель нейрона, вычислив её выход для заданных входов вручную","key_topics":["Биологическая мотивация","Линейная комбинация входов","Функции активации: ReLU, sigmoid, tanh","Геометрическая интерпретация в пространстве признаков","Пороговое решение и гиперплоскости"],"exercises":[{"exercise_title":"Ручной forward одного нейрона","exercise_instructions":"1. Задайте веса w=[0.8, -1.2] и смещение b=0.3. 2. Для входа x=[5, 10] вычислите z=w·x+b. 3. Примените sigmoid и ReLU активации. 4. Сравните результаты и объясните разницу словами."},{"exercise_title":"Визуализация гиперплоскости","exercise_instructions":"1. Сгенерируйте 100 точек 2D-пространства в диапазоне [-5,5]. 2. Используя w=[2, -1] и b=1, раскрасьте точки по значению σ(w·x+b). 3. Постройте линию решений σ=0.5. 4. Анимируйте изменение линии при варьировании b от -3 до 3."}]},{"lesson_number":2,"lesson_title":"Архитектура полносвязной сети","lesson_objective":"Спроектировать полносвязную сеть с заданным количеством скрытых слоёв и нейронов, рассчитав общее число обучаемых параметров","key_topics":["Слои input, hidden, output","Матричное представление forward pass","Инициализация весов: Xavier, He","Размерность тензоров на каждом слое","Граф вычислений"],"exercises":[{"exercise_title":"Подсчёт параметров","exercise_instructions":"1. Определите сеть 3-10-5-1. 2. Вычислите количество весов и смещений на каждом слое. 3. Суммируйте общее число параметров. 4. Реализуйте функцию count_params(layers) и проверьте результат."},{"exercise_title":"Инициализация весов","exercise_instructions":"1. Создайте сеть 2-4-1. 2. Инициализируйте веса методом Xavier. 3. Проведите 100 forward проходов для случайных входов. 4. Постройте гистограмму выходов до и после активации, объясните наблюдаемое распределение."}]},{"lesson_number":3,"lesson_title":"Forward и Backward проходы вручную","lesson_objective":"Реализовать полный цикл forward и backward проходов для сети 2-2-1 на NumPy и выполнить одно обновление весов","key_topics":["Цепное правило для градиентов","Градиенты функций активации","Векторизованные вычисления","Обновление весов с помощью градиентного спуска"],"exercises":[{"exercise_title":"Backward pass вручную","exercise_instructions":"1. Задайте сеть 2-2-1 с фиксированными весами. 2. Для пары (x,y) вычислите MSE-ошибку. 3. Вычислите градиенты dW2, db2, dW1, db1 пошагово. 4. Сравните результат с автоматическим дифференцированием из JAX или PyTorch."}]},{"lesson_number":4,"lesson_title":"Тренировка сети на бинарной классификации","lesson_objective":"Дообучить полносвих сеть до ≥85 % accuracy на синтетическом наборе make_moons(n_samples=1000, noise=0.2) за ≤500 эпох","key_topics":["Пакетный градиентный спуск","Функция потерь BCE","Метрики accuracy и F1","Переобучение и регуляризация","Обучающие и валидационные кривые"],"exercises":[{"exercise_title":"Полный цикл обучения","exercise_instructions":"1. Сгенерируйте make_moons(1000, noise=0.2, random_state=42). 2. Разделите на 80/20. 3. Реализуйте обучение с batch_size=32, lr=0.1. 4. Каждые 10 эпох записывайте loss и accuracy на валидации. 5. Остановитесь при достижении 85 % accuracy."},{"exercise_title":"Эксперимент с learning rate","exercise_instructions":"1. Повторите обучение при lr=[0.01, 0.1, 1.0]. 2. Постройте графики loss vs epoch. 3. Определите оптимальный lr. 4. Объясните влияние слишком большого и слишком маленького lr."}]},{"lesson_number":5,"lesson_title":"Сравнение функций активации","lesson_objective":"Провести систематическое сравнение ReLU, LeakyReLU и tanh на задаче регрессии и выбрать наилучшую по скорости сходимости и итоговой MSE","key_topics":["Проблема исчезающего градиента","Параметр наклона LeakyReLU","Экспериментальный дизайн и таблица результатов","Визуализация градиентов по слоям"],"exercises":[{"exercise_title":"Абляционное сравнение","exercise_instructions":"1. Сгенерируйте синусоиду y=sin(x)+ε, 1000 точек. 2. Обучите сеть 1-10-10-1 три раза, меняя только активацию. 3. Фиксируйте random_seed для воспроизводимости. 4. Сравните MSE и количество эпох до сходимости (<0.01 MSE). 5. Постройте boxplot по 5 запускам."}]}]}