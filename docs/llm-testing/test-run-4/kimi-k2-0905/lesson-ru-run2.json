{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Изучите биологическое вдохновение, математическую модель и базовые алгоритмы обучения нейронных сетей. Разберитесь, как из простых нейронов строятся сети, способные решать задачи классификации и регрессии.","learning_objectives":["Объяснить принцип работы искусственного нейрона и его отличие от биологического прототипа","Реализовать forward pass и backpropagation для однослойного перцептрона на Python","Построить и обучить двухслойную нейросеть для бинарной классификации с точностью ≥90% на синтетическом наборе","Выбрать оптимальную функцию активации и конфигурацию скрытого слоя для заданной задачи","Произвести визуализацию процесса обучения и диагностику переобучения по графикам loss/accuracy"],"lessons":[{"lesson_number":1,"lesson_title":"От биологического нейрона к искусственному","lesson_objective":"Построить график искусственного нейрона и вычислить его выход при заданных весах и сигмоидальной активации","key_topics":["Структура биологического нейрона","МакКаллок-Питс нейрон","Взвешенная сумма входов","Функции активации: step, sigmoid, ReLU","Геометрическая интерпретация в пространстве признаков"],"exercises":[{"exercise_title":"Ручной forward-pass нейрона","exercise_instructions":"1. Создайте функцию neuron(x, w, b, activation='sigmoid'), где x=[x1,x2,x3], w=[w1,w2,w3]. 2. Для x=[2,-1,0.5], w=[0.3,1.2,-0.8], b=-0.1 вычислите выход вручную. 3. Проверьте результат вызовом функции. 4. Постройте график зависимости выхода от изменения w1 в диапазоне [-2,2]."},{"exercise_title":"Классификация AND/OR вручную","exercise_instructions":"1. Подберите веса и порог для нейрона с step-активацией, реализующего логическое AND. 2. Повторите для OR. 3. Докажите, что XOR невозможно реализовать одним нейроном. 4. Запишите веса в таблицу и приложите график разделяющей прямой."}]},{"lesson_number":2,"lesson_title":"Перцептрон и правило обучения Rosenblatt","lesson_objective":"Реализовать перцептрон с правилом обновления весов и достичь 100% точности на линейно разделимом наборе данных","key_topics":["Алгоритм перцептрона","Правило обновления весов: w = w + α·(y−ŷ)·x","Эпохи обучения и сходимость","Линейная разделимость","Пределы однослойного перцептрона"],"exercises":[{"exercise_title":"Перцептрон с нуля на NumPy","exercise_instructions":"1. Сгенерируйте 100 точек 2D: 50 классов 0 и 50 классов 1, разделенных прямой y=0.5x+0.25. 2. Реализуйте класс Perceptron с методами fit() и predict(). 3. Обучите модель, выводите число ошибок каждую эпоху. 4. Постройте анимацию изменения разделяющей прямой. 5. Добейтесь нулевой ошибки за ≤20 эпох при α=0.1."},{"exercise_title":"Влияние скорости обучения","exercise_instructions":"1. Повторите обучение при α ∈ {0.001,0.01,0.1,1,10}. 2. Для каждого α постройте график количества ошибок по эпохам. 3. Определите минимальное α, при котором модель сходится за ≤50 эпох. 4. Сделайте вывод о связи α и скорости сходимости."}]},{"lesson_number":3,"lesson_title":"Многослойный перцептрон и backpropagation","lesson_objective":"Построить двухслойную нейросеть и обучить её на наборе Iris, достигая ≥96% точности на валидации","key_topics":["Скрытые слои и универсальная аппроксимация","Цепное правило и backpropagation","Инициализация весов Xavier/He","Функции потерь для классификации и регрессии","Градиентный спуск и его варианты"],"exercises":[{"exercise_title":"Backpropagation вручную для сети 2-2-1","exercise_instructions":"1. Нарисуйте схему сети с двумя входами, двумя скрытыми и одним выходным нейроном. 2. Задайте случайные веса в диапазоне [-0.5,0.5]. 3. Проведите forward pass для x=[0.8,−0.4]. 4. Вычислите частные производные потерь MSE по каждому весу. 5. Обновите веса одним шагом при η=0.1, приведите таблицу до и после."},{"exercise_title":"MLP на NumPy для Iris","exercise_instructions":"1. Загрузите данные Iris, стандартизируйте признаки. 2. Реализуйте MLP с архитектурой 4-5-3 (softmax выход). 3. Используйте кросс-энтропию и SGDM с momentum=0.9. 4. Обучите 200 эпох, сохраняйте loss и accuracy каждые 10 эпох. 5. Постройте графики обучения и достигните ≥96% accuracy на тесте."}]},{"lesson_number":4,"lesson_title":"Функции активации и выбор архитектуры","lesson_objective":"Сравнить 4 функции активации и выбрать оптимальное число нейронов скрытого слоя по кривым валидации","key_topics":["ReLU, Leaky ReLU, ELU, Swish","Затухание и взрыв градиента","Правило подбора размера скрытого слоя: 2/3·N_inputs+N_outputs","Регуляризация через dropout","Batch Normalization"],"exercises":[{"exercise_title":"Сравнение активаций на синтетическом наборе","exercise_instructions":"1. Сгенерируйте 1000 точек в виде двух переплетённых спиралей. 2. Обучите 2-128-2 сеть с каждой активацией (ReLU, tanh, sigmoid, LeakyReLU). 3. Используйте одинаковый seed и 200 эпох. 4. Постройте overlay-график train/val accuracy. 5. Сделайте таблицу итоговой точности и времени обучения, выберите лидера."},{"exercise_title":"Подбор числа нейронов скрытого слоя","exercise_instructions":"1. Используйте датасет Breast Cancer Wisconsin. 2. Переберите число нейронов: [4,8,16,32,64,128]. 3. Для каждого постройте среднюю accuracy по 3-кратной кросс-валидации. 4. Постройте box-plot распределения accuracy. 5. Определите минимальный размер, при котором val-accuracy ≥98%. 6. Проверьте соответствие эмпирического результата эвристике."}]},{"lesson_number":5,"lesson_title":"Диагностика обучения и борьба с переобучением","lesson_objective":"Проанализировать процесс обучения и добиться снижения разрыва между train и val accuracy до ≤2% без потери общей точности","key_topics":["Кривые обучения: high bias vs high variance","L2 и L1 регуляризация","Dropout как ансамбль","Раннее прекращение (early stopping)","Data augmentation и шум"],"exercises":[{"exercise_title":"Визуализация кривых обучения","exercise_instructions":"1. Возьмите небалансированный датасет Wine (класс 0: 59, класс 1: 71, класс 2: 48). 2. Обучите сеть 13-20-3 без регуляризации 300 эпох. 3. Сохраняйте loss и accuracy каждую эпоху для train и val. 4. Постройте двойной график (loss/accuracy) и отметьте точку переобучения. 5. Вычислите момент, когда разница val-train accuracy впервые превышает 5%."},{"exercise_title":"Борьба с переобучением комплексом мер","exercise_instructions":"1. Добавьте L2=0.001, dropout=0.3, early stopping patience=20. 2. Повторите обучение, зафиксируйте лучшую эпоху. 3. Сравните финальные метрики: accuracy, precision, recall, F1. 4. Постройте матрицу ошибок до и после. 5. Сделайте вывод: какая мера дала наибольший прирост val-accuracy и наибольшее снижение overfitting gap."}]}]}