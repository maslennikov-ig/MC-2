{"section_number":1,"section_title":"Основы нейронных сетей","section_description":"Изучите фундаментальные концепции нейронных сетей, от биологической основы до математических моделей. Научитесь строить простые сети с нуля и понимать механизм обучения через обратное распространение ошибки.","learning_objectives":["Объяснить математическую модель искусственного нейрона и его компоненты","Реализовать forward propagation для многослойного перцептрона вручную","Применить алгоритм обратного распространения ошибки для обновления весов","Построить и обучить нейронную сеть для бинарной классификации на синтетических данных","Оценить влияние функций активации и архитектуры сети на качество модели"],"lessons":[{"lesson_number":1,"lesson_title":"Биологическая основа и математическая модель нейрона","lesson_objective":"Сформулировать математическую модель искусственного нейрона и объяснить роль каждого компонента","key_topics":["Биологический нейрон vs искусственный нейрон","Взвешенная сумма входов и смещение","Функции активации: ReLU, Sigmoid, Tanh","Геометрическая интерпретация линейного разделения","Понятие многослойного перцептрона"],"exercises":[{"exercise_title":"Ручной расчет выхода нейрона","exercise_instructions":"1. Создайте входной вектор x = [0.5, -0.3, 0.8] и вектор весов w = [0.2, 0.6, -0.4] со смещением b = 0.1. 2. Вычислите взвешенную сумму z = w·x + b. 3. Примените функции активации: Sigmoid (σ(z) = 1/(1+e⁻ᶻ)), ReLU (max(0,z)), Tanh ((eᶻ-e⁻ᶻ)/(eᶻ+e⁻ᶻ)). 4. Проанализируйте, как разные функции влияют на выход при z = 0, z > 0, z < 0."},{"exercise_title":"Визуализация границы решения","exercise_instructions":"1. Сгенерируйте 20 точек 2D-данных: 10 точек класса 0 (x₁<0.5) и 10 точек класса 1 (x₁≥0.5). 2. Инициализируйте веса нейрона w = [1, 0] и смещение b = -0.5. 3. Постройте scatter plot данных и линию разделения w₁x₁ + w₂x₂ + b = 0. 4. Пересчитайте предсказания для всех точек и вычислите accuracy вручную."}]},{"lesson_number":2,"lesson_title":"Архитектуры нейронных сетей и forward propagation","lesson_objective":"Построить многослойную нейронную сеть и реализовать forward propagation для произвольной архитектуры","key_topics":["Структура скрытых слоев","Матричные вычисления в нейросетях","Размерности тензоров на каждом слое","Выбор количества нейронов и слоев","Проблема переобучения и недообучения"],"exercises":[{"exercise_title":"Реализация forward pass вручную","exercise_instructions":"1. Определите архитектуру сети: 3 входа, 2 нейрона в скрытом слое (ReLU), 1 выход (Sigmoid). 2. Инициализируйте веса и смещения случайными значениями из нормального распределения. 3. Для входного вектора x = [1.0, 0.5, -0.8] вычислите: a) скрытый слой z₁ = W₁x + b₁, a₁ = ReLU(z₁); b) выходной слой z₂ = W₂a₁ + b₂, a₂ = Sigmoid(z₂). 4. Проверьте размерности всех промежуточных результатов."},{"exercise_title":"Сравнение архитектур на XOR-задаче","exercise_instructions":"1. Подготовьте данные XOR: входы [[0,0],[0,1],[1,0],[1,1]] и метки [0,1,1,0]. 2. Создайте три архитектуры: а) без скрытого слоя (2→1), б) один скрытый слой (2→2→1), в) два скрытых слоя (2→4→2→1). 3. Инициализируйте веса одинаково для всех архитектур. 4. Запустите forward propagation для каждой сети и проанализируйте, почему первая архитектура не может решить задачу."}]},{"lesson_number":3,"lesson_title":"Обратное распространение ошибки и градиентный спуск","lesson_objective":"Выполнить полный цикл обучения нейронной сети, реализовав backpropagation и градиентный спуск вручную","key_topics":["Функция потерь для бинарной классификации","Вычисление градиентов по цепному правилу","Алгоритм обратного распространения","Скорость обучения и сходимость","Момент и другие модификации оптимизатора"],"exercises":[{"exercise_title":"Ручной расчет градиентов для одного нейрона","exercise_instructions":"1. Используйте данные из урока 1: x = [0.5, -0.3, 0.8], y_true = 1, веса w = [0.2, 0.6, -0.4], b = 0.1. 2. Вычислите forward pass с Sigmoid: a = σ(w·x + b). 3. Вычислите бинарную кросс-энтропию loss = -[y·log(a) + (1-y)·log(1-a)]. 4. Вычислите градиенты: ∂loss/∂w, ∂loss/∂b используя цепное правило. 5. Обновите веса с learning_rate=0.1: w_new = w - lr·∂loss/∂w."},{"exercise_title":"Реализация одного шага backpropagation","exercise_instructions":"1. Возьмите сеть из упражнения 2.1 (2→2→1) и один пример XOR: x = [1,0], y_true = 1. 2. Запустите forward propagation и сохраните все промежуточные значения. 3. Вычислите градиент на выходном слое: ∂loss/∂z₂. 4. Распространите ошибку назад на скрытый слой: ∂loss/∂z₁ = W₂ᵀ·∂loss/∂z₂ ⊙ ReLU'(z₁). 5. Вычислите градиенты для всех весов и смещений. 6. Обновите параметры с learning_rate=0.5."},{"exercise_title":"Наблюдение за сходимостью","exercise_instructions":"1. Создайте сеть 2→4→1 для XOR-задачи. 2. Обучите сеть вручную на 4 примерах в течение 50 эпох, используя full batch gradient descent. 3. Запишите loss после каждой эпохи. 4. Постройте график loss vs epoch и проанализируйте динамику обучения. 5. Экспериментируйте с learning_rate ∈ {0.01, 0.1, 1.0} и опишите, как скорость обучения влияет на сходимость."}]},{"lesson_number":4,"lesson_title":"Практика: построение нейронной сети с нуля на NumPy","lesson_objective":"Разработать полноценную реализацию нейронной сети с использованием только NumPy для решения задачи бинарной классификации","key_topics":["Классификация vs регрессия в нейросетях","Инициализация весов (Xavier, He)","Разделение данных на train/validation/test","Метрики оценки (accuracy, precision, recall)","Раннее остановка (early stopping)"],"exercises":[{"exercise_title":"Реализация класса NeuralNetwork","exercise_instructions":"1. Создайте класс NeuralNetwork с методами: __init__(layer_sizes), forward(X), backward(X, y), update(lr), compute_loss(X, y). 2. Реализуйте инициализацию весов по методу Xavier для tanh и He для ReLU. 3. Поддержите произвольное количество скрытых слоев. 4. Протестируйте класс на синтетических 2D-данных (make_classification из sklearn). 5. Сравните результаты с вашей ручной реализацией из предыдущих упражнений."},{"exercise_title":"Обучение на реальных данных","exercise_instructions":"1. Загрузите датасет Breast Cancer из sklearn. 2. Предобработайте данные: масштабируйте признаки, разделите на train (70%) и test (30%). 3. Создайте сеть с архитектурой 30→16→8→1. 4. Обучите сеть в течение 100 эпох с batch_size=16 и learning_rate=0.01. 5. Вычислите accuracy, precision и recall на тестовой выборке."},{"exercise_title":"Эксперимент с гиперпараметрами","exercise_instructions":"1. Используя тот же датасет Breast Cancer, создайте 3 конфигурации: а) мелкая сеть (30→8→1), б) глубокая сеть (30→32→16→8→1), в) широкая сеть (30→64→1). 2. Обучите каждую конфигурацию 5 раз с разными начальными весами. 3. Запишите среднее время обучения и средний test accuracy. 4. Проанализируйте trade-off между сложностью архитектуры и производительностью."}]},{"lesson_number":5,"lesson_title":"Введение в PyTorch: автоматическое дифференцирование и GPU","lesson_objective":"Перенести реализацию нейронной сети в PyTorch и использовать автоматическое дифференцирование для ускорения разработки","key_topics":["Тензоры PyTorch и автоград","Модуль nn.Module и слои","Оптимизаторы (SGD, Adam)","Перенос вычислений на GPU","Сохранение и загрузка моделей"],"exercises":[{"exercise_title":"Перенос numpy-реализации в PyTorch","exercise_instructions":"1. Возьмите вашу реализацию сети для XOR-задачи из урока 4. 2. Перепишите её на PyTorch используя torch.nn.Module, torch.nn.Linear, torch.nn.ReLU, torch.nn.Sigmoid. 3. Используйте torch.nn.BCELoss() и torch.optim.SGD. 4. Убедитесь, что результаты совпадают с numpy-версией (в пределах 1e-6). 5. Сравните количество строк кода и время разработки."},{"exercise_title":"Обучение на GPU и профилирование","exercise_instructions":"1. Подготовьте датасет из 10000 примеров 2D-данных для бинарной классификации. 2. Создайте сеть 2→128→64→1. 3. Обучите модель на CPU, измерьте время одной эпохи. 4. Перенесите модель и данные на GPU используя .to('cuda'), если доступно. 5. Сравните время обучения на GPU vs CPU и вычислите ускорение."},{"exercise_title":"Сохранение модели и инференс","exercise_instructions":"1. Обучите модель из упражнения 5.1 на XOR-задаче до 100% accuracy на тренировочных данных. 2. Сохраните модель целиком и только веса (state_dict) в разные файлы. 3. Создайте новый скрипт, который загружает сохраненную модель без кода обучения. 4. Реализуйте inference для новых данных: [[0.2, 0.8], [0.9, 0.1]]. 5. Сравните размер файлов и удобство использования двух подходов к сохранению."}]}]}