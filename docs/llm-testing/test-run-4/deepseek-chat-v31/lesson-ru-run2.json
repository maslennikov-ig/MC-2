{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Данная секция знакомит с фундаментальными концепциями искусственных нейронных сетей. Вы изучите базовую архитектуру нейрона, принцип прямого распространения и научитесь обучать простую сеть с помощью алгоритма обратного распространения ошибки.",
  "learning_objectives": [
    "Построить архитектуру полносвязной нейронной сети с нуля, используя NumPy.",
    "Реализовать алгоритм прямого распространения сигнала (forward propagation) для получения предсказаний.",
    "Применить алгоритм обратного распространения ошибки (backpropagation) для вычисления градиентов.",
    "Обучить простую нейронную сеть на синтетическом наборе данных для выполнения задачи бинарной классификации."
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Введение в архитектуру нейронных сетей",
      "lesson_objective": "Сконструировать структуру полносвязной нейронной сети и объяснить функцию каждого компонента.",
      "key_topics": [
        "Биологический прототип и искусственный нейрон",
        "Структура перцептрона: входы, веса, смещение, функция активации",
        "Типы функций активации: сигмоида, ReLU, гиперболический тангенс",
        "Построение слоев: входной, скрытые, выходной слои",
        "Понятие о глубине и ширине сети",
        "Представление сети в виде матричных операций"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет выхода искусственного нейрона",
          "exercise_instructions": "1. Задайте три входных значения (x1, x2, x3) вручную. 2. Инициализируйте случайные веса (w1, w2, w3) и смещение (b). 3. Рассчитайте взвешенную сумму (z). 4. Примените функцию активации sigmoid к значению z для получения выхода нейрона (a). 5. Запишите все вычисления и итоговый результат."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Прямое распространение (Forward Propagation)",
      "lesson_objective": "Реализовать алгоритм прямого распространения для двухслойной сети и получить предсказания по заданным входным данным.",
      "key_topics": [
        "Вычисление выхода для всего слоя с помощью матричного умножения",
        "Последовательный проход данных через слои сети",
        "Практическая реализация на Python с использованием NumPy",
        "Преобразование входных данных в предсказания на выходном слое",
        "Интерпретация выходных значений"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация прямого распространения на NumPy",
          "exercise_instructions": "1. Создайте массив входных данных X (формата 3x2). 2. Инициализируйте матрицы весов W1 (формата 2x4) и W2 (формата 4x1) и векторы смещений b1, b2 случайными значениями. 3. Напишите функцию для вычисления сигмоиды. 4. Реализуйте прямой проход: рассчитайте выход первого скрытого слоя A1 = σ(X * W1 + b1). 5. Рассчитайте итоговый выход сети A2 = σ(A1 * W2 + b2). 6. Выведите полученное предсказание A2."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Обратное распространение ошибки (Backpropagation)",
      "lesson_objective": "Вычислить градиенты функции потерь по всем параметрам сети (весам и смещениям) с помощью алгоритма обратного распространения ошибки.",
      "key_topics": [
        "Функция потерь (бинарная кросс-энтропия)",
        "Интуиция: распространение ошибки от выхода ко входу",
        "Цепное правило для вычисления градиентов",
        "Вывод формул для градиентов на выходном и скрытом слоях",
        "Обновление параметров с использованием вычисленных градиентов"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет градиентов для одного примера",
          "exercise_instructions": "1. Возьмите архитектуру из 1 входного, 1 скрытого (2 нейрона) и 1 выходного нейрона. 2. Зафиксируйте все веса, смещения, входное значение и истинную метку (y_true). 3. Выполните прямой проход и рассчитайте ошибку. 4. Используя цепное правило, вручную рассчитайте градиенты функции потерь по весам и смещениям выходного и скрытого слоев. 5. Запишите все промежуточные вычисления."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Обучение нейронной сети",
      "lesson_objective": "Объединить прямой и обратный проход для реализации полного цикла обучения и обучить сеть на синтетических данных.",
      "key_topics": [
        "Цикл эпохи: forward pass, loss calculation, backward pass, weight update",
        "Градиентный спуск и скорость обучения (learning rate)",
        "Создание синтетического набора данных для классификации",
        "Визуализация процесса обучения и сходимости функции потерь",
        "Оценка точности обученной модели"
      ],
      "exercises": [
        {
          "exercise_title": "Полный цикл обучения на синтетических данных",
          "exercise_instructions": "1. Сгенерируйте синтетический датасет для бинарной классификации с помощью make_moons из sklearn.datasets. 2. Реализуйте класс простой нейронной сети с одним скрытым слоем. 3. Напишите функции для forward propagation, вычисления потерь и backpropagation. 4. Реализуйте цикл обучения на 1000 эпох с фиксированной скоростью обучения. 5. После каждой 100-й эпохи выводите значение функции потерь. 6. Постройте график сходимости функции потерь и визуализируйте решающую границу обученной модели."
        }
      ]
    }
  ]
}