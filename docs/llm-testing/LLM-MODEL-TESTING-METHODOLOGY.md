# LLM Model Testing Methodology

**Version**: 1.0
**Date**: 2025-11-13
**Purpose**: Standardized methodology for evaluating LLM models for course generation tasks

---

## ðŸŽ¯ Core Principle

**Token Count â‰  Quality**

A model generating 5,000 tokens of verbose, repetitive content is **worse** than a model generating 500 tokens of precise, well-structured JSON.

---

## ðŸ“‹ Testing Framework

### Phase 1: Quantitative Metrics (Basic)

**What to Measure:**
- âœ… **Output Tokens**: Raw token count (input + output)
- âœ… **Generation Time**: Duration in milliseconds
- âœ… **Cost**: Estimated cost based on pricing ($input/$output per 1M tokens)
- âœ… **Success Rate**: % of tests that complete without errors

**Tools:**
- Test scripts: `scripts/test-model-*.ts`
- Output logs: `/tmp/*-complete.log`

**Limitations:**
- âŒ Does NOT measure quality
- âŒ Does NOT validate structure
- âŒ Does NOT check schema compliance
- âŒ Does NOT verify content accuracy

---

### Phase 2: Qualitative Analysis (CRITICAL)

**What to Inspect:**

#### 2.1 JSON Structure Validation
```typescript
// Required checks:
- âœ… Valid JSON syntax (no parsing errors)
- âœ… snake_case field naming (not camelCase)
- âœ… All required fields present
- âœ… Correct data types (string, number, array, object)
- âœ… No extra/unexpected fields
```

**Example Issues:**
```json
// âŒ BAD: camelCase fields
{
  "courseTitle": "...",
  "learningObjectives": []
}

// âœ… GOOD: snake_case fields
{
  "course_title": "...",
  "learning_outcomes": []
}
```

#### 2.2 Content Quality Assessment

**For Metadata (T1, T2):**
| Field | Quality Criteria |
|-------|-----------------|
| `course_title` | 10-100 chars, descriptive, no filler words |
| `course_description` | 50-500 chars, engaging, clear value proposition |
| `course_overview` | 500-3000 chars, detailed but not verbose, structured |
| `target_audience` | Specific personas, clear prerequisites |
| `learning_outcomes` | 3-8 outcomes, SMART format, Bloom's taxonomy levels |
| `prerequisites` | 0-5 items, realistic, not overly restrictive |
| `course_tags` | 5-15 tags, relevant keywords |

**For Lesson Structure (T3, T4):**
| Field | Quality Criteria |
|-------|-----------------|
| `section_title` | Clear, specific topic |
| `section_description` | 100-500 chars, motivational |
| `lessons` | **3-5 lessons** (NOT 1!), each with full structure |
| `lesson_objectives` | 2-5 per lesson, measurable, progressive difficulty |
| `key_topics` | 3-8 topics, specific concepts (not generic) |
| `exercises` | 1-3 per lesson, actionable, clear instructions |

#### 2.3 Language Quality

**For English (en):**
- âœ… Natural, fluent grammar
- âœ… Professional tone
- âœ… Technical terms used correctly
- âŒ No machine translation artifacts

**For Russian (ru):**
- âœ… Native Russian phrasing (not translated from English)
- âœ… Proper declensions and conjugations
- âœ… Appropriate technical terminology
- âŒ No word-for-word translations

#### 2.4 Content Depth Analysis

**Verbosity vs Value:**

| Metric | Measurement | Interpretation |
|--------|-------------|----------------|
| **Info Density** | Unique concepts per 100 tokens | Higher = better |
| **Repetition Rate** | % of repeated phrases | Lower = better |
| **Filler Words** | Count of "very", "really", etc. | Lower = better |
| **Example Quality** | Specificity of examples | Concrete > Generic |

**Example Comparison:**

```json
// âŒ LOW QUALITY (1000 tokens, low density)
{
  "course_overview": "This course is really, really great. It will teach you many things. You will learn a lot. It's very comprehensive. You'll gain knowledge about various topics. The course covers everything you need. It's very detailed and thorough..."
}

// âœ… HIGH QUALITY (300 tokens, high density)
{
  "course_overview": "Master Python fundamentals through 12 hands-on projects: build a web scraper, REST API, data analyzer, and CLI tool. Learn variables, control flow, functions, OOP, and file I/O with immediate application. Includes debugging techniques and PEP8 best practices."
}
```

---

## ðŸ”¬ Quality Scoring System

### Schema Compliance Score (0-1.0)

```typescript
function calculateSchemaScore(output: any): number {
  let score = 0;
  const weights = {
    validJSON: 0.25,           // Parses without errors
    correctFields: 0.25,       // All required fields present
    correctTypes: 0.25,        // Data types match schema
    namingConvention: 0.25     // snake_case throughout
  };

  if (isValidJSON(output)) score += weights.validJSON;
  if (hasAllRequiredFields(output)) score += weights.correctFields;
  if (hasCorrectDataTypes(output)) score += weights.correctTypes;
  if (usesSnakeCase(output)) score += weights.namingConvention;

  return score;
}
```

### Content Quality Score (0-1.0)

```typescript
function calculateContentScore(output: any, scenario: 'metadata' | 'lesson'): number {
  let score = 0;

  if (scenario === 'metadata') {
    // Check learning outcomes quality
    if (hasActionVerbs(output.learning_outcomes)) score += 0.2;
    if (hasBloomsTaxonomy(output.learning_outcomes)) score += 0.2;

    // Check description depth
    if (output.course_overview.length >= 500) score += 0.2;
    if (hasSpecificExamples(output.course_overview)) score += 0.2;

    // Check target audience specificity
    if (hasPersonaDetails(output.target_audience)) score += 0.2;
  }

  if (scenario === 'lesson') {
    // Check lesson count
    if (output.lessons.length >= 3 && output.lessons.length <= 5) score += 0.3;
    else if (output.lessons.length === 1) score += 0.0; // Major penalty!

    // Check objectives quality
    if (allLessonsHaveObjectives(output.lessons)) score += 0.2;

    // Check exercise quality
    if (allLessonsHaveExercises(output.lessons)) score += 0.3;

    // Check topic specificity
    if (hasSpecificTopics(output.lessons)) score += 0.2;
  }

  return Math.min(1.0, score);
}
```

### Overall Quality Score

```
Overall = (Schema Score Ã— 0.4) + (Content Score Ã— 0.4) + (Language Score Ã— 0.2)
```

**Quality Tiers:**
- **S-Tier**: 0.90-1.00 (Production-ready)
- **A-Tier**: 0.75-0.89 (Good, minor issues)
- **B-Tier**: 0.60-0.74 (Acceptable, needs improvement)
- **C-Tier**: 0.40-0.59 (Poor quality)
- **F-Tier**: 0.00-0.39 (Unusable)

---

## ðŸ“Š Ranking Methodology

### Step 1: Separate by Task Type

**Metadata Generation:**
- Models optimized for detailed descriptions
- Higher token counts may indicate thoroughness

**Lesson Structure Generation:**
- Models optimized for hierarchical JSON
- Critical: Must generate 3-5 lessons, not 1!

### Step 2: Multi-Dimensional Ranking

**For Each Model, Calculate:**

1. **Quality Score** (0-1.0)
   - Schema compliance
   - Content quality
   - Language accuracy

2. **Efficiency Score** (0-1.0)
   - Quality per token
   - Quality per dollar
   - Quality per second

3. **Consistency Score** (0-1.0)
   - Success rate across all tests
   - Variance in quality scores
   - Retry requirements

4. **Composite Score**
   ```
   Composite = (Quality Ã— 0.5) + (Efficiency Ã— 0.3) + (Consistency Ã— 0.2)
   ```

### Step 3: Task-Specific Rankings

**Metadata Ranking:**
```
Rank by: Composite Score
Tiebreaker: Output token count (higher = more detailed)
```

**Lesson Structure Ranking:**
```
Rank by: Composite Score
Tiebreaker: Lesson count (3-5 preferred)
Penalty: -0.5 if only 1 lesson generated
```

---

## âš ï¸ Common Pitfalls to Avoid

### 1. Token Count Fallacy
âŒ **WRONG**: "Model A generates 5000 tokens, Model B generates 500, so A is better"
âœ… **CORRECT**: Read actual outputs and compare information density

### 2. Success Rate Illusion
âŒ **WRONG**: "Model passed 4/4 tests, so it's production-ready"
âœ… **CORRECT**: Check if outputs meet quality standards, not just parse

### 3. Pricing Myopia
âŒ **WRONG**: "Model A is cheapest, so use it everywhere"
âœ… **CORRECT**: Factor in quality - poor quality = wasted money

### 4. Single-Test Bias
âŒ **WRONG**: "Model performed well on English test, assume Russian is fine"
âœ… **CORRECT**: Test all language/scenario combinations separately

### 5. Schema Compromise
âŒ **WRONG**: "Model uses camelCase, close enough to snake_case"
âœ… **CORRECT**: Schema compliance is binary - either correct or incorrect

---

## ðŸ› ï¸ Improved Testing Workflow

### Current Issues (2025-11-13)

Our test scripts measure **quantity** but not **quality**:

```typescript
// âŒ What we currently do:
console.log(`SUCCESS - ${tokens.output} output tokens`);

// âœ… What we SHOULD do:
const quality = analyzeQuality(response);
console.log(`SUCCESS - ${tokens.output} tokens, Quality: ${quality.toFixed(2)}`);
```

### Recommended Improvements

1. **Save Full Outputs**
   ```typescript
   // In test scripts:
   writeFileSync(`/tmp/model-${modelName}-T${testId}-output.json`, response);
   ```

2. **Automated Quality Analysis**
   ```bash
   # After tests complete:
   pnpm tsx scripts/analyze-quality.ts /tmp/*-output.json
   ```

3. **Side-by-Side Comparison**
   ```bash
   # Compare two models:
   pnpm tsx scripts/compare-models.ts model-a model-b
   ```

4. **Visual Diff Tool**
   - Show differences in JSON structure
   - Highlight schema violations
   - Compare learning outcomes quality

---

## ðŸ“ Test Case Design Principles

### 1. Representative Scenarios

**Metadata Tests:**
- T1: English, Beginner, Technical (e.g., "Introduction to Python")
- T2: Russian, Intermediate, Conceptual (e.g., "Machine Learning Basics")

**Lesson Tests:**
- T3: English, Programming, Hands-on (e.g., "Variables in Python")
- T4: Russian, Theory, Conceptual (e.g., "Neural Networks Fundamentals")

### 2. Consistent Prompts

All models MUST receive **identical prompts** for fair comparison.

**Template Requirements:**
- Explicit JSON schema in prompt
- Clear field naming convention (snake_case)
- Specific token length expectations
- Example structure (optional)

### 3. Controlled Variables

**Fixed:**
- Temperature: 0.7 (balance creativity/consistency)
- Max tokens: 8000 (sufficient for all scenarios)
- Language: Explicit in prompt
- RAG context: 0 tokens (for initial tests)

**Variable:**
- Model name
- Pricing
- Context window size

---

## ðŸ“ˆ Reporting Standards

### Minimum Report Contents

1. **Executive Summary**
   - Models tested
   - Test scenarios
   - Top 3 recommendations

2. **Quantitative Results Table**
   ```
   | Model | T1 Tokens | T2 Tokens | T3 Tokens | T4 Tokens | Avg Cost | Success Rate |
   ```

3. **Qualitative Analysis**
   - Schema compliance issues
   - Content quality observations
   - Language accuracy notes

4. **Sample Outputs**
   - Best example (highest quality)
   - Worst example (lowest quality)
   - Typical example (median quality)

5. **Recommendations**
   - By use case
   - By budget constraint
   - By quality requirement

---

## ðŸ”„ Continuous Improvement

### Model Retesting Triggers

Retest models when:
- âœ… New model version released
- âœ… Pricing changes significantly (>20%)
- âœ… Schema requirements updated
- âœ… New language support needed
- âœ… Quality issues reported in production

### Benchmark Evolution

Update test cases when:
- âœ… New course types introduced
- âœ… User feedback indicates gaps
- âœ… Competitor models set new standards

---

## ðŸ“š Reference: Previous Mistakes

### Case Study 1: deepseek-chat-v3.1 Misjudgment

**Initial Assessment:**
- âœ… 4/4 tests passed
- âœ… Fast generation (13.8s avg)
- â­ Rated as "S-TIER"

**Actual Quality (After Review):**
- âŒ Quality score: 0.80 (not 1.00!)
- âŒ Only 1 lesson generated (not 3-5)
- âŒ Used camelCase fields
- âŒ Output: 463 tokens (6x less than kimi-k2-thinking)

**Lesson Learned:**
> "Success rate alone is meaningless. Always inspect actual outputs."

### Case Study 2: Token Count Overvaluation

**Initial Ranking:**
1. qwen3-235b-thinking: 4,927 tokens (metadata)
2. kimi-k2-thinking: 4,259 tokens (metadata)

**After Quality Review:**
- Both had excellent quality (1.00)
- qwen3-235b: More verbose descriptions
- kimi-k2: More structured, concise

**Lesson Learned:**
> "Higher token count may indicate verbosity, not quality. Prioritize information density."

---

## âœ… Quality Checklist for Manual Review

Before finalizing model rankings, verify:

### Metadata Outputs (T1, T2)
- [ ] `course_description` is engaging (not generic)
- [ ] `course_overview` has specific examples
- [ ] `learning_outcomes` use action verbs
- [ ] `learning_outcomes` follow Bloom's taxonomy
- [ ] `target_audience` defines clear personas
- [ ] `prerequisites` are realistic
- [ ] All fields use snake_case

### Lesson Outputs (T3, T4)
- [ ] Generated 3-5 lessons (NOT 1!)
- [ ] Each lesson has 2-5 objectives
- [ ] Objectives are measurable
- [ ] Topics are specific (not generic)
- [ ] Exercises have clear instructions
- [ ] Estimated durations are reasonable
- [ ] All fields use snake_case

### Language Quality
- [ ] Grammar is natural (not machine-translated)
- [ ] Technical terms used correctly
- [ ] Russian uses native phrasing (if applicable)
- [ ] No repetitive filler words

---

## ðŸŽ“ Conclusion

Effective LLM model evaluation requires:

1. **Quantitative metrics** (tokens, cost, speed) for initial filtering
2. **Qualitative analysis** (structure, content, language) for final ranking
3. **Task-specific criteria** (metadata vs lessons have different quality standards)
4. **Continuous validation** (retest as models evolve)

**Golden Rule:**
> Read the actual outputs. Numbers lie, JSON doesn't.

---

**Document Owner**: Claude Code
**Last Updated**: 2025-11-13
**Next Review**: When new models are released or quality issues arise
