# T075.17: Large PDF File Support via Chunking (STANDARD + PREMIUM)

**Task ID**: T075.17-PDF-CHUNKING
**Parent**: T075 (RAG Implementation)
**Tiers**: STANDARD (>10 MB) + PREMIUM (>10 MB)
**Priority**: High (Stage 2 - after STANDARD tier RAG)
**Status**: ⏸️ Deferred to Stage 2
**Created**: 2025-10-27

---

## Executive Summary

Implement PDF chunking strategy to enable users to upload and process PDF files larger than 10 MB (up to 100 MB for PREMIUM tier). This involves splitting large PDFs into smaller chunks (~10 MB each), processing them through Docling independently, and merging the markdown results.

**Current Limitation**:
- Docling MCP server timeout on files > 10-15 MB (~3+ minutes processing time)
- Increased timeout to 20 minutes helps, but files > 15-20 MB still risk timeout

**Solution**: PDF splitting → chunk processing → markdown merging

---

## Tier-Specific Behavior

### STANDARD Tier (10 MB limit)
- **Scenario**: User uploads 15 MB PDF
- **Action**: Show error message: "File too large. Please use a PDF under 10 MB or upgrade to PREMIUM tier for files up to 100 MB"
- **No chunking** for STANDARD tier in initial implementation

### PREMIUM Tier (100 MB limit)
- **Scenario 1**: User uploads 8 MB PDF → Direct processing (no chunking)
- **Scenario 2**: User uploads 25 MB PDF → Automatic chunking (3 chunks × ~8 MB)
- **Scenario 3**: User uploads 100 MB PDF → Automatic chunking (10 chunks × 10 MB)
- **Chunking threshold**: Files > 10 MB

---

## Background Research

### Performance Data (ArXiv 2408.09869)

**Docling Processing Speed**:
- **CPU (8 cores)**: 0.32 pages/sec → 100 страниц = ~5 минут
- **MacBook M3 Max**: 0.79 pages/sec → 100 страниц = ~2 минуты
- **Nvidia L4 GPU**: 2.08 pages/sec → 100 страниц = ~48 секунд

**Real-world Examples** (GitHub Issues):
- **63 MB PDF**: 17.5 минут (CPU с OCR)
- **300 страниц PDF**: 10-20 минут
- **3000+ страниц PDF**: 8+ часов (не завершилось!)

**Current Setup**:
- Timeout: 20 minutes (1200000ms)
- Can process: ~10-15 MB files reliably
- Target: Support up to 100 MB for PREMIUM

### Community Solutions

**Proven Approach** (from GitHub Issue #1283):
> "I wrote a simple script to convert my large PDF into smaller PDFs with less pages and then converted them."

This is the **community-validated solution** that works reliably.

---

## Technical Implementation

### Core Module: PDF Splitter

```typescript
// packages/course-gen-platform/src/shared/docling/pdf-splitter.ts

import { PDFDocument } from 'pdf-lib';
import fs from 'fs/promises';
import path from 'path';
import type { DoclingClient } from './client';

/**
 * Split large PDF into chunks by size
 *
 * @param pdfPath - Path to PDF file
 * @param maxSizeMB - Maximum chunk size in MB (default: 10)
 * @param outputDir - Directory for chunk files
 * @returns Array of chunk file paths
 */
export async function splitPdfBySize(
  pdfPath: string,
  maxSizeMB: number = 10,
  outputDir: string
): Promise<string[]> {
  const pdfBytes = await fs.readFile(pdfPath);
  const pdfDoc = await PDFDocument.load(pdfBytes);
  const totalPages = pdfDoc.getPageCount();

  const chunks: string[] = [];
  let currentChunkStart = 0;
  let currentChunkDoc = await PDFDocument.create();
  let currentChunkSize = 0;

  for (let i = 0; i < totalPages; i++) {
    const [page] = await currentChunkDoc.copyPages(pdfDoc, [i]);
    currentChunkDoc.addPage(page);

    // Estimate size (rough approximation)
    const tempBytes = await currentChunkDoc.save();
    currentChunkSize = tempBytes.length / (1024 * 1024); // MB

    // If chunk exceeds limit or last page, save chunk
    if (currentChunkSize >= maxSizeMB || i === totalPages - 1) {
      const chunkPath = path.join(outputDir, `chunk_${currentChunkStart}-${i}.pdf`);
      await fs.writeFile(chunkPath, await currentChunkDoc.save());
      chunks.push(chunkPath);

      console.log(`Created chunk ${chunks.length}: pages ${currentChunkStart}-${i} (${currentChunkSize.toFixed(2)} MB)`);

      // Start new chunk
      currentChunkStart = i + 1;
      currentChunkDoc = await PDFDocument.create();
      currentChunkSize = 0;
    }
  }

  return chunks;
}

/**
 * Process large PDF by splitting into chunks
 *
 * @param pdfPath - Path to large PDF file
 * @param doclingClient - Configured Docling client
 * @param maxSizeMB - Maximum chunk size (default: 10)
 * @returns Combined markdown content
 */
export async function processLargePdf(
  pdfPath: string,
  doclingClient: DoclingClient,
  maxSizeMB: number = 10
): Promise<string> {
  const tempDir = path.join('/tmp', `pdf-chunks-${Date.now()}`);
  await fs.mkdir(tempDir, { recursive: true });

  try {
    // Step 1: Split PDF
    console.log(`Splitting PDF into ${maxSizeMB}MB chunks...`);
    const chunks = await splitPdfBySize(pdfPath, maxSizeMB, tempDir);
    console.log(`Created ${chunks.length} chunks`);

    // Step 2: Process each chunk sequentially
    const markdownResults: string[] = [];
    for (let i = 0; i < chunks.length; i++) {
      console.log(`Processing chunk ${i + 1}/${chunks.length}...`);
      const markdown = await doclingClient.convertToMarkdown(chunks[i]);
      markdownResults.push(markdown);
    }

    // Step 3: Combine results with page continuity markers
    const combinedMarkdown = markdownResults.join('\n\n---\n\n');

    console.log(`Successfully processed ${chunks.length} chunks`);
    return combinedMarkdown;

  } finally {
    // Cleanup temporary files
    await fs.rm(tempDir, { recursive: true, force: true });
  }
}
```

### Integration into Document Processing

```typescript
// packages/course-gen-platform/src/orchestrator/handlers/document-processing.ts

import { processLargePdf } from '../../shared/docling/pdf-splitter';

// In processDocument function (around line 60-80):
if (fileMimeType === 'application/pdf') {
  const fileStats = await fs.stat(uploadedFilePath);
  const fileSizeMB = fileStats.size / (1024 * 1024);

  // Check tier-based limits
  if (tier === 'premium' && fileSizeMB > 10) {
    // PREMIUM tier: Use chunking for large files
    console.log(`Large PDF detected (${fileSizeMB.toFixed(2)} MB), using chunked processing...`);
    markdown = await processLargePdf(uploadedFilePath, doclingClient, 10);
  } else if (tier !== 'premium' && fileSizeMB > 10) {
    // STANDARD tier: Reject files > 10 MB
    throw new Error(
      `PDF file is too large (${fileSizeMB.toFixed(1)} MB). ` +
      `STANDARD tier supports PDF files up to 10 MB. ` +
      `Please reduce file size or upgrade to PREMIUM tier for files up to 100 MB.`
    );
  } else {
    // Both tiers: Direct processing for small files
    markdown = await doclingClient.convertToMarkdown(uploadedFilePath);
  }
} else {
  // Other formats (DOCX, PPTX, HTML, TXT, MD)
  markdown = await doclingClient.convertToMarkdown(uploadedFilePath);
}
```

---

## Dependencies

### NPM Packages

```bash
pnpm add pdf-lib --filter @megacampus/course-gen-platform
```

**pdf-lib**: Pure JavaScript PDF manipulation library
- Size: ~500 KB
- Zero native dependencies
- Works in Node.js and browsers
- MIT License

### Existing Infrastructure

✅ Already available:
- Docling MCP client (`src/shared/docling/client.ts`)
- Document processing handler (`src/orchestrator/handlers/document-processing.ts`)
- BullMQ queues (optional, for future async processing)

---

## Implementation Plan

### Phase 1: Core PDF Splitting (2 days)
1. ✅ Install `pdf-lib` dependency
2. ✅ Create `pdf-splitter.ts` module
3. ✅ Implement `splitPdfBySize()` function
4. ✅ Implement `processLargePdf()` function
5. ✅ Unit tests for splitting logic

### Phase 2: Integration (1 day)
6. ✅ Integrate into `document-processing.ts`
7. ✅ Add tier check (PREMIUM only for files > 10 MB)
8. ✅ Add file size validation with clear error messages
9. ✅ Integration tests with 15-100 MB PDFs

### Phase 3: Testing (1 day)
10. ✅ Test with STANDARD tier (10 MB limit enforcement)
11. ✅ Test with PREMIUM tier (chunking for 15-100 MB files)
12. ✅ Error handling and cleanup verification

**Total Estimated Time**: 4 days

---

## Testing Strategy

### Test Files Needed

1. **8 MB PDF** - Small file (no chunking needed)
2. **15 MB PDF** - Medium file (2 chunks)
3. **25 MB PDF** - Large file (3 chunks)
4. **50 MB PDF** - Very large file (5 chunks)
5. **100 MB PDF** - Maximum file (10 chunks)

### Test Scenarios

**STANDARD Tier**:
- ✅ 8 MB PDF → Process successfully (direct)
- ❌ 15 MB PDF → Reject with upgrade message

**PREMIUM Tier**:
- ✅ 8 MB PDF → Process successfully (direct, no chunking)
- ✅ 15 MB PDF → Process successfully (2 chunks)
- ✅ 25 MB PDF → Process successfully (3 chunks)
- ✅ 50 MB PDF → Process successfully (5 chunks)
- ✅ 100 MB PDF → Process successfully (10 chunks)

**Error Handling**:
- ❌ Chunk processing fails → Proper error message
- ✅ Temporary files cleanup → No leftover files
- ✅ Markdown merging → Page continuity preserved

---

## Success Criteria

1. ✅ PREMIUM tier can upload PDFs up to 100 MB
2. ✅ STANDARD tier shows clear error for files > 10 MB
3. ✅ Files > 10 MB automatically use chunking (PREMIUM only)
4. ✅ All chunks processed successfully (< 20 minutes total)
5. ✅ Markdown results merged correctly (page continuity preserved)
6. ✅ Temporary files cleaned up after processing
7. ✅ Error handling with rollback on failure

---

## Expected Processing Times

**Sequential Processing** (current implementation):
- 15 MB PDF (2 chunks) → **6-8 минут**
- 25 MB PDF (3 chunks) → **9-12 минут**
- 50 MB PDF (5 chunks) → **15-20 минут**
- 100 MB PDF (10 chunks) → **30-40 минут** ⚠️ (exceeds 20min timeout)

**Future Optimization** (parallel processing):
- 100 MB PDF (10 chunks, 4 concurrent workers) → **10-15 минут**

---

## Future Enhancements (Optional)

### Phase 4: Parallel Processing (2 days)
- Process 4 chunks concurrently
- Reduces 100 MB file processing to ~10-15 minutes
- Requires careful resource management

### Phase 5: Progress Tracking (1 day)
- Real-time progress updates via WebSocket
- Show "Processing chunk 3/10..." to user
- Better UX for large files

### Phase 6: Chunk Caching (1 day)
- Cache processed chunks (avoid reprocessing on retry)
- Speeds up error recovery
- Reduces server load

---

## Risks & Mitigations

**Risk 1**: PDF splitting loses structure (headings, tables across page boundaries)
**Mitigation**:
- Test with complex PDFs containing multi-page tables
- Validate markdown output quality
- Use page boundary detection (avoid splitting mid-table)

**Risk 2**: 100 MB files exceed 20-minute timeout (sequential processing)
**Mitigation**:
- Phase 4: Implement parallel processing (4 concurrent workers)
- Alternative: Move to async queue processing (BullMQ)
- Show realistic processing time estimates to users

**Risk 3**: High memory usage for multiple concurrent chunks
**Mitigation**:
- Limit concurrency to 4 workers maximum
- Monitor memory usage in production
- Implement queue-based processing if needed

---

## Alternatives Considered

### ❌ Option A: Just Increase Timeout to 60 minutes
- **Problem**: Not scalable, users wait too long
- **Verdict**: Rejected

### ❌ Option B: GPU Acceleration
- **Cost**: $1-2/hour (AWS g6.2xlarge)
- **Speedup**: 6.5x faster
- **Verdict**: Good for future optimization (Stage 4), not immediate solution

### ✅ Option C: PDF Chunking (SELECTED)
- **Proven**: Community uses this approach successfully
- **Cost**: Zero infrastructure changes
- **Scalable**: Works with existing CPU setup
- **Verdict**: Best immediate solution

---

## Related Tasks

- **T075**: RAG Implementation (parent task)
- **T074.1.2**: Docling MCP Server Integration (✅ completed)
- **T075.18**: Image Processing Research (PREMIUM tier, deferred)
- **T075.9-PREMIUM**: Reranking (deferred)

---

## References

- **Research Document**: `docs/investigations/docling-large-files-research.md`
- **Pricing Tiers**: `docs/PRICING-TIERS.md` (line 477-481: PDF Chunking Pre-processor)
- **Docling Client**: `packages/course-gen-platform/src/shared/docling/client.ts`
- **GitHub Issues**:
  - [Issue #568](https://github.com/docling-project/docling/issues/568) - Performance Degradation
  - [Issue #1283](https://github.com/docling-project/docling/issues/1283) - Converter Gets Stuck (community solution)
- **ArXiv Paper**: [2408.09869](https://arxiv.org/html/2408.09869v4) - Docling Benchmarks

---

**Next Steps**:
1. Defer to Stage 2 (after STANDARD tier RAG implementation - T075)
2. Add to backlog for both STANDARD (error handling) and PREMIUM (chunking) tiers
3. Revisit when users start uploading files > 10 MB
