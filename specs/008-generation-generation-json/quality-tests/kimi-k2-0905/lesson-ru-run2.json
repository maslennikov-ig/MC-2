{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает фундаментальные концепции нейронных сетей: биологическую мотивацию, математическую формализацию перцептрона, механизм обратного распространения ошибки и базовые архитектуры. Приводится сравнение линейных и нелинейных моделей, объясняется роль функций активации и инициализации весов. Для каждой темы приведены числовые примеры расчётов и блок-схемы алгоритмов.",
  "learning_objectives": [
    "Студент сможет вычислить выход перцептрона вручную для заданных весов и входов",
    "Студент сможет провести один шаг обратного распространения ошибки и обновить веса",
    "Студент сможет объяснить, почему ReLU решает проблему исчезающего градиента",
    "Студент сможет выбрать архитектуру сети под задачу бинарной классификации"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному: формализация Мак-Каллока-Питтса",
      "lesson_objective": "Студент построит искусственный нейрон, повторяющий логические функции И и ИЛИ, и вычислит пороговое значение вручную",
      "key_topics": [
        "Бионическая модель: аксоны, дендриты, синапсы",
        "Уравнение Мак-Каллока-Питтса: weighted sum и порог",
        "Функции активации: хевисайд и сигмоида",
        "Геометрическая интерпретация: разделяющая гиперплоскость",
        "Пример: нейрон для бинарной классификации цветков ириса"
      ],
      "exercises": [
        {
          "exercise_title": "Построение нейрона для логической функции XOR",
          "exercise_instructions": "Заданы входы x1, x2 ∈ {0,1}. Подберите веса w1, w2 и порог b так, чтобы нейрон на хевисайде выдавал 1 только при (0,1) или (1,0). Проверьте все четыре комбинации вручную и запишите итоговую таблицу истинности."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Механизм обратного распространения: цепное правило на числовом примере",
      "lesson_objective": "Студент проведёт полный цикл forward и backward pass для сети 2-2-1 и обновит веса по правилу дельта",
      "key_topics": [
        "Цепное правило для частных производных",
        "Градиент потерь MSE по весам скрытого слоя",
        "Вычислительный граф: от потерь к входу",
        "Пример: ручной расчёт для входа [-1, 0.5] и цели 1",
        "Почему градиенты уменьшаются ближе ко входу"
      ],
      "exercises": [
        {
          "exercise_title": "Расчёт градиента для нейрона с сигмоидой",
          "exercise_instructions": "Дан нейрон y = σ(w·x+b), где x=0.8, w=0.3, b=-0.2, цель t=1. Вычислите выход нейрона, затем ∂L/∂w и ∂L/∂b при L=½(t-y)². Производную σ′ запишите через саму σ. Округлите результаты до 4 знаков."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции активации: сравнительный анализ ReLU, Leaky-ReLU и сигмоиды",
      "lesson_objective": "Студент построит графики производных и выявит зоны затухающего градиента для каждой функции",
      "key_topics": [
        "Проблема исчезающего градиента в глубоких сетях",
        "ReLU: преимущества и «умирание» нейронов",
        "Leaky-ReLU: параметр 0.01 и его влияние",
        "Сигмоида и гиперболический тангенс: области насыщения",
        "Эксперимент: распределение градиентов по слоям"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение скорости сходимости на наборе MNIST-2-класса",
          "exercise_instructions": "Обучите однослойную сеть 784-50-1 три раза: с сигмоидой, ReLU и Leaky-ReLU. Используйте скорость обучения 0.01, 5 эпох, батч 64. Запишите финальные значения функции потерь и точности. Постройте график потерь по итерациям и сделайте вывод о скорости сходимости."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Инициализация весов: почему случайные числа влияют на обучение",
      "lesson_objective": "Студент сравнит Xavier и He инициализацию и вычислит дисперсию выходов每层",
      "key_topics": [
        "Дисперсия выхода как критерий инициализации",
        "Xavier: учёт размера входа и выхода",
        "He: коррекция для ReLU",
        "Эксперимент: 10-слойная сеть с тождественной функцией",
        "Практическое правило: sqrt(2/(fan_in+fan_out))"
      ],
      "exercises": [
        {
          "exercise_title": "Измерение дисперсии после инициализации",
          "exercise_instructions": "Создайте линейный стек 5 слоёв по 256 нейронов. Проинициализируйте веса стандартным нормальным распределением, затем Xavier и He. Пропустите через сеть 1000 случайных векторов из N(0,1). Для каждого слоя вычислите выборочную дисперсию выходов и постройте гистограмму. Сделайте вывод, какая инициализация сохраняет дисперсию ближе к 1."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Архитектура «вход-скрытое-выход»: выбор размерности скрытого слоя",
      "lesson_objective": "Студент обоснует выбор числа нейронов в скрытом слое исходя из переобучения и недообучения на валидационной кривой",
      "key_topics": [
        "Правило «среднего» между входом и выходом",
        "Кривые обучения: тренировочная vs валидационная ошибка",
        "Роль регуляризации: dropout 0.5 как способ борьбы с переобучением",
        "Пример: предсказание возраста по 20 признакам",
        "Таблица рекомендованных размерностей под задачу"
      ],
      "exercises": [
        {
          "exercise_title": "Подбор оптимального размера скрытого слоя",
          "exercise_instructions": "На наборе California Housing обучите сеть с 1 скрытым слоем, перебрав число нейронов: 4, 8, 16, 32, 64, 128. Используйте 80/20 split, MSE--loss, 100 эпох, раннюю остановку по валидации с patience=10. Постройте график валидационной MAE vs число нейронов. Укажите минимум и сделайте вывод о переобучении."
        }
      ]
    }
  ]
}