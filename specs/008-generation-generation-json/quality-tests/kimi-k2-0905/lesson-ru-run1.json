{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает фундаментальные концепции нейронных сетей: биологическую мотивацию, математическую формализацию, архитектуру персептрона и базовые алгоритмы обучения. Студенты научатся связывать биологические нейроны с искусственными моделями, вычислять выходы нейрона вручную и программно, анализировать влияние весов и порогов на классификацию и визуализировать процесс градиентного спуска на поверхности потерь.",
  "learning_objectives": [
    "Объяснять биологическую мотивацию искусственных нейронов, приводя примеры сенсорных и моторных нейронов человека",
    "Вычислять выход линейного и нелинейного нейрона заданных весов и входов без калькулятора",
    "Реализовать на Python функцию активации (ступенчатую, сигмоиду, ReLU) и применить её к вектору данных",
    "Строить график поверхности потерь MSE для линейно-разделимых данных и отмечать на нём шаги градиентного спуска",
    "Классифицировать 2D-точки вручную с помощью обученного персептрона и проверить результат кодом"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к формальному нейрону МакКаллока-Питтса",
      "lesson_objective": "Построить и запрограммировать формальный нейрон МакКаллока-Питтса, демонстрирующий логические операции И, ИЛИ, НЕ на входных бинарных векторах",
      "key_topics": ["Структура биологического нейрона: дендриты, сомата, аксон", "Модель МакКаллока-Питтса: взвешенная сумма и пороговая функция", "Булевы функции как задача классификации", "Пространство весов и гиперплоскость решений", "Ограничения линейной модели: проблема XOR"],
      "exercises": [
        {
          "exercise_title": "Ручной запуск нейрона И",
          "exercise_instructions": "Заполните таблицу истинности для нейрона с весами w1=1, w2=1, порогом θ=1.5. Подтвердите, что выход совпадает с операцией И для всех четырёх комбинаций входов."
        },
        {
          "exercise_title": "Кодирование нейрона НЕ",
          "exercise_instructions": "Напишите функцию на Python, принимающую один вход x и возвращающую НЕ x с помощью нейрона МакКаллока-Питтса. Подберите вес и порог вручную и проверьте на x=0 и x=1."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации: от ступеньки до сигмоиды и ReLU",
      "lesson_objective": "Реализовать и визуализировать на графике четыре функции активации, сравнив их производные и объяснив, почему ReLU ускоряет обучение глубоких сетей",
      "key_topics": ["Проблема дифференцируемости ступенчатой функции", "Сигмоида: формула, производная, эффект затухающего градиента", "Гиперболический тангенс: центрирование данных около нуля", "ReLU: формула, производная, проблема «мёртвых» нейронов", "Leaky-ReLU и Parametric-ReLU как решение проблемы"],
      "exercises": [
        {
          "exercise_title": "График производной сигмоиды",
          "exercise_instructions": "Постройте на одном графике сигмоиду σ(x) и её производную σ′(x) для x∈[−5,5]. Отметьте точку максимума производной и вычислите её значение."
        },
        {
          "exercise_title": "Сравнение скорости сходимости",
          "exercise_instructions": "Сгенерируйте 1000 случайных точек 2D-данных. Обучите однослойную сеть с сигмоидой и ReLU, измерив число эпох до достижения accuracy 0.95. Сравните результаты."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Персептрон Розенблатта и правило обучения",
      "lesson_objective": "Реализовать персептрон Розенблатта с правилом обновления весов, обучить его на линейно-разделимом датасете и визуализировать динамику изменения разделяющей прямой",
      "key_topics": ["Алгоритм персептрона: инициализация, предсказание, обновление", "Правило обучения: Δw = η (y_true − y_pred) x", "Концепция margin и сходимости при линейной разделимости", "Эпохи и скорость обучения η: эффект на скорость сходимости", "Пределы однослойного персептрона: пример XOR"],
      "exercises": [
        {
          "exercise_title": "Ручное обучение на трёх точках",
          "exercise_instructions": "Даны точки (0,0) класс 0, (1,0) класс 1, (0,1) класс 1. Выполните три шага персептрона вручную при η=0.5, начиная с весов [0,0] и смещения 0. Запишите новые веса."
        },
        {
          "exercise_title": "Визуализация разделяющей линии",
          "exercise_instructions": "Напишите скрипт Python, который после каждой эпохи рисует точки данных и текущую разделяющую прямую персептрона. Сохраните анимацию GIF, показывающую процесс сходимости."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Градиентный спуск и поверхность потерь MSE",
      "lesson_objective": "Построить 3D-график поверхности потерь MSE для линейного нейрона, реализовать градиентный спуск вручную для пяти шагов и показать траекторию на поверхности",
      "key_topics": ["Функция потерь MSE: формула и градиент по весам", "Локальные и глобальные минимумы: визуализация на 2D-сечении", "Скорость обучения: слишком большая vs слишком маленькая", "Градиентный спуск с импульсом: формула и интуиция", "Адаптивные скорости: AdaGrad, RMSProp, Adam (обзор)"],
      "exercises": [
        {
          "exercise_title": "Вычисление градиента вручную",
          "exercise_instructions": "Для нейрона с одним входом и MSE-функцией потерь выведите формулу градиента ∂L/∂w. Подставьте x=2, y_true=3, w=0, вычислите численное значение градиента."
        },
        {
          "exercise_title": "Траектория градиентного спуска на 3D-мesh",
          "exercise_instructions": "С помощью matplotlib постройте 3D-график MSE(w1,w2) для синтетического набора из 20 точек. Реализуйте градиентный спуск, сохраняйте координаты (w1,w2,L) и нарисуйте траекторию красной линией."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Многослойный перцептрон: объединение нейронов в сеть",
      "lesson_objective": "Собрать многослойный перцептрон из реализованных нейронов, обучить его на нелинейно-разделимом круге в квадрате и доказать, что скрытый слой решает проблему XOR",
      "key_topics": ["Архитектура: входной, скрытый, выходной слои", "Прямое распространение: матричные乘法和 векторизация", "Проблема XOR: геометрическое доказательство необходимости скрытого слоя", "Инициализация весов: случайная vs Xavier vs He", "Количество скрытых нейронов: underfitting vs overfitting"],
      "exercises": [
        {
          "exercise_title": "Ручной forward-pass двухслойной сети",
          "exercise_instructions": "Дана сеть 2-2-1 с весами W1=[[1,−1],[1,−1]], b1=[0,1], W2=[1,1], b2=0. Вычислите выход при входе x=[1,0], используя сигмоиду на скрытом и выходном слоях. Ответ округлите до 3 знаков."
        },
        {
          "exercise_title": "Визуализация решения XOR",
          "exercise_instructions": "Обучите MLP 2-2-1 на датасете XOR. Постройте контурный график предсказаний сети на сетке 100×100. Отметьте обучающие точки цветами классов и убедитесь, что граница нелинейна."
        }
      ]
    }
  ]
}