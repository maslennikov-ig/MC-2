 {
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Глубокое погружение в концептуальную теорию нейронных сетей с практическими примерами реализации базовых компонентов от нуля. Раздел охватывает биологическую основу, математические модели, архитектурные решения и фундаментальные принципы обучения.",
  "learning_objectives": [
    "Формулировать аналогию между биологическими и искусственными нейронами",
    "Реализовать перцептрон и многослойный перцептрон с использованием матричных операций",
    "Сравнивать функции активации и обосновывать выбор для конкретных задач",
    "Вычислять прямое распространение вручную через сети произвольной архитектуры",
    "Формализовать функции потерь для задач регрессии и классификации"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Биологические нейроны и искусственный перцептрон",
      "lesson_objective": "Обучающиеся смогут объяснить структурные компоненты биологического нейрона и реализовать функционирующий перцептрон для линейно разделимых задач",
      "key_topics": [
        "Строение биологического нейрона: дендриты, аксон, синапсы",
        "Модель Мак-Каллока-Питтса: сумма взвешенных входов с порогом",
        "Архитектура перцептрона: входной слой, синаптические веса, выход",
        "Пороговая функция активации и принятие бинарного решения",
        "Геометрическая интерпретация: разделяющая гиперплоскость"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона для логических операций",
          "exercise_instructions": "Реализуйте класс Perceptron с методами fit и predict. Обучите модель на данных для логических операций И, ИЛИ. Визуализируйте разделяющую линию на плоскости для каждой операции. Проанализируйте, почему перцептрон не справляется с операцией ИСКЛЮЧАЮЩЕЕ ИЛИ."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Эволюция функций активации от порога к нелинейности",
      "lesson_objective": "Обучающиеся смогут проанализировать градиентные свойства функций активации и обосновать их применимость в глубоких сетях",
      "key_topics": [
        "Ограничения ступенчатой функции: нулевой градиент и разрывность",
        "Сигмоида: выраженная нелинейность с затухающим градиентом",
        "Гиперболический тангенс: нулевое среднее и нормализация",
        "ReLU: кусочно-линейная функция и проблема 'умирания' нейронов",
        "Leaky ReLU и PReLU: параметризованные отрицательные части"
      ],
      "exercises": [
        {
          "exercise_title": "Анализ градиентного потока функций активации",
          "exercise_instructions": "Реализуйте функции активации sigmoid, tanh, ReLU, LeakyReLU и их производные. Постройте графики функций и градиентов в диапазоне [-5, 5]. Рассчитайте среднее и дисперсию выходов для нормально распределенных входов. Сформулируйте рекомендации по выбору функции для скрытых слоев."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Многослойный перцептрон и теорема универсальной аппроксимации",
      "lesson_objective": "Обучающиеся смогут построить MLP архитектуру и математически обосновать способность сети решать нелинейные задачи",
      "key_topics": [
        "Проблема XOR: доказательство недостаточности одного слоя",
        "Скрытые слои как генераторы нелинейных признаков",
        "Теорема универсальной аппроксимации: формулировка и следствия",
        "Выбор количества нейронов в скрытом слое: компромисс сложность/переобучение",
        "Количество параметров модели и вычислительная сложность"
      ],
      "exercises": [
        {
          "exercise_title": "Построение MLP для задачи XOR",
          "exercise_instructions": "Создайте двухслойную сеть с одним скрытым слоем из 2 нейронов с сигмоидной активацией. Реализуйте прямое распространение без использования фреймворков. Вручную подберите веса, обеспечивающие 100% точность на XOR. Постройте визуализацию решающих границ каждого нейрона скрытого слоя."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Прямое распространение и вычислительные графы",
      "lesson_objective": "Обучающиеся смогут вычислять промежуточные активации вручную и представлять вычисления в виде ориентированного ациклического графа",
      "key_topics": [
        "Матричное представление взвешенной суммы для батча данных",
        "Послойное применение активаций: от входа к выходу",
        "Вычислительный граф: узлы операций и ребра данных",
        "Чувствительность выхода к изменениям весов: частные производные",
        "Сохранение промежуточных значений для обратного распространения"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет forward pass в MLP",
          "exercise_instructions": "Задана сеть с входом размерности 3, скрытым слоем из 4 нейронов (ReLU) и выходом из 2 нейронов (софтмакс). Используя предоставленные веса и смещения, вычислите все промежуточные активации для входного вектора [0.5, -1.0, 2.0]. Нарисуйте вычислительный граф с указанием всех операций и размерностей тензоров."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Формализация функций потерь для обучения с учителем",
      "lesson_objective": "Обучающиеся смогут формулировать функции потерь для регрессионных и классификационных задач и анализировать ландшафт потерь",
      "key_topics": [
        "Среднеквадратичная ошибка: математическая запись и геометрия",
        "Кросс-энтропия для бинарной классификации: логистическая потеря",
        "Категориальная кросс-энтропия: многоклассовая классификация",
        "Ландшафт потерь: выпуклость, локальные минимумы, седловые точки",
        "Эмпирический риск и проблема переобучения: разделение на train/val"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация и сравнение функций потерь",
          "exercise_instructions": "Реализуйте MSELoss и CrossEntropyLoss с поддержкой батчей. Сгенерируйте синтетические предсказания и истинные метки для задач регрессии (3 выхода) и классификации (5 классов). Вычислите потери для различных сценариев: идеальные предсказания, случайные, систематические ошибки. Проанализируйте градиенты по отношению к выходам сети."
        }
      ]
    }
  ]
}