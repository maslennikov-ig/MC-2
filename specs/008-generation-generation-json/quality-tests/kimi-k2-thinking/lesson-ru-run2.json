 {
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Концептуальная теоретическая секция с практическими примерами, охватывающая биологические основы, математические модели и алгоритмы обучения нейронных сетей от простого перцептрона до многослойных архитектур",
  "learning_objectives": [
    "Объяснять принципы работы биологических и искусственных нейронов",
    "Вычислять выходные значения перцептронов и многослойных сетей",
    "Сравнивать функции активации и обосновывать их выбор",
    "Проектировать архитектуру нейронной сети для задач классификации",
    "Вычислять функции потерь и выполнять шаги градиентного спуска",
    "Применять алгоритм обратного распространения для обновления весов"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "От биологического нейрона к искусственному перцептрону",
      "lesson_objective": "Учащиеся смогут объяснить связь между биологическими нейронами и искусственными перцептронами, а также вычислить выход перцептрона для заданных входов и весов",
      "key_topics": [
        "Структура биологического нейрона: дендриты, аксон, синапсы",
        "Модель Мак-Каллока-Питтса: формализация нейрона",
        "Архитектура перцептрона: взвешенная сумма и пороговая функция",
        "Функция активации step: принцип работы и ограничения",
        "Геометрическая интерпретация: разделяющая гиперплоскость"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет перцептрона",
          "exercise_instructions": "Дан перцептрон с 3 входами, весами w1=0.5, w2=-0.3, w3=0.8, порогом θ=0.4. Вычислите выход для входных векторов: а) [1, 0, 1], б) [0, 1, 0], в) [1, 1, 1]. Покажите промежуточные значения взвешенной суммы."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации: от пороговой до гибкой",
      "lesson_objective": "Учащиеся смогут сравнить различные функции активации, проанализировать их свойства и выбрать подходящую для конкретной задачи",
      "key_topics": [
        "Пороговая функция: простота и недостатки дифференцируемости",
        "Сигмоида: свойства, выходной диапазон и проблема затухающего градиента",
        "ReLU: преимущества, проблема 'мертвых нейронов'",
        "Leaky ReLU и PReLU: модификации для улучшения обучения",
        "Softmax: вероятностная интерпретация для многоклассовой классификации"
      ],
      "exercises": [
        {
          "exercise_title": "Анализ и визуализация функций активации",
          "exercise_instructions": "Постройте графики сигмоиды, ReLU и Leaky ReLU (коэффициент 0.01) для диапазона x от -5 до 5. Вычислите производные в точках x=0 и x=2. Опишите, какая функция лучше подходит для глубоких сетей и почему."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Многослойные сети: компоновка нейронов в систему",
      "lesson_objective": "Учащиеся смогут спроектировать архитектуру многослойной нейронной сети для задачи классификации и выполнить прямое распространение данных",
      "key_topics": [
        "Трехслойная архитектура: входной, скрытый и выходной слои",
        "Прямое распространение: последовательные матричные преобразования",
        "Выбор количества нейронов: эвристики и практические соображения",
        "Представительная способность: теорема универсальной аппроксимации",
        "Проблема переобучения: связь с сложностью сети"
      ],
      "exercises": [
        {
          "exercise_title": "Проектирование сети для распознавания цифр",
          "exercise_instructions": "Спроектируйте полносвязную нейронную сеть для классификации рукописных цифр (28x28 пикселей). Определите: размер входного слоя, количество и размер скрытых слоев, размер выходного слоя. Обоснуйте каждое решение. Выполните прямое распространение для одного примера."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Функции потерь и градиентный спуск: обучение сетей",
      "lesson_objective": "Учащиеся смогут вычислить значение функции потерь для различных задач и выполнить один шаг градиентного спуска для обновления весов",
      "key_topics": [
        "Среднеквадратичная ошибка (MSE): применение в регрессии",
        "Кросс-энтропия: принцип работы для классификации",
        "Градиентный спуск: идея и математическая формулировка",
        "Скорость обучения: влияние на сходимость и устойчивость",
        "Стохастический и мини-пакетный градиентный спуск: компромиссы"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчет потерь и обновление весов",
          "exercise_instructions": "Дана сеть с 2 входами, 1 выходом. Цель: 0.9, предсказание: 0.4. Вычислите MSE и кросс-энтропию. При learning_rate=0.1 и градиенте 0.5, вычислите новый вес. Проанализируйте, как изменение learning_rate влияет на величину шага."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Алгоритм обратного распространения: математика обучения",
      "lesson_objective": "Учащиеся смогут вычислить градиенты с помощью правила цепочки и объяснить пошаговую работу алгоритма обратного распространения ошибки",
      "key_topics": [
        "Правило цепочки: дифференцирование сложных функций",
        "Вычисление градиентов для выходного слоя: от потерь к весам",
        "Распространение ошибки на скрытые слои: последовательное дифференцирование",
        "Граф вычислений: визуализация потока градиентов",
        "Проблема затухающего и взрывающегося градиента: причины и следствия"
      ],
      "exercises": [
        {
          "exercise_title": "Вычисление градиентов для двухслойной сети",
          "exercise_instructions": "Дана сеть: вход 2, скрытый слой 2 нейрона (сигмоида), выход 1 (сигмоида). Вычислите градиенты dL/dw для всех весов вручную, используя правило цепочки. Начальные веса: w1=0.3, w2=-0.2, w3=0.5, w4=0.1, w5=0.4, w6=-0.3. Вход: [0.5, 0.8], цель: 1. Покажите все промежуточные вычисления."
        }
      ]
    }
  ]
}