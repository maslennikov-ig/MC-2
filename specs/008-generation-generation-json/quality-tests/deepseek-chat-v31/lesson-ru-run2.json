{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Этот раздел знакомит с фундаментальными принципами работы нейронных сетей. Студенты изучат базовые архитектуры, поймут процесс обучения модели и реализуют свою первую простую нейронную сеть с нуля, что заложит основу для дальнейшего изучения глубокого обучения.",
  "learning_objectives": [
    "Объяснить архитектуру и принцип работы искусственного нейрона",
    "Построить и обучить простую полносвязную нейронную сеть для задачи классификации",
    "Реализовать алгоритм обратного распространения ошибки для обновления весов",
    "Проанализировать влияние гиперпараметров на процесс обучения модели",
    "Интерпретировать результаты работы нейронной сети и вычислить метрики качества"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Искусственный нейрон и перцептрон",
      "lesson_objective": "Создать и обучить простейший перцептрон для решения задачи логического ИЛИ, визуализировав процесс принятия решения.",
      "key_topics": [
        "Биологический прототип и математическая модель искусственного нейрона",
        "Активационные функции: ступенчатая, сигмоида, гиперболический тангенс",
        "Структура и ограничения перцептрона Розенблатта",
        "Графическое представление разделяющей гиперплоскости"
      ],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона на Python",
          "exercise_instructions": "Напишите код класса Perceptron с методами для прямого прохода и обновления весов. Обучите модель на данных для операции ИЛИ, подобрав скорость обучения и количество эпох. Визуализируйте полученную разделяющую линию и исходные точки."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Многослойные сети и алгоритм обратного распространения ошибки",
      "lesson_objective": "Реализовать алгоритм обратного распространения ошибки для обновления весов в двухслойной нейронной сети.",
      "key_topics": [
        "Преодоление ограничений перцептрона: теорема Цыбенко",
        "Архитектура многослойного перцептрона (MLP): входной, скрытый и выходной слои",
        "Цепное правило для вычисления градиентов (Backpropagation)",
        "Матричная форма представления forward и backward pass"
      ],
      "exercises": [
        {
          "exercise_title": "Расчет градиентов вручную",
          "exercise_instructions": "Для заданной простой сети с двумя нейронами рассчитайте вручную градиенты функции потерь по всем весам, используя цепное правило. Проверьте свои вычисления с помощью численных методов."
        },
        {
          "exercise_title": "Реализация Backpropagation для XOR",
          "exercise_instructions": "Создайте двухслойную нейронную сеть для решения задачи XOR. Реализуйте полный цикл обучения: прямой проход, вычисление ошибки, обратное распространение и обновление весов. Сравните результат с однослойным перцептроном."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции потерь и оптимизация градиентного спуска",
      "lesson_objective": "Проанализировать влияние различных функций потерь и методов оптимизации на сходимость модели при решении задачи классификации изображений MNIST.",
      "key_topics": [
        "Функции потерь для регрессии (MSE) и классификации (кросс-энтропия)",
        "Проблемы vanilla градиентного спуска: локальные минимумы, медленная сходимость",
        "Методы оптимизации: SGD с моментом, RMSprop, Adam",
        "Анализ кривых обучения: переобучение и недообучение"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение оптимизаторов",
          "exercise_instructions": "Обучите одну и ту же нейронную сеть на наборе данных MNIST, используя SGD, SGD с моментом и Adam. Постройте графики сходимости функции потерь для каждого метода и сравните их по количеству эпох до достижения заданной точности."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Регуляризация и настройка гиперпараметров",
      "lesson_objective": "Применить методы регуляризации для борьбы с переобучением и провести поиск по сетке для настройки гиперпараметров модели.",
      "key_topics": [
        "L1 и L2 регуляризация (weight decay)",
        "Метод исключения (Dropout)",
        "Ранняя остановка (Early Stopping)",
        "Разбиение данных на обучающую, валидационную и тестовую выборки",
        "Стратегии подбора гиперпараметров: grid search, random search"
      ],
      "exercises": [
        {
          "exercise_title": "Борьба с переобучением на синтетических данных",
          "exercise_instructions": "Сгенерируйте сложный нелинейно разделяемый набор данных. Создайте модель, склонную к переобучению. Последовательно примените L2-регуляризацию, Dropout и раннюю остановку. Проанализируйте, как каждый метод влияет на графики обучения и итоговую точность."
        },
        {
          "exercise_title": "Оптимизация гиперпараметров",
          "exercise_instructions": "Для модели с предыдущего упражнения определите сетку гиперпараметров (скорость обучения, коэффициент регуляризации, размер скрытого слоя). Реализуйте простой grid search для нахождения наилучшей комбинации, оценивая качество на валидационной выборке."
        }
      ]
    }
  ]
}