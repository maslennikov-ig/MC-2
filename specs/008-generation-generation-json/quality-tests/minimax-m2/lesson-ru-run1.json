
{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Раздел охватывает теоретические основы нейронных сетей с акцентом на математические модели, функции потерь, методы оптимизации и обратное распространение ошибки. Разобраны примеры вычислений для персептрона и многослойной сети, показаны ограничения линейных моделей и практические аспекты градиентного спуска. Материал формирует концептуальное понимание процесса обучения нейронных сетей и подготовку к практическим задачам.",
  "learning_objectives": [
    "объяснить модель нейрона как аффинного преобразования и нелинейной функции активации",
    "вычислить выход персептрона для заданных весов и входных данных",
    "различать линейно разделимые и неразделимые задачи на примерах (OR, XOR)",
    "рассчитать значение функции потерь и её градиент по параметрам модели",
    "применить шаг градиентного спуска для обновления весов с заданной скоростью обучения",
    "выполнить прямое и обратное распространение в простой многослойной сети"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Нейрон как математическая функция: веса, смещения и активация",
      "lesson_objective": "вычислить выход нейрона для заданных весов, смещения и функции активации и объяснить роль каждого компонента",
      "key_topics": ["скалярное произведение и аффинное преобразование", "смещение (bias) и его геометрическая интерпретация", "сигмоидальная и ReLU функции активации", "реализация нейрона в numpy"],
      "exercises": [
        {
          "exercise_title": "Вычисление выхода нейрона",
          "exercise_instructions": "Дано: x = [0.2, -0.1, 0.7], w = [0.5, -0.3, 0.9], b = 0.1. Используя сигмоиду, вычислите выход нейрона. Округлите результат до четырёх знаков. Представьте промежуточные вычисления (скалярное произведение и аргумент сигмоиды)."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Однослойный персептрон: линейная разделимость и задача XOR",
      "lesson_objective": "обосновать, почему персептрон решает OR, но не решает XOR, опираясь на геометрические соображения",
      "key_topics": ["пороговая функция активации", "гиперплоскость разделяющего класса", "линейно разделимые и неразделимые задачи", "ограничения однослойных моделей"],
      "exercises": [
        {
          "exercise_title": "Классификация логической операции с помощью персептрона",
          "exercise_instructions": "Задана таблица истинности для OR. Предложите набор весов w и смещение b персептрона, который корректно классифицирует все четыре входа: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→1. Обоснуйте выбор и покажите рассчёты для каждого входа."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции потерь: квадратичная и перекрестная энтропия",
      "lesson_objective": "вычислить значение функции потерь и её градиент по предсказаниям и параметрам модели на конкретных данных",
      "key_topics": ["среднеквадратичная ошибка и её производные", "перекрестная энтропия для бинарной классификации", "градиент по предсказаниям и параметрам", "численная проверка градиента"],
      "exercises": [
        {
          "exercise_title": "Расчёт производной MSE по параметрам модели",
          "exercise_instructions": "Дано: одно наблюдение (x=[1, -1], y=1), модель: ŷ = σ(w·x + b). Вычислите аналитически dL/dw и dL/db при L = (y - ŷ)^2. Покажите все промежуточные производные."
        },
        {
          "exercise_title": "Потери на батче и усреднённый градиент",
          "exercise_instructions": "Дан батч: [(x=[1, 0], y=0), (x=[-1, 1], y=1)]. Используя сигмоиду и L = (1/N)∑(y - ŷ)^2, вычислите значение потерь для батча и dL/dw (усреднённый градиент) с w=[0.5, -0.5], b=0.2."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Градиентный спуск: обновление весов и скорость обучения",
      "lesson_objective": "выполнить итерацию градиентного спуска для обновления параметров модели с заданной скоростью обучения",
      "key_topics": ["правило обновления весов (w ← w - η∇w L)", "выбор и влияние скорости обучения", "норма градиента и остановка по сходимости", "визуализация траектории спуска на плоскости параметров"],
      "exercises": [
        {
          "exercise_title": "Шаг градиентного спуска для простой регрессии",
          "exercise_instructions": "Модель: ŷ = w1·x1 + w2·x2 + b. Дано: данные [(x=[1, 2], y=3), (x=[2, -1], y=0)], L = (1/N)∑(y - ŷ)^2. Вычислите текущий градиент по w1, w2, b и выполните один шаг с η=0.1. Представьте обновлённые параметры и значение потерь до и после шага."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Многослойные сети и обратное распространение ошибки",
      "lesson_objective": "реализовать прямое и обратное распространение в двуслойной сети с сигмоидой для вычисления градиентов по параметрам",
      "key_topics": ["прямое распространение: скрытый и выходной слои", "производные сигмоиды и цепочки правил", "формулы градиентов для скрытого и выходного слоёв", "проверка градиентов на конечных разностях"],
      "exercises": [
        {
          "exercise_title": "Прямое и обратное распространение в двуслойной сети",
          "exercise_instructions": "Сеть: вход x=[1, -1]→скрытый слой h=σ(W1x+b1) с 2 нейронами→выход ŷ=σ(W2h+b2). Дано: W1=[[0.5, -0.2], [0.1, 0.8]], b1=[0, 0], W2=[0.6, -0.4], b2=0.0, y=1. Выполните прямое распространение, вычислите L=(y-ŷ)^2, затем градиенты dL/dW2, dL/db2, dL/dW1, dL/db1. Представьте все промежуточные значения."
        }
      ]
    }
  ]
}