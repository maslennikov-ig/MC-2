{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Теоретический раздел, посвящённый фундаментальным концепциям нейронных сетей. Включает изучение базовых элементов, функций активации, принципов прямого распространения и обратного распространения ошибки. Содержит практические примеры и задачи для закрепления материала.",
  "learning_objectives": [
    "Объяснить принцип работы искусственного нейрона и перцептрона",
    "Классифицировать и применять различные функции активации в зависимости от задачи",
    "Проанализировать архитектуру многослойных нейронных сетей прямого распространения",
    "Выбрать подходящую функцию потерь для конкретной задачи машинного обучения",
    "Реализовать процесс обратного распространения ошибки для обновления весов"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Искусственный нейрон и перцептрон Розенблатта",
      "lesson_objective": "Студенты смогут объяснить структуру искусственного нейрона и реализовать алгоритм обучения перцептрона на линейно разделимых данных",
      "key_topics": ["Биологический нейрон vs искусственный нейрон", "Взвешенная сумма и пороговая функция", "Алгоритм обучения перцептрона", "Геометрическая интерпретация разделяющей гиперплоскости", "Ограничения перцептрона: проблема XOR"],
      "exercises": [
        {
          "exercise_title": "Реализация перцептрона для классификации ирисов",
          "exercise_instructions": "Написать функцию на Python, реализующую перцептрон для классификации двух видов ирисов по длине и ширине лепестков. Использовать алгоритм обучения перцептрона с обновлением весов после каждой неправильной классификации."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Функции активации и их свойства",
      "lesson_objective": "Студенты смогут выбрать и обосновать выбор функции активации для конкретного слоя нейронной сети в зависимости от решаемой задачи",
      "key_topics": ["Пороговая функция (step function) и её разрывность", "Сигмоидальная функция и её производная", "Гиперболический тангенс (tanh) и нормализация выходов", "ReLU и её варианты (Leaky ReLU, Parametric ReLU)", "Функция Softmax для многоклассовой классификации", "Проблема исчезающего градиента"],
      "exercises": [
        {
          "exercise_title": "Сравнение функций активации на синтетических данных",
          "exercise_instructions": "Создать визуализацию различных функций активации и их производных. Сгенерировать синтетические данные для задачи регрессии и классификации, обучить простую нейронную сеть с каждой функцией активации и сравнить скорость сходимости и качество результатов."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Многослойные сети прямого распространения",
      "lesson_objective": "Студенты смогут спроектировать архитектуру многослойной нейронной сети и объяснить роль каждого слоя в процессе принятия решений",
      "key_topics": ["Входной, скрытый и выходной слои", "Полносвязные (dense) слои и матричные операции", "Прямое распространение сигнала через сеть", "Выбор количества нейронов в скрытых слоях", "Проблема переобучения и регуляризация", "Теорема универсальной аппроксимации"],
      "exercises": [
        {
          "exercise_title": "Архитектура сети для распознавания рукописных цифр",
          "exercise_instructions": "Спроектировать и реализовать нейронную сеть для классификации рукописных цифр (dataset MNIST). Определить оптимальное количество слоёв и нейронов в каждом слое, обосновать выбор функции активации для каждого слоя."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Функции потерь и оптимизация",
      "lesson_objective": "Студенты смогут выбрать подходящую функцию потерь и метод оптимизации для различных типов задач машинного обучения",
      "key_topics": ["Mean Squared Error (MSE) для регрессии", "Cross-Entropy Loss для бинарной и многоклассовой классификации", "Hinge Loss для методов опорных векторов", "Градиентный спуск и его варианты (стохастический, мини-батч)", "Инициализация весов: Xavier/He инициализация", "Скорость обучения и планировщики скорости"],
      "exercises": [
        {
          "exercise_title": "Сравнение функций потерь на задаче классификации",
          "exercise_instructions": "Обучить две идентичные нейронные сети на датасете Fashion-MNIST с разными функциями потерь (Cross-Entropy и MSE). Сравнить скорость сходимости, качество классификации и устойчивость к зашумлённым данным. Визуализировать графики обучения."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Обратное распространение ошибки",
      "lesson_objective": "Студенты смогут вручную вывести и реализовать алгоритм обратного распространения для многослойной нейронной сети",
      "key_topics": ["Правило дифференцирования сложной функции (цепное правило)", "Вычисление градиентов от выходного слоя к входному", "Обновление весов по правилу градиентного спуска", "Автоматическое дифференцирование в фреймворках", "Векторизация вычислений для эффективности", "Валидация градиентов: численная проверка"],
      "exercises": [
        {
          "exercise_title": "Реализация backprop с нуля",
          "exercise_instructions": "Создать полную реализацию обратного распространения ошибки для двухслойной нейронной сети без использования библиотек автодифа. Включить прямое распространение, вычисление градиентов для всех весов и обновление параметров. Протестировать на простом синтетическом датасете и сравнить с результатами библиотечной реализации."
        }
      ]
    }
  ]
}