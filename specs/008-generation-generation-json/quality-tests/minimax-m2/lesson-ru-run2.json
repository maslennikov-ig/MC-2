{
  "section_number": 1,
  "section_title": "Основы нейронных сетей",
  "section_description": "Концептуальный раздел, раскрывающий фундаментальные принципы построения и обучения нейронных сетей. Рассматриваются модель нейрона и функции активации, архитектуры многослойных перцептронов (MLP), алгоритм обратного распространения ошибки, процесс обучения (инициализация, функции потерь, градиентные методы, переобучение и регуляризация). Включает примеры и практические задания для закрепления понимания без написания кода.",
  "learning_objectives": [
    "объяснить роль функции активации и условия для линейной разделимости",
    "изобразить архитектуру MLP и описать прямой и обратный проход",
    "вычислить градиенты простой функции потерь по параметрам вручную",
    "сравнить оптимизаторы по правилу обновления и типичным траекториям",
    "распознать симптомы переобучения и предложить меры регуляризации"
  ],
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Нейрон и функции активации",
      "lesson_objective": "студент сможет описать структуру искусственного нейрона и выбрать подходящую функцию активации для задачи классификации или регрессии",
      "key_topics": [
        "линейная комбинация входов и весов, смещение (bias)",
        "ступенчатая, сигмоидальная, гиперболический тангенс, ReLU и варианты",
        "дифференцируемость и градиенты функций активации",
        "линейная разделимость и ограничения одного нейрона"
      ],
      "exercises": [
        {
          "exercise_title": "Сравнение функций активации",
          "exercise_instructions": "Сопоставьте каждой функции активации её ключевые свойства: область значений, дифференцируемость, поведение при больших |x|, наличие «умирающего» градиента. Оформите ответ в виде таблицы."
        },
        {
          "exercise_title": "Анализ линейной разделимости",
          "exercise_instructions": "Для двух классов на плоскости (например, выше/ниже прямой y=x) объясните, почему один перцептрон справляется с задачей, а для «исключающего ИЛИ» (XOR) ему не хватает表达能力. Приведите схематичное объяснение."
        }
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Многослойный перцептрон: архитектура и прямой проход",
      "lesson_objective": "студент сможет объяснить структуру MLP: слои, нейроны, веса, и описать прямой проход через скрытые слои с нелинейными активациями",
      "key_topics": [
        "входной, скрытый и выходной слои; полносвязная архитектура",
        "необходимость нелинейностей для аппроксимации сложных функций",
        "размерность слоёв, подсчёт числа параметров",
        "формулы прямого прохода: линейное преобразование и активация"
      ],
      "exercises": [
        {
          "exercise_title": "Подсчёт параметров MLP",
          "exercise_instructions": "Дана архитектура: вход 784 признака, два скрытых слоя по 128 нейронов, выход 10. Посчитайте общее число параметров (веса и смещения) для сети без и с учётом bias в каждом слое. Приведите пошаговый расчёт."
        },
        {
          "exercise_title": "Интерпретация скрытых представлений",
          "exercise_instructions": "Опишите, как активации нейронов первого скрытого слоя могут представлять промежуточные признаки данных (например, грани и текстуры для изображений). Приведите качественный пример без кода."
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Функции потерь и градиентный спуск",
      "lesson_objective": "студент сможет объяснить выбор функции потерь для задач регрессии и классификации и выполнить базовый расчёт градиентов для простого случая",
      "key_topics": [
        "квадратичная ошибка и кросс-энтропия для бинарной/многоклассовой классификации",
        "градиентный спуск: правило обновления параметров, скорость обучения",
        "мини-батчи и стохастичность, влияние на сходимость",
        "кривые обучения и валидации, ранняя остановка"
      ],
      "exercises": [
        {
          "exercise_title": "Ручной расчёт градиента",
          "exercise_instructions": "Для задачи бинарной классификации с одним нейроном (w, b), сигмоидой σ(z) и логистической потерей L = −(y·logŷ + (1−y)·log(1−ŷ)) выведите выражения производных ∂L/∂w и ∂L/∂b. Покажите пошаговые преобразования."
        },
        {
          "exercise_title": "Диагностика по кривым обучения",
          "exercise_instructions": "Нарисуйте качественные графики training/validation loss для случаев недообучения и переобучения. Объясните, как по этим кривым выбрать момент ранней остановки."
        }
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Оптимизация: стохастический градиент и его варианты",
      "lesson_objective": "студент сможет сравнить классический SGD, момент, Нестерова, AdaGrad, RMSProp и Adam по правилу обновления и типичным свойствам траекторий",
      "key_topics": "обновление параметров, момент и Нестеров, адаптивные методы и их гиперпараметры, выбор скорости обучения, графики траекторий оптимизации",
      "exercises": [
        {
          "exercise_title": "Сравнительная таблица оптимизаторов",
          "exercise_instructions": "Составьте таблицу, где для каждого оптимизатора указаны: формула обновления параметров, используемые моменты/состояния, гиперпараметры, преимущества и недостатки, типичные сценарии применения."
        },
        {
          "exercise_title": "Планирование скорости обучения",
          "exercise_instructions": "Опишите, как изменится динамика обучения при слишком высокой и слишком низкой скорости обучения. Предложите стратегии её адаптации (например, staircase/cosine) и объясните ожидаемые эффекты."
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Регуляризация и обобщающая способность",
      "lesson_objective": "студент сможет распознать симптомы переобучения и выбрать подходящие методы регуляризации для повышения обобщающей способности модели",
      "key_topics": [
        "переобучение: симптомы и причины, компромисс смещение/дисперсия",
        "L1/L2-регуляризация, ограничение весов (weight decay)",
        "дропаут: вероятность «зануления» нейронов и масштабирование",
        "ранняя остановка по валидации и выбор эпохи",
        "нормировка батчей: смещение и масштаб, влияние на обучение"
      ],
      "exercises": [
        {
          "exercise_title": "Комбинация методов регуляризации",
          "exercise_instructions": "Для датасета с 5000 обучающими примерами и 200 параметрами предложите комбинацию L2-дропаута, weight decay и ранней остановки. Объясните, как каждый компонент влияет на переобучение и ожидаемые кривые обучения."
        },
        {
          "exercise_title": "Диагностика недообучения и переобучения",
          "exercise_instructions": "Сравните сценарии: «high bias» и «high variance». Для каждого укажите, какие изменения архитектуры, регуляризации и данных могут помочь (например, увеличить/уменьшить число нейронов, добавить L2, собрать больше данных)."
        }
      ]
    }
  ]
}